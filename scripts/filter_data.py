#!/usr/bin/env python3
"""
RNAæ•°æ®é•¿åº¦ç­›é€‰è„šæœ¬
æ ¹æ®åºåˆ—é•¿åº¦ï¼ˆ16-256ntï¼‰ç­›é€‰RNA3D_DATAä¸­çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨CD-HITè¿›è¡Œåºåˆ—å»å†—ä½™
å°†ç¬¦åˆæ¡ä»¶çš„æ•°æ®å¤åˆ¶åˆ°processed_dataæ–‡ä»¶å¤¹ï¼Œä¿æŒåŸæœ‰çš„ç›®å½•ç»“æ„
"""

import os
import sys
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
import argparse
from collections import defaultdict


def read_fasta_sequence(filepath: str) -> Optional[str]:
    """
    è¯»å–FASTAæ–‡ä»¶ä¸­çš„ç¬¬ä¸€ä¸ªåºåˆ—
    
    Args:
        filepath: FASTAæ–‡ä»¶è·¯å¾„
        
    Returns:
        åºåˆ—å­—ç¬¦ä¸²ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯åˆ™è¿”å›None
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        sequence = ""
        for line in lines:
            line = line.strip()
            if line and not line.startswith('>'):
                sequence += line
        
        return sequence.upper() if sequence else None
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
        return None


def read_seq_file_sequence(filepath: str) -> Optional[str]:
    """
    è¯»å–.seqæ–‡ä»¶ä¸­çš„åºåˆ—
    
    Args:
        filepath: .seqæ–‡ä»¶è·¯å¾„
        
    Returns:
        åºåˆ—å­—ç¬¦ä¸²ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯åˆ™è¿”å›None
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        sequence = ""
        for line in lines:
            line = line.strip()
            if line and not line.startswith('>'):
                sequence += line
        
        return sequence.upper() if sequence else None
        
    except Exception as e:
        print(f"âŒ è¯»å–seqæ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
        return None


def get_sequence_length(sequence: str) -> int:
    """è·å–åºåˆ—é•¿åº¦ï¼ˆå¿½ç•¥éå­—æ¯å­—ç¬¦ï¼‰"""
    if not sequence:
        return 0
    
    # åªè®¡ç®—å­—æ¯å­—ç¬¦
    return len([c for c in sequence if c.isalpha()])


def scan_all_sequences(data_dir: str) -> Dict[str, Tuple[str, int, str]]:
    """
    æ‰«ææ‰€æœ‰åºåˆ—æ–‡ä»¶å¹¶è·å–é•¿åº¦ä¿¡æ¯
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶basenameï¼Œå€¼ä¸º(file_path, length, file_type)å…ƒç»„
        file_typeå¯èƒ½æ˜¯'fasta'æˆ–'seq'
    """
    sequences_dir = Path(data_dir) / "sequences"
    seq_dir = Path(data_dir) / "seq"
    
    all_sequences = {}
    
    # å¤„ç†sequencesç›®å½•ä¸­çš„.fastaæ–‡ä»¶
    if sequences_dir.exists():
        print(f"ğŸ” æ‰«æ {sequences_dir}...")
        for fasta_file in sequences_dir.glob("*.fasta"):
            basename = fasta_file.stem
            sequence = read_fasta_sequence(str(fasta_file))
            if sequence:
                length = get_sequence_length(sequence)
                all_sequences[basename] = (str(fasta_file), length, 'fasta')
                print(f"  ğŸ“„ {basename}: {length}nt")
            else:
                print(f"  âŒ æ— æ³•è¯»å–: {basename}")
    
    # å¤„ç†seqç›®å½•ä¸­çš„.seqæ–‡ä»¶
    if seq_dir.exists():
        print(f"ğŸ” æ‰«æ {seq_dir}...")
        for seq_file in seq_dir.glob("*.seq"):
            basename = seq_file.stem
            # å¦‚æœå·²ç»åœ¨fastaä¸­æ‰¾åˆ°äº†ï¼Œè·³è¿‡
            if basename in all_sequences:
                continue
                
            sequence = read_seq_file_sequence(str(seq_file))
            if sequence:
                length = get_sequence_length(sequence)
                all_sequences[basename] = (str(seq_file), length, 'seq')
                print(f"  ğŸ“„ {basename}: {length}nt")
            else:
                print(f"  âŒ æ— æ³•è¯»å–: {basename}")
    
    return all_sequences


def filter_by_length(sequences: Dict[str, Tuple[str, int, str]], 
                    min_length: int = 16, 
                    max_length: int = 256) -> Dict[str, Tuple[str, int, str]]:
    """
    æ ¹æ®é•¿åº¦èŒƒå›´ç­›é€‰åºåˆ—
    
    Args:
        sequences: åºåˆ—ä¿¡æ¯å­—å…¸
        min_length: æœ€å°é•¿åº¦
        max_length: æœ€å¤§é•¿åº¦
        
    Returns:
        ç­›é€‰åçš„åºåˆ—å­—å…¸
    """
    filtered = {}
    
    print(f"ğŸ” ç­›é€‰é•¿åº¦èŒƒå›´: {min_length}-{max_length}nt")
    print("-" * 60)
    
    for basename, (file_path, length, file_type) in sequences.items():
        if min_length <= length <= max_length:
            filtered[basename] = (file_path, length, file_type)
            print(f"âœ… {basename}: {length}nt - ç¬¦åˆæ¡ä»¶")
        else:
            print(f"âŒ {basename}: {length}nt - è¶…å‡ºèŒƒå›´")
    
    print("-" * 60)
    print(f"ğŸ“Š ç­›é€‰ç»“æœ: åŸå§‹ {len(sequences)} ä¸ªï¼Œç¬¦åˆæ¡ä»¶ {len(filtered)} ä¸ª")
    
    return filtered


def create_temp_fasta(filtered_sequences: Dict[str, Tuple[str, int, str]]) -> str:
    """
    åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰ç­›é€‰åçš„åºåˆ—
    
    Args:
        filtered_sequences: ç­›é€‰åçš„åºåˆ—å­—å…¸
        
    Returns:
        ä¸´æ—¶FASTAæ–‡ä»¶è·¯å¾„
    """
    temp_fasta = tempfile.NamedTemporaryFile(mode='w', suffix='.fasta', delete=False)
    
    print("ğŸ“ åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶ç”¨äºCD-HITèšç±»...")
    
    for basename, (file_path, length, file_type) in filtered_sequences.items():
        # è¯»å–åºåˆ—å†…å®¹
        if file_type == 'fasta':
            sequence = read_fasta_sequence(file_path)
        else:
            sequence = read_seq_file_sequence(file_path)
        
        if sequence:
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_fasta.write(f">{basename}\n{sequence}\n")
    
    temp_fasta.close()
    print(f"ğŸ“ ä¸´æ—¶FASTAæ–‡ä»¶å·²åˆ›å»º: {temp_fasta.name}")
    
    return temp_fasta.name


def run_cdhit_clustering(input_fasta: str, similarity_threshold: float = 0.8, 
                        threads: int = 4) -> Tuple[str, str]:
    """
    è¿è¡ŒCD-HITè¿›è¡Œåºåˆ—èšç±»
    
    Args:
        input_fasta: è¾“å…¥FASTAæ–‡ä»¶è·¯å¾„
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        threads: ä½¿ç”¨çš„çº¿ç¨‹æ•°
        
    Returns:
        (è¾“å‡ºæ–‡ä»¶è·¯å¾„, èšç±»ä¿¡æ¯æ–‡ä»¶è·¯å¾„)
    """
    output_fasta = input_fasta + ".cdhit"
    cluster_file = output_fasta + ".clstr"
    
    print(f"ğŸ”¬ å¼€å§‹CD-HITåºåˆ—èšç±» (ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold*100}%)...")
    
    # æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®è¯é•¿
    if similarity_threshold >= 0.9:
        word_length = 8
    elif similarity_threshold >= 0.85:
        word_length = 7
    elif similarity_threshold >= 0.8:
        word_length = 5
    elif similarity_threshold >= 0.7:
        word_length = 4
    else:
        word_length = 3
    
    cmd = [
        "cd-hit-est",
        "-i", input_fasta,
        "-o", output_fasta,
        "-c", str(similarity_threshold),
        "-n", str(word_length),
        "-M", "2000",  # å†…å­˜é™åˆ¶2GB
        "-T", str(threads),
        "-d", "0"  # è¾“å‡ºå®Œæ•´çš„åºåˆ—æè¿°
    ]
    
    try:
        print(f"ğŸ”§ è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        print("âœ… CD-HITèšç±»å®Œæˆ!")
        print(f"ğŸ“Š ä»£è¡¨æ€§åºåˆ—æ–‡ä»¶: {output_fasta}")
        print(f"ğŸ“Š èšç±»ä¿¡æ¯æ–‡ä»¶: {cluster_file}")
        
        return output_fasta, cluster_file
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ CD-HITè¿è¡Œå¤±è´¥: {e}")
        print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        raise


def parse_cdhit_output(output_fasta: str) -> Set[str]:
    """
    è§£æCD-HITè¾“å‡ºæ–‡ä»¶ï¼Œè·å–ä»£è¡¨æ€§åºåˆ—çš„IDåˆ—è¡¨
    
    Args:
        output_fasta: CD-HITè¾“å‡ºçš„FASTAæ–‡ä»¶
        
    Returns:
        ä»£è¡¨æ€§åºåˆ—IDçš„é›†åˆ
    """
    representative_ids = set()
    
    print("ğŸ“‹ è§£æCD-HITè¾“å‡ºï¼Œè·å–ä»£è¡¨æ€§åºåˆ—...")
    
    try:
        with open(output_fasta, 'r') as f:
            for line in f:
                if line.startswith('>'):
                    # æå–åºåˆ—ID
                    seq_id = line.strip()[1:].split()[0]
                    representative_ids.add(seq_id)
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(representative_ids)} ä¸ªä»£è¡¨æ€§åºåˆ—")
        
    except Exception as e:
        print(f"âŒ è§£æCD-HITè¾“å‡ºå¤±è´¥: {e}")
        raise
    
    return representative_ids


def filter_by_similarity(filtered_sequences: Dict[str, Tuple[str, int, str]], 
                        similarity_threshold: float = 0.8,
                        threads: int = 4) -> Dict[str, Tuple[str, int, str]]:
    """
    ä½¿ç”¨CD-HITè¿›è¡Œåºåˆ—ç›¸ä¼¼åº¦ç­›é€‰
    
    Args:
        filtered_sequences: é•¿åº¦ç­›é€‰åçš„åºåˆ—å­—å…¸
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        threads: çº¿ç¨‹æ•°
        
    Returns:
        å»å†—ä½™åçš„åºåˆ—å­—å…¸
    """
    if not filtered_sequences:
        return filtered_sequences
    
    print(f"\nğŸ§¬ ç¬¬ä¸‰æ­¥: åºåˆ—å»å†—ä½™ (ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold*100}%)")
    print("-" * 60)
    
    # åˆ›å»ºä¸´æ—¶FASTAæ–‡ä»¶
    temp_fasta = create_temp_fasta(filtered_sequences)
    
    try:
        # è¿è¡ŒCD-HIT
        output_fasta, cluster_file = run_cdhit_clustering(
            temp_fasta, similarity_threshold, threads
        )
        
        # è§£æè¾“å‡ºè·å–ä»£è¡¨æ€§åºåˆ—
        representative_ids = parse_cdhit_output(output_fasta)
        
        # ç­›é€‰å‡ºä»£è¡¨æ€§åºåˆ—
        deduplicated_sequences = {}
        for seq_id in representative_ids:
            if seq_id in filtered_sequences:
                deduplicated_sequences[seq_id] = filtered_sequences[seq_id]
        
        print("-" * 60)
        print(f"ğŸ“Š å»å†—ä½™ç»“æœ: è¾“å…¥ {len(filtered_sequences)} ä¸ªï¼Œè¾“å‡º {len(deduplicated_sequences)} ä¸ª")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in [temp_fasta, output_fasta, cluster_file]:
            if os.path.exists(temp_file):
                os.remove(temp_file)
        
        return deduplicated_sequences
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_fasta):
            os.remove(temp_fasta)
        raise


def copy_related_files(data_dir: str, filtered_sequences: Dict[str, Tuple[str, int, str]], 
                      output_dir: str) -> Dict[str, List[str]]:
    """
    å¤åˆ¶ç¬¦åˆæ¡ä»¶çš„åºåˆ—ç›¸å…³çš„æ‰€æœ‰æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
    
    Args:
        data_dir: æºæ•°æ®ç›®å½•
        filtered_sequences: ç­›é€‰åçš„åºåˆ—å­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        å¤åˆ¶æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
    """
    source_path = Path(data_dir)
    target_path = Path(output_dir)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    subdirs = ["sequences", "seq", "pdb", "rMSA", "cache", "list"]
    for subdir in subdirs:
        (target_path / subdir).mkdir(parents=True, exist_ok=True)
    
    copy_stats = defaultdict(list)
    
    print("ğŸšš å¼€å§‹å¤åˆ¶æ–‡ä»¶...")
    print("-" * 80)
    
    for basename, (file_path, length, file_type) in filtered_sequences.items():
        print(f"\nğŸ“ å¤„ç† {basename} ({length}nt):")
        
        # å¤åˆ¶åŸå§‹åºåˆ—æ–‡ä»¶
        if file_type == 'fasta':
            source_file = source_path / "sequences" / f"{basename}.fasta"
            target_file = target_path / "sequences" / f"{basename}.fasta"
            if source_file.exists():
                shutil.copy2(str(source_file), str(target_file))
                print(f"  âœ… å¤åˆ¶ sequences/{basename}.fasta")
                copy_stats["sequences"].append(basename)
        elif file_type == 'seq':
            source_file = source_path / "seq" / f"{basename}.seq"
            target_file = target_path / "seq" / f"{basename}.seq"
            if source_file.exists():
                shutil.copy2(str(source_file), str(target_file))
                print(f"  âœ… å¤åˆ¶ seq/{basename}.seq")
                copy_stats["seq"].append(basename)
        
        # å¤åˆ¶ç›¸å…³çš„PDBæ–‡ä»¶
        pdb_source = source_path / "pdb" / f"{basename}.pdb"
        if pdb_source.exists():
            pdb_target = target_path / "pdb" / f"{basename}.pdb"
            shutil.copy2(str(pdb_source), str(pdb_target))
            print(f"  âœ… å¤åˆ¶ pdb/{basename}.pdb")
            copy_stats["pdb"].append(basename)
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ° pdb/{basename}.pdb")
        
        # å¤åˆ¶MSAæ–‡ä»¶
        msa_source = source_path / "rMSA" / f"{basename}.a3m"
        if msa_source.exists():
            msa_target = target_path / "rMSA" / f"{basename}.a3m"
            shutil.copy2(str(msa_source), str(msa_target))
            print(f"  âœ… å¤åˆ¶ rMSA/{basename}.a3m")
            copy_stats["rMSA"].append(basename)
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ° rMSA/{basename}.a3m")
        
        # å¤åˆ¶cacheæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        for cache_file in (source_path / "cache").glob(f"*{basename}*"):
            cache_target = target_path / "cache" / cache_file.name
            shutil.copy2(str(cache_file), str(cache_target))
            print(f"  âœ… å¤åˆ¶ cache/{cache_file.name}")
            copy_stats["cache"].append(cache_file.name)
    
    # å¤åˆ¶listæ–‡ä»¶å¤¹çš„å†…å®¹
    list_source = source_path / "list"
    list_target = target_path / "list"
    if list_source.exists():
        print(f"\nğŸ“ å¤åˆ¶listç›®å½•å†…å®¹...")
        for list_file in list_source.glob("*"):
            if list_file.is_file():
                shutil.copy2(str(list_file), str(list_target / list_file.name))
                print(f"  âœ… å¤åˆ¶ list/{list_file.name}")
                copy_stats["list"].append(list_file.name)
    
    return dict(copy_stats)


def generate_summary_report(original_count: int, length_filtered_count: int,
                           final_sequences: Dict[str, Tuple[str, int, str]], 
                           copy_stats: Dict[str, List[str]], 
                           similarity_threshold: float,
                           output_file: str = "filter_summary.txt"):
    """ç”Ÿæˆç­›é€‰å’Œå¤åˆ¶çš„æ±‡æ€»æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("RNAæ•°æ®ç­›é€‰æ±‡æ€»æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        f.write(f"ç­›é€‰æ¡ä»¶:\n")
        f.write(f"  - é•¿åº¦èŒƒå›´: 16-256nt\n")
        f.write(f"  - åºåˆ—ç›¸ä¼¼åº¦: <{similarity_threshold*100}%\n\n")
        
        f.write(f"ç­›é€‰æµç¨‹:\n")
        f.write(f"  1. åŸå§‹åºåˆ—æ•°: {original_count}\n")
        f.write(f"  2. é•¿åº¦ç­›é€‰å: {length_filtered_count}\n")
        f.write(f"  3. å»å†—ä½™å: {len(final_sequences)}\n\n")
        
        f.write("é•¿åº¦åˆ†å¸ƒ:\n")
        f.write("-" * 40 + "\n")
        
        # ç»Ÿè®¡é•¿åº¦åˆ†å¸ƒ
        length_counts = defaultdict(int)
        for _, (_, length, _) in final_sequences.items():
            length_range = f"{(length//10)*10}-{(length//10)*10+9}"
            length_counts[length_range] += 1
        
        for length_range in sorted(length_counts.keys(), key=lambda x: int(x.split('-')[0])):
            f.write(f"{length_range}nt: {length_counts[length_range]} ä¸ª\n")
        
        f.write("\nå¤åˆ¶æ–‡ä»¶ç»Ÿè®¡:\n")
        f.write("-" * 40 + "\n")
        for file_type, files in copy_stats.items():
            f.write(f"{file_type}: {len(files)} ä¸ªæ–‡ä»¶\n")
        
        f.write("\næœ€ç»ˆä¿ç•™çš„åºåˆ—åˆ—è¡¨:\n")
        f.write("-" * 40 + "\n")
        f.write(f"{'åºåˆ—å':<20} {'é•¿åº¦':<8} {'ç±»å‹':<8}\n")
        f.write("-" * 40 + "\n")
        
        for basename, (_, length, file_type) in sorted(final_sequences.items()):
            f.write(f"{basename:<20} {length:<8} {file_type:<8}\n")
    
    print(f"ğŸ“„ å·²ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ ¹æ®åºåˆ—é•¿åº¦ç­›é€‰RNAæ•°æ®å¹¶è¿›è¡Œå»å†—ä½™")
    
    parser.add_argument("--input_dir", type=str, default="./RNA3D_DATA", 
                       help="è¾“å…¥æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ./RNA3D_DATA)")
    parser.add_argument("--output_dir", type=str, default="./processed_data",
                       help="è¾“å‡ºæ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ./processed_data)")
    parser.add_argument("--min_length", type=int, default=16,
                       help="æœ€å°åºåˆ—é•¿åº¦ (é»˜è®¤: 16)")
    parser.add_argument("--max_length", type=int, default=256,
                       help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 256)")
    parser.add_argument("--similarity", type=float, default=0.8,
                       help="åºåˆ—ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: 0.8, å³80%)")
    parser.add_argument("--threads", type=int, default=4,
                       help="CD-HITä½¿ç”¨çš„çº¿ç¨‹æ•° (é»˜è®¤: 4)")
    parser.add_argument("--dry_run", action="store_true",
                       help="åªåˆ†æä¸å¤åˆ¶æ–‡ä»¶")
    parser.add_argument("--skip_similarity", action="store_true",
                       help="è·³è¿‡ç›¸ä¼¼åº¦ç­›é€‰ï¼Œåªè¿›è¡Œé•¿åº¦ç­›é€‰")
    parser.add_argument("--report", type=str, default="filter_summary.txt",
                       help="æ±‡æ€»æŠ¥å‘Šæ–‡ä»¶å (é»˜è®¤: filter_summary.txt)")
    
    args = parser.parse_args()
    
    print("ğŸ§¬ RNAæ•°æ®ç­›é€‰å·¥å…· (é•¿åº¦ + ç›¸ä¼¼åº¦)")
    print("=" * 60)
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“ é•¿åº¦èŒƒå›´: {args.min_length}-{args.max_length}nt")
    print(f"ğŸ”¬ ç›¸ä¼¼åº¦é˜ˆå€¼: <{args.similarity*100}% {'(è·³è¿‡)' if args.skip_similarity else ''}")
    print(f"ğŸ§µ çº¿ç¨‹æ•°: {args.threads}")
    print(f"ğŸš« ä»…åˆ†ææ¨¡å¼: {args.dry_run}")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not Path(args.input_dir).exists():
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        
        # æ‰«ææ‰€æœ‰åºåˆ—æ–‡ä»¶
        print("\nğŸ” ç¬¬ä¸€æ­¥: æ‰«ææ‰€æœ‰åºåˆ—æ–‡ä»¶...")
        all_sequences = scan_all_sequences(args.input_dir)
        print(f"ğŸ“Š æ‰¾åˆ° {len(all_sequences)} ä¸ªåºåˆ—æ–‡ä»¶")
        original_count = len(all_sequences)
        
        # æ ¹æ®é•¿åº¦ç­›é€‰
        print("\nğŸ” ç¬¬äºŒæ­¥: æ ¹æ®é•¿åº¦ç­›é€‰...")
        length_filtered = filter_by_length(
            all_sequences, args.min_length, args.max_length
        )
        length_filtered_count = len(length_filtered)
        
        # åºåˆ—ç›¸ä¼¼åº¦ç­›é€‰
        if not args.skip_similarity and length_filtered:
            final_sequences = filter_by_similarity(
                length_filtered, args.similarity, args.threads
            )
        else:
            final_sequences = length_filtered
            if args.skip_similarity:
                print("\nâ­ï¸  è·³è¿‡ç›¸ä¼¼åº¦ç­›é€‰")
        
        if not args.dry_run and final_sequences:
            # å¤åˆ¶æ–‡ä»¶
            print(f"\nğŸšš ç¬¬{'å››' if not args.skip_similarity else 'ä¸‰'}æ­¥: å¤åˆ¶ç›¸å…³æ–‡ä»¶...")
            copy_stats = copy_related_files(
                args.input_dir, final_sequences, args.output_dir
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            generate_summary_report(
                original_count, length_filtered_count, 
                final_sequences, copy_stats, args.similarity, args.report
            )
            
            print("\n" + "=" * 60)
            print("âœ… æ•°æ®ç­›é€‰å’Œå¤åˆ¶å®Œæˆ!")
            print(f"ğŸ“Š åŸå§‹åºåˆ—: {original_count} ä¸ª")
            print(f"ğŸ“Š é•¿åº¦ç­›é€‰å: {length_filtered_count} ä¸ª")
            print(f"ğŸ“Š å»å†—ä½™å: {len(final_sequences)} ä¸ª")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
            print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Š: {args.report}")
            
        elif args.dry_run:
            print("\n" + "=" * 60)
            print("ğŸš« ä»…åˆ†ææ¨¡å¼ - æœªå¤åˆ¶æ–‡ä»¶")
            print(f"ğŸ“Š åŸå§‹åºåˆ—: {original_count} ä¸ª")
            print(f"ğŸ“Š é•¿åº¦ç­›é€‰å: {length_filtered_count} ä¸ª")
            print(f"ğŸ“Š å»å†—ä½™å: {len(final_sequences)} ä¸ª")
            
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            if final_sequences:
                lengths = [length for _, length, _ in final_sequences.values()]
                print(f"ğŸ“ é•¿åº¦èŒƒå›´: {min(lengths)}-{max(lengths)}nt")
                print(f"ğŸ“ å¹³å‡é•¿åº¦: {sum(lengths)/len(lengths):.1f}nt")
        else:
            print("\n" + "=" * 60)
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„åºåˆ—")
    
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

