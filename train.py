#!/usr/bin/env python3
"""
Diffoldæ¨¡å‹è®­ç»ƒè„šæœ¬
åŒ…å«ï¼šæ–­ç‚¹ä¿å­˜ã€è¿›åº¦æ˜¾ç¤ºã€è®­ç»ƒæ›²çº¿ç»˜åˆ¶ã€å°è§„æ¨¡æµ‹è¯•ç­‰åŠŸèƒ½
"""

import os
import sys
import json
import time
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®æ¨¡å—
from diffold.diffold import Diffold
from diffold.dataloader import RNA3DDataLoader
from rhofold.config import rhofold_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
    ]
)
logger = logging.getLogger(__name__)


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    
    def __init__(self):
        # æ•°æ®é…ç½®
        self.data_dir = "./processed_data"
        self.batch_size = 4
        self.max_sequence_length = 256
        self.num_workers = 4
        self.use_msa = True
        
        # æ¨¡å‹é…ç½®
        self.rhofold_checkpoint = "./pretrained/model_20221010_params.pt"
        
        # è®­ç»ƒé…ç½®
        self.num_epochs = 100
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.grad_clip_norm = 1.0
        self.warmup_epochs = 5
        
        # è°ƒåº¦å™¨é…ç½®
        self.scheduler_type = "cosine"  # "cosine", "plateau"
        self.patience = 10
        
        # ä¿å­˜é…ç½®
        self.output_dir = "./output"
        self.checkpoint_dir = "./checkpoints"
        self.save_every = 5
        self.keep_last_n_checkpoints = 5
        
        # éªŒè¯é…ç½®
        self.validate_every = 1
        self.early_stopping_patience = 20
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.mixed_precision = True
        
        # å¤šGPUé…ç½®
        self.use_data_parallel = True  # æ˜¯å¦ä½¿ç”¨DataParallel
        self.gpu_ids = None  # æŒ‡å®šä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        
        # å°è§„æ¨¡æµ‹è¯•é…ç½®
        self.test_mode = False
        self.test_samples = 10
        self.test_epochs = 5
        
        # äº¤å‰éªŒè¯é…ç½®
        self.fold = 0
        self.num_folds = 10
        self.use_all_folds = False  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨æ‰€æœ‰æŠ˜æ•°


class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡è®°å½•ç±»"""
    
    def __init__(self):
        self.train_losses = []
        self.valid_losses = []
        self.learning_rates = []
        self.epoch_times = []
        
        self.best_valid_loss = float('inf')
        self.best_epoch = 0
        self.early_stopping_counter = 0
    
    def update_train(self, loss: float, lr: float, epoch_time: float):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.train_losses.append(loss)
        self.learning_rates.append(lr)
        self.epoch_times.append(epoch_time)
    
    def update_valid(self, loss: float, epoch: int):
        """æ›´æ–°éªŒè¯æŒ‡æ ‡"""
        self.valid_losses.append(loss)
        
        if loss < self.best_valid_loss:
            self.best_valid_loss = loss
            self.best_epoch = epoch
            self.early_stopping_counter = 0
            return True  # æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹
        else:
            self.early_stopping_counter += 1
            return False
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'train_losses': self.train_losses,
            'valid_losses': self.valid_losses,
            'learning_rates': self.learning_rates,
            'epoch_times': self.epoch_times,
            'best_valid_loss': self.best_valid_loss,
            'best_epoch': self.best_epoch,
            'early_stopping_counter': self.early_stopping_counter
        }


class DiffoldTrainer:
    """Diffoldæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.metrics = TrainingMetrics()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.setup_directories()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device(config.device)
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self.setup_model()
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.setup_data_loaders()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.setup_optimizer_and_scheduler()
        
        # åˆå§‹åŒ–tensorboard
        self.writer = SummaryWriter(log_dir=self.config.output_dir / "tensorboard")
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.mixed_precision and self.device.type == 'cuda':
            self.scaler = torch.cuda.amp.GradScaler()
            logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            self.scaler = None
        
        # è®°å½•å¼€å§‹æ—¶é—´
        self.start_time = time.time()
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        self.config.output_dir = Path(self.config.output_dir)
        self.config.checkpoint_dir = Path(self.config.checkpoint_dir)
        
        self.config.output_dir.mkdir(exist_ok=True)
        self.config.checkpoint_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.config.output_dir / "plots").mkdir(exist_ok=True)
        (self.config.output_dir / "tensorboard").mkdir(exist_ok=True)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        # æ·»åŠ æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
        log_file = self.config.output_dir / "training.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        logger.info("="*60)
        logger.info("å¼€å§‹Diffoldæ¨¡å‹è®­ç»ƒ")
        logger.info("="*60)
        
    def setup_model(self) -> nn.Module:
        """åˆå§‹åŒ–æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–Diffoldæ¨¡å‹...")
        
        # ä½¿ç”¨RhoFoldé…ç½®
        model = Diffold(
            config=rhofold_config,
            rhofold_checkpoint_path=self.config.rhofold_checkpoint
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(self.device)
        
        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        model.set_train_mode()
        
        # å¤šGPUè®¾ç½®
        if (self.config.use_data_parallel and 
            self.device.type == 'cuda' and 
            torch.cuda.device_count() > 1):
            
            # æ£€æµ‹å¯ç”¨GPU
            available_gpus = torch.cuda.device_count()
            logger.info(f"æ£€æµ‹åˆ° {available_gpus} ä¸ªGPU")
            
            # ç¡®å®šä½¿ç”¨çš„GPU
            if self.config.gpu_ids is None:
                gpu_ids = list(range(available_gpus))
            else:
                gpu_ids = self.config.gpu_ids
                # éªŒè¯GPU IDçš„æœ‰æ•ˆæ€§
                for gpu_id in gpu_ids:
                    if gpu_id >= available_gpus:
                        raise ValueError(f"GPU ID {gpu_id} ä¸å­˜åœ¨ï¼Œåªæœ‰ {available_gpus} ä¸ªGPU")
            
            logger.info(f"ä½¿ç”¨GPU: {gpu_ids}")
            
            # åŒ…è£…æ¨¡å‹ä¸ºDataParallel
            model = nn.DataParallel(model, device_ids=gpu_ids)
            
            # æ›´æ–°æœ‰æ•ˆbatch size
            effective_batch_size = self.config.batch_size * len(gpu_ids)
            logger.info(f"DataParallelå¯ç”¨ï¼šæ¯GPU batch_size={self.config.batch_size}, æœ‰æ•ˆbatch_size={effective_batch_size}")
            
            # æ ‡è®°æ˜¯å¦ä½¿ç”¨äº†DataParallel
            self.using_data_parallel = True
            self.num_gpus = len(gpu_ids)
        else:
            if self.device.type == 'cuda':
                logger.info("ä½¿ç”¨å•GPUè®­ç»ƒ")
            else:
                logger.info("ä½¿ç”¨CPUè®­ç»ƒ")
            self.using_data_parallel = False
            self.num_gpus = 1
        
        # è·å–å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        if self.using_data_parallel:
            trainable_params = model.module.get_trainable_parameters()
        else:
            trainable_params = model.get_trainable_parameters()
        
        total_params = sum(p.numel() for p in trainable_params)
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {total_params:,}")
        
        return model
    
    def setup_data_loaders(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        
        if self.config.test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šä¿æŒç”¨æˆ·è®¾ç½®çš„batch_sizeï¼Œä½†é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦
            batch_size = self.config.batch_size
            max_length = min(128, self.config.max_sequence_length)
        else:
            batch_size = self.config.batch_size
            max_length = self.config.max_sequence_length
        
        data_loader = RNA3DDataLoader(
            data_dir=self.config.data_dir,
            batch_size=batch_size,
            max_length=max_length,
            use_msa=self.config.use_msa,
            num_workers=self.config.num_workers,
            force_reload=False,
            enable_missing_atom_mask=True
        )
        
        if self.config.use_all_folds:
            # ä½¿ç”¨æ‰€æœ‰æŠ˜æ•°çš„æ•°æ®
            logger.info("ä½¿ç”¨æ‰€æœ‰æŠ˜æ•°çš„è®­ç»ƒæ•°æ®...")
            all_loaders = data_loader.get_all_folds_dataloaders()
            
            # åˆå¹¶æ‰€æœ‰æŠ˜æ•°çš„è®­ç»ƒæ•°æ®åŠ è½½å™¨
            from torch.utils.data import ConcatDataset, DataLoader
            from diffold.dataloader import collate_fn
            
            all_train_datasets = []
            all_valid_datasets = []
            
            for fold, loaders in all_loaders.items():
                train_dataset = loaders['train'].dataset
                valid_dataset = loaders['valid'].dataset
                all_train_datasets.append(train_dataset)
                all_valid_datasets.append(valid_dataset)
                
                # å®‰å…¨è·å–æ•°æ®é›†å¤§å°
                if hasattr(train_dataset, '__len__') and hasattr(valid_dataset, '__len__'):
                    train_size = len(train_dataset)  # type: ignore
                    valid_size = len(valid_dataset)  # type: ignore
                    logger.info(f"Fold {fold}: è®­ç»ƒæ ·æœ¬ {train_size}, éªŒè¯æ ·æœ¬ {valid_size}")
                else:
                    logger.info(f"Fold {fold}: å·²æ·»åŠ åˆ°åˆå¹¶æ•°æ®é›†")
            
            # åˆå¹¶æ•°æ®é›†
            combined_train_dataset = ConcatDataset(all_train_datasets)
            combined_valid_dataset = ConcatDataset(all_valid_datasets)
            
            # åˆ›å»ºæ–°çš„æ•°æ®åŠ è½½å™¨
            self.train_loader = DataLoader(
                combined_train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=self.config.num_workers,
                collate_fn=collate_fn,
                pin_memory=True
            )
            
            self.valid_loader = DataLoader(
                combined_valid_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=self.config.num_workers,
                collate_fn=collate_fn,
                pin_memory=True
            )
            
            logger.info(f"åˆå¹¶åè®­ç»ƒæ ·æœ¬æ•°: {len(combined_train_dataset)}")
            logger.info(f"åˆå¹¶åéªŒè¯æ ·æœ¬æ•°: {len(combined_valid_dataset)}")
            
        else:
            # ä½¿ç”¨å•ä¸ªæŠ˜æ•°çš„æ•°æ®
            self.train_loader = data_loader.get_train_dataloader(fold=self.config.fold)
            self.valid_loader = data_loader.get_valid_dataloader(fold=self.config.fold)
            
            # å®‰å…¨è·å–æ•°æ®é›†å¤§å°
            if hasattr(self.train_loader.dataset, '__len__'):
                train_size = len(self.train_loader.dataset)  # type: ignore
                logger.info(f"Fold {self.config.fold} - è®­ç»ƒæ ·æœ¬æ•°: {train_size}")
            
            if hasattr(self.valid_loader.dataset, '__len__'):
                valid_size = len(self.valid_loader.dataset)  # type: ignore
                logger.info(f"Fold {self.config.fold} - éªŒè¯æ ·æœ¬æ•°: {valid_size}")
        
        if self.config.test_mode:
            logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶ä¸ºå‰{self.config.test_samples}ä¸ªæ ·æœ¬")
    
    def setup_optimizer_and_scheduler(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # è·å–å¯è®­ç»ƒå‚æ•°
        if self.using_data_parallel:
            trainable_params = self.model.module.get_trainable_parameters()
        else:
            trainable_params = self.model.get_trainable_parameters()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            trainable_params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.scheduler_type == "cosine":
            self.scheduler = CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs,
                eta_min=self.config.learning_rate * 0.01
            )
        elif self.config.scheduler_type == "plateau":
            self.scheduler = ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=self.config.patience,
                verbose=True
            )
        else:
            self.scheduler = None
        
        logger.info(f"ä¼˜åŒ–å™¨: AdamW (lr={self.config.learning_rate})")
        logger.info(f"è°ƒåº¦å™¨: {self.config.scheduler_type}")
    
    def train_one_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.set_train_mode()
        
        total_loss = 0.0
        num_batches = 0
        
        # æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶batchæ•°é‡
        if self.config.test_mode:
            max_batches = min(self.config.test_samples // self.config.batch_size + 1, 5)
        else:
            max_batches = len(self.train_loader)
        
        progress_bar = tqdm(
            enumerate(self.train_loader),
            total=min(max_batches, len(self.train_loader)),
            desc=f"Epoch {epoch+1}/{self.config.num_epochs}",
            leave=False
        )
        
        for batch_idx, batch in progress_bar:
            if self.config.test_mode and batch_idx >= max_batches:
                break
                
            try:
                loss = self.train_step(batch)
                
                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):
                    total_loss += loss.item()
                    num_batches += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix({
                        'loss': f"{loss.item():.6f}",
                        'avg_loss': f"{total_loss/num_batches:.6f}"
                    })
                else:
                    logger.warning(f"Batch {batch_idx}: æ— æ•ˆæŸå¤±ï¼Œè·³è¿‡")
                    
            except Exception as e:
                logger.warning(f"Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def train_step(self, batch: Dict) -> Optional[torch.Tensor]:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        tokens = batch['tokens'].to(self.device)
        sequences = batch['sequences']
        coordinates = batch.get('coordinates', None)
        missing_atom_masks = batch.get('missing_atom_masks', None)
        
        if coordinates is not None:
            coordinates = coordinates.to(self.device)
        if missing_atom_masks is not None:
            missing_atom_masks = missing_atom_masks.to(self.device)
        
        # rna_fm_tokenså¤„ç†
        rna_fm_tokens = batch.get('rna_fm_tokens', None)
        if rna_fm_tokens is not None:
            rna_fm_tokens = rna_fm_tokens.to(self.device)
        
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        if self.scaler is not None:
            with torch.cuda.amp.autocast():
                result = self.model(
                    tokens=tokens,
                    rna_fm_tokens=rna_fm_tokens,
                    seq=sequences,
                    target_coords=coordinates,
                    missing_atom_mask=missing_atom_masks
                )
        else:
            result = self.model(
                tokens=tokens,
                rna_fm_tokens=rna_fm_tokens,
                seq=sequences,
                target_coords=coordinates,
                missing_atom_mask=missing_atom_masks
            )
        
        # å¤„ç†æ¨¡å‹è¾“å‡º
        if result is None:
            return None
        
        # æ–°çš„å­—å…¸æ ¼å¼è¿”å›å€¼
        if isinstance(result, dict):
            loss = result.get('loss', None)
            if loss is None:
                return None
        elif isinstance(result, tuple):
            # å…¼å®¹æ—§æ ¼å¼
            loss = result[0]
        else:
            loss = result
        
        # åå‘ä¼ æ’­
        if self.scaler is not None:
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            if self.using_data_parallel:
                torch.nn.utils.clip_grad_norm_(self.model.module.get_trainable_parameters(), self.config.grad_clip_norm)
            else:
                torch.nn.utils.clip_grad_norm_(self.model.get_trainable_parameters(), self.config.grad_clip_norm)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            loss.backward()
            if self.using_data_parallel:
                torch.nn.utils.clip_grad_norm_(self.model.module.get_trainable_parameters(), self.config.grad_clip_norm)
            else:
                torch.nn.utils.clip_grad_norm_(self.model.get_trainable_parameters(), self.config.grad_clip_norm)
            self.optimizer.step()
        
        return loss
    
    def validate(self, epoch: int) -> float:
        """éªŒè¯æ¨¡å‹"""
        self.model.set_eval_mode()
        
        total_loss = 0.0
        num_batches = 0
        
        # æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶batchæ•°é‡
        if self.config.test_mode:
            max_batches = min(3, len(self.valid_loader))
        else:
            max_batches = len(self.valid_loader)
        
        with torch.no_grad():
            progress_bar = tqdm(
                enumerate(self.valid_loader),
                total=min(max_batches, len(self.valid_loader)),
                desc="éªŒè¯",
                leave=False
            )
            
            for batch_idx, batch in progress_bar:
                if self.config.test_mode and batch_idx >= max_batches:
                    break
                    
                try:
                    # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    tokens = batch['tokens'].to(self.device)
                    sequences = batch['sequences']
                    coordinates = batch.get('coordinates', None)
                    missing_atom_masks = batch.get('missing_atom_masks', None)
                    
                    if coordinates is not None:
                        coordinates = coordinates.to(self.device)
                    if missing_atom_masks is not None:
                        missing_atom_masks = missing_atom_masks.to(self.device)
                    
                    # rna_fm_tokenså¤„ç†
                    rna_fm_tokens = batch.get('rna_fm_tokens', None)
                    if rna_fm_tokens is not None:
                        rna_fm_tokens = rna_fm_tokens.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    result = self.model(
                        tokens=tokens,
                        rna_fm_tokens=rna_fm_tokens,
                        seq=sequences,
                        target_coords=coordinates,
                        missing_atom_mask=missing_atom_masks
                    )
                    
                    if result is not None:
                        # æ–°çš„å­—å…¸æ ¼å¼è¿”å›å€¼
                        if isinstance(result, dict):
                            loss = result.get('loss', None)
                        elif isinstance(result, tuple):
                            # å…¼å®¹æ—§æ ¼å¼
                            loss = result[0]
                        else:
                            loss = result
                        
                        if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):
                            total_loss += loss.item()
                            num_batches += 1
                            
                            progress_bar.set_postfix({'val_loss': f"{loss.item():.6f}"})
                
                except Exception as e:
                    logger.warning(f"éªŒè¯ Batch {batch_idx} å¤±è´¥: {e}")
                    continue
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # å¤„ç†DataParallelçš„state_dict
        if self.using_data_parallel:
            model_state_dict = self.model.module.state_dict()
        else:
            model_state_dict = self.model.state_dict()
            
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state_dict,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'metrics': self.metrics.to_dict(),
            'config': self.config.__dict__,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'using_data_parallel': self.using_data_parallel,
            'num_gpus': self.num_gpus,
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.checkpoint_dir / f"checkpoint_epoch_{epoch:03d}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.config.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self.cleanup_old_checkpoints()
        
        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    def cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoint_files = list(self.config.checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        if len(checkpoint_files) > self.config.keep_last_n_checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            checkpoint_files.sort(key=lambda x: x.stat().st_mtime)
            # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
            for old_file in checkpoint_files[:-self.config.keep_last_n_checkpoints]:
                old_file.unlink()
                logger.debug(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_file}")
    
    def load_checkpoint(self, checkpoint_path: str) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0
        
        logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€ï¼ˆå¤„ç†DataParallelï¼‰
        model_state_dict = checkpoint['model_state_dict']
        if self.using_data_parallel:
            self.model.module.load_state_dict(model_state_dict)
        else:
            self.model.load_state_dict(model_state_dict)
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint.get('scheduler_state_dict'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.scaler and checkpoint.get('scaler_state_dict'):
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        # æ¢å¤è®­ç»ƒæŒ‡æ ‡
        if 'metrics' in checkpoint:
            metrics_dict = checkpoint['metrics']
            self.metrics.train_losses = metrics_dict.get('train_losses', [])
            self.metrics.valid_losses = metrics_dict.get('valid_losses', [])
            self.metrics.learning_rates = metrics_dict.get('learning_rates', [])
            self.metrics.epoch_times = metrics_dict.get('epoch_times', [])
            self.metrics.best_valid_loss = metrics_dict.get('best_valid_loss', float('inf'))
            self.metrics.best_epoch = metrics_dict.get('best_epoch', 0)
            self.metrics.early_stopping_counter = metrics_dict.get('early_stopping_counter', 0)
        
        start_epoch = checkpoint['epoch'] + 1
        logger.info(f"ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ")
        return start_epoch
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.metrics.train_losses:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        epochs = range(1, len(self.metrics.train_losses) + 1)
        axes[0, 0].plot(epochs, self.metrics.train_losses, 'b-', label='Training Loss')
        if self.metrics.valid_losses:
            valid_epochs = range(1, len(self.metrics.valid_losses) + 1)
            axes[0, 0].plot(valid_epochs, self.metrics.valid_losses, 'r-', label='Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if self.metrics.learning_rates:
            axes[0, 1].plot(epochs, self.metrics.learning_rates, 'g-')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Learning Rate')
            axes[0, 1].set_title('Learning Rate Schedule')
            axes[0, 1].grid(True)
        
        # è®­ç»ƒæ—¶é—´
        if self.metrics.epoch_times:
            axes[1, 0].plot(epochs, self.metrics.epoch_times, 'orange')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Time (seconds)')
            axes[1, 0].set_title('Epoch Training Time')
            axes[1, 0].grid(True)
        
        # æŸå¤±åˆ†å¸ƒï¼ˆæœ€è¿‘10ä¸ªepochï¼‰
        if len(self.metrics.train_losses) > 1:
            recent_losses = self.metrics.train_losses[-10:]
            axes[1, 1].hist(recent_losses, bins=min(10, len(recent_losses)), alpha=0.7, color='blue')
            axes[1, 1].set_xlabel('Loss')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Recent Training Loss Distribution')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = self.config.output_dir / "plots" / f"training_curves_epoch_{len(self.metrics.train_losses)}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Saved training curves: {plot_path}")
    
    def train(self, resume_from: Optional[str] = None):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        start_epoch = 0
        if resume_from:
            start_epoch = self.load_checkpoint(resume_from)
        
        # è®­ç»ƒå¾ªç¯
        num_epochs = self.config.test_epochs if self.config.test_mode else self.config.num_epochs
        
        for epoch in range(start_epoch, num_epochs):
            epoch_start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self.train_one_epoch(epoch)
            
            # éªŒè¯
            valid_loss = None
            if epoch % self.config.validate_every == 0:
                valid_loss = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler is not None:
                if self.config.scheduler_type == "plateau" and valid_loss is not None:
                    self.scheduler.step(valid_loss)
                elif self.config.scheduler_type == "cosine":
                    self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            current_lr = self.optimizer.param_groups[0]['lr']
            epoch_time = time.time() - epoch_start_time
            
            self.metrics.update_train(train_loss, current_lr, epoch_time)
            
            is_best = False
            if valid_loss is not None:
                is_best = self.metrics.update_valid(valid_loss, epoch)
            
            # è®°å½•åˆ°tensorboard
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            if valid_loss is not None:
                self.writer.add_scalar('Loss/Valid', valid_loss, epoch)
            self.writer.add_scalar('LearningRate', current_lr, epoch)
            self.writer.add_scalar('EpochTime', epoch_time, epoch)
            
            # è®¡ç®—é¢„è®¡æ—¶é—´
            current_time = time.time()
            elapsed_time = current_time - self.start_time
            
            # è®¡ç®—å¹³å‡epochæ—¶é—´ï¼ˆä½¿ç”¨æœ€è¿‘çš„epochæ—¶é—´æ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ï¼‰
            if len(self.metrics.epoch_times) > 0:
                # ä½¿ç”¨æœ€è¿‘5ä¸ªepochçš„å¹³å‡æ—¶é—´ï¼Œå¦‚æœä¸è¶³5ä¸ªåˆ™ä½¿ç”¨æ‰€æœ‰çš„
                recent_times = self.metrics.epoch_times[-5:]
                avg_epoch_time = sum(recent_times) / len(recent_times)
            else:
                avg_epoch_time = epoch_time
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            remaining_epochs = num_epochs - (epoch + 1)
            estimated_remaining_time = remaining_epochs * avg_epoch_time
            estimated_completion_time = current_time + estimated_remaining_time
            
            # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
            def format_time(seconds):
                if seconds < 60:
                    return f"{seconds:.1f}ç§’"
                elif seconds < 3600:
                    minutes = seconds / 60
                    return f"{minutes:.1f}åˆ†é’Ÿ"
                else:
                    hours = seconds / 3600
                    return f"{hours:.1f}å°æ—¶"
            
            def format_datetime(timestamp):
                return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
            
            # æ‰“å°è¿›åº¦
            log_msg = f"Epoch {epoch+1}/{num_epochs} - "
            log_msg += f"è®­ç»ƒæŸå¤±: {train_loss:.6f}, "
            if valid_loss is not None:
                log_msg += f"éªŒè¯æŸå¤±: {valid_loss:.6f}, "
            log_msg += f"å­¦ä¹ ç‡: {current_lr:.2e}, "
            log_msg += f"æ—¶é—´: {epoch_time:.1f}s"
            
            if is_best:
                log_msg += " â­ æœ€ä½³æ¨¡å‹!"
            
            logger.info(log_msg)
            
            # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡ä¿¡æ¯
            time_msg = f"â±ï¸  å·²ç”¨æ—¶é—´: {format_time(elapsed_time)}, "
            time_msg += f"é¢„è®¡å‰©ä½™: {format_time(estimated_remaining_time)}, "
            time_msg += f"é¢„è®¡å®Œæˆ: {format_datetime(estimated_completion_time)}"
            logger.info(time_msg)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_every == 0:
                self.save_checkpoint(epoch, is_best)
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            if (epoch + 1) % (self.config.save_every * 2) == 0:
                self.plot_training_curves()
            
            # æ—©åœæ£€æŸ¥
            if (self.metrics.early_stopping_counter >= self.config.early_stopping_patience 
                and not self.config.test_mode):
                logger.info(f"æ—©åœè§¦å‘ (patience={self.config.early_stopping_patience})")
                break
        
        # è®­ç»ƒç»“æŸ
        total_time = time.time() - self.start_time
        logger.info("="*60)
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.metrics.best_valid_loss:.6f} (Epoch {self.metrics.best_epoch+1})")
        logger.info("="*60)
        
        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint(epoch, False)
        self.plot_training_curves()
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        metrics_path = self.config.output_dir / "training_metrics.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics.to_dict(), f, indent=2, ensure_ascii=False)
        
        self.writer.close()


def run_small_scale_test():
    """è¿è¡Œå°è§„æ¨¡æµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œå°è§„æ¨¡æµ‹è¯•...")
    
    # æµ‹è¯•é…ç½®
    config = TrainingConfig()
    config.test_mode = True
    config.test_epochs = 1
    config.test_samples = 6  # æ”¹å›åˆ°6ä¸ªæ ·æœ¬
    config.batch_size = 2    # ä½¿ç”¨batch_size=2ï¼Œé¿å…å•æ ·æœ¬é—®é¢˜
    config.max_sequence_length = 128  # ä½¿ç”¨æ›´åˆç†çš„åºåˆ—é•¿åº¦
    config.device = "cpu"
    config.num_workers = 0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    config.output_dir = "./test_output"
    config.checkpoint_dir = "./test_checkpoints"
    config.mixed_precision = False  # æµ‹è¯•æ—¶ç¦ç”¨æ··åˆç²¾åº¦
    config.use_data_parallel = False  # æµ‹è¯•æ—¶ç¦ç”¨å¤šGPU
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = DiffoldTrainer(config)
        
        # è¿è¡Œè®­ç»ƒ
        trainer.train()
        
        print("âœ… å°è§„æ¨¡æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
        print(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: {config.checkpoint_dir}")
        
    except Exception as e:
        print(f"âŒ å°è§„æ¨¡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Diffoldæ¨¡å‹è®­ç»ƒ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, default="./processed_data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½è¿›ç¨‹æ•°")
    parser.add_argument("--fold", type=int, default=0, help="äº¤å‰éªŒè¯æŠ˜æ•° (0-9)")
    parser.add_argument("--use_all_folds", action="store_true", help="ä½¿ç”¨æ‰€æœ‰æŠ˜æ•°çš„æ•°æ®è¿›è¡Œè®­ç»ƒ", default=False)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--rhofold_checkpoint", type=str, 
                       default="./pretrained/model_20221010_params.pt", 
                       help="RhoFoldé¢„è®­ç»ƒæƒé‡è·¯å¾„")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints", help="æ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--save_every", type=int, default=5, help="æ¯Nè½®ä¿å­˜ä¸€æ¬¡")
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (auto/cpu/cuda)")
    parser.add_argument("--no_mixed_precision", action="store_true", help="ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--no_data_parallel", action="store_true", help="ç¦ç”¨DataParallelå¤šGPUè®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=int, nargs='+', help="æŒ‡å®šä½¿ç”¨çš„GPU ID (ä¾‹å¦‚: --gpu_ids 0 1 2)")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--resume", type=str, default=None, help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œå°è§„æ¨¡æµ‹è¯•", default=True)
    
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼
    if args.test:
        run_small_scale_test()
        return
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig()
    
    # æ›´æ–°é…ç½®
    config.data_dir = args.data_dir
    config.batch_size = args.batch_size
    config.max_sequence_length = args.max_length
    config.num_workers = args.num_workers
    config.fold = args.fold
    config.use_all_folds = args.use_all_folds
    
    config.num_epochs = args.epochs
    config.learning_rate = args.learning_rate
    config.weight_decay = args.weight_decay
    config.grad_clip_norm = args.grad_clip
    
    config.rhofold_checkpoint = args.rhofold_checkpoint
    config.output_dir = args.output_dir
    config.checkpoint_dir = args.checkpoint_dir
    config.save_every = args.save_every
    
    if args.device == "auto":
        config.device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        config.device = args.device
    
    config.mixed_precision = not args.no_mixed_precision
    config.use_data_parallel = not args.no_data_parallel
    config.gpu_ids = args.gpu_ids
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DiffoldTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(resume_from=args.resume)


if __name__ == "__main__":
    main()
