#!/usr/bin/env python3
"""
Diffoldæ¨¡å‹è®­ç»ƒè„šæœ¬ - å¢å¼ºç‰ˆ
æ•´åˆäº†æ‰€æœ‰è®­ç»ƒä¿éšœæªæ–½å’Œä¼˜åŒ–åŠŸèƒ½
"""

import argparse
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, cast
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import os

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®å¤„ç†
from diffold.diffold import Diffold
from diffold.dataloader import create_data_loaders

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ”¥ å¯¼å…¥å¢å¼ºåŠŸèƒ½æ¨¡å—
try:
    from diffold.training_monitor import TrainingMonitor
    from diffold.advanced_optimizers import AdaptiveOptimizer, DataLoaderOptimizer, EvaluationMetrics
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError as e:
    logger.warning(f"âš ï¸ å¢å¼ºåŠŸèƒ½ä¸å¯ç”¨: {e}")
    logger.info("ğŸ’¡ å®‰è£…ä¾èµ–: pip install psutil matplotlib")
    ENHANCED_FEATURES_AVAILABLE = False

class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±» - å…¼å®¹åŸç‰ˆå’Œå¢å¼ºç‰ˆ"""
    
    def __init__(self):
        # åŸºç¡€æ•°æ®é…ç½®
        self.data_dir = "./processed_data"
        self.batch_size = 4
        self.max_sequence_length = 256
        self.num_workers = 4
        self.use_msa = True
        
        # æ¨¡å‹é…ç½®
        self.rhofold_checkpoint = "./pretrained/model_20221010_params.pt"
        
        # è®­ç»ƒé…ç½®
        self.num_epochs = 100
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.grad_clip_norm = 1.0
        self.warmup_epochs = 3  # ä¿®å¤: å‡å°‘é¢„çƒ­è½®æ•°ï¼Œé’ˆå¯¹100è½®ä¼˜åŒ–
        
        # è°ƒåº¦å™¨é…ç½®
        self.scheduler_type = "cosine"  # "cosine", "plateau", "warmup_cosine"
        self.patience = 10
        
        # ä¿å­˜é…ç½®
        self.output_dir = "./output"
        self.checkpoint_dir = "./checkpoints"
        self.save_every = 5
        self.keep_last_n_checkpoints = 5
        
        # éªŒè¯é…ç½®
        self.validate_every = 1
        self.early_stopping_patience = 20
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.mixed_precision = True
        # torch.compile é…ç½®
        self.use_torch_compile = False
        self.torch_compile_mode = 'default'  # å¯é€‰: 'default', 'reduce-overhead', 'max-autotune'
        
        # å¤šGPUé…ç½®
        self.use_data_parallel = True
        self.gpu_ids = None
        
        # å°è§„æ¨¡æµ‹è¯•é…ç½®
        self.test_mode = False
        self.test_samples = 10
        self.test_epochs = 5
        
        # äº¤å‰éªŒè¯é…ç½®
        self.fold = 0
        self.num_folds = 10
        self.use_all_folds = False
        
        # ğŸ”¥ å¢å¼ºåŠŸèƒ½é…ç½®
        self.enhanced_features = {
            'enable_enhanced_training': ENHANCED_FEATURES_AVAILABLE,  # æ€»å¼€å…³
            'monitoring': {
                'enable_performance_monitoring': True,
                'enable_memory_monitoring': True,
                'enable_health_checking': True,
                'monitoring_interval': 10,
                'save_monitoring_plots': True,
                'memory_cleanup_threshold': 0.85
            },
            'optimizer': {
                'use_advanced_optimizer': True,
                'optimizer_name': 'adamw',  # 'adamw', 'adam', 'sgd', 'lion'
                'gradient_accumulation_steps': 1,
                'scheduler_type': 'warmup_cosine'  # 'warmup_cosine', 'warmup_cosine_restarts', 'plateau'
            },
            'dataloader': {
                'enable_prefetch': True,
                'prefetch_factor': 2,
                'cache_size': 100,
                'pin_memory': True,
                'persistent_workers': True
            },
            'evaluation': {
                'compute_structure_metrics': True,
                'compute_confidence_metrics': True,
                'save_predictions': False
            },
            'error_recovery': {
                'auto_retry_on_oom': True,
                'max_retry_attempts': 3,
                'reduce_batch_size_on_oom': True
            }
        }
    
    def apply_enhanced_preset(self, preset_name: str):
        """åº”ç”¨å¢å¼ºåŠŸèƒ½é¢„è®¾"""
        if not ENHANCED_FEATURES_AVAILABLE:
            logger.warning("å¢å¼ºåŠŸèƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡é¢„è®¾åº”ç”¨")
            return
        
        presets = {
            'performance': {
                'dataloader': {'enable_prefetch': True, 'prefetch_factor': 3},
                'optimizer': {'gradient_accumulation_steps': 2 if self.batch_size < 8 else 1},
                'monitoring': {'enable_performance_monitoring': True}
            },
            'safety': {
                'monitoring': {'enable_health_checking': True},
                'error_recovery': {'auto_retry_on_oom': True},
                'batch_size': min(self.batch_size, 4)  # æ›´ä¿å®ˆçš„batch size
            },
            'memory': {
                'batch_size': max(1, self.batch_size // 2),
                'optimizer': {'gradient_accumulation_steps': self.enhanced_features['optimizer']['gradient_accumulation_steps'] * 2},
                'dataloader': {'prefetch_factor': 1},
                'monitoring': {'memory_cleanup_threshold': 0.75}
            },
            'debug': {
                'monitoring': {'monitoring_interval': 1, 'save_monitoring_plots': True},
                'evaluation': {'compute_structure_metrics': True, 'save_predictions': True},
                'test_mode': True
            }
        }
        
        if preset_name in presets:
            preset = presets[preset_name]
            for category, settings in preset.items():
                if category in self.enhanced_features:
                    self.enhanced_features[category].update(settings)
                elif hasattr(self, category):
                    setattr(self, category, settings)
            logger.info(f"âœ… åº”ç”¨é¢„è®¾: {preset_name}")
        else:
            logger.warning(f"æœªçŸ¥é¢„è®¾: {preset_name}")


class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡è®°å½•ç±»"""
    
    def __init__(self):
        self.train_losses = []
        self.valid_losses = []
        self.learning_rates = []
        self.epoch_times = []
        
        self.best_valid_loss = float('inf')
        self.best_epoch = 0
        self.early_stopping_counter = 0
    
    def update_train(self, loss: float, lr: float, epoch_time: float):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡"""
        self.train_losses.append(loss)
        self.learning_rates.append(lr)
        self.epoch_times.append(epoch_time)
    
    def update_valid(self, loss: float, epoch: int):
        """æ›´æ–°éªŒè¯æŒ‡æ ‡"""
        self.valid_losses.append(loss)
        
        if loss < self.best_valid_loss:
            self.best_valid_loss = loss
            self.best_epoch = epoch
            self.early_stopping_counter = 0
            return True  # æ‰¾åˆ°æ›´å¥½çš„æ¨¡å‹
        else:
            self.early_stopping_counter += 1
            return False
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'train_losses': self.train_losses,
            'valid_losses': self.valid_losses,
            'learning_rates': self.learning_rates,
            'epoch_times': self.epoch_times,
            'best_valid_loss': self.best_valid_loss,
            'best_epoch': self.best_epoch,
            'early_stopping_counter': self.early_stopping_counter
        }


class DiffoldTrainer:
    """Diffoldæ¨¡å‹è®­ç»ƒå™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, config: TrainingConfig, local_rank: int = 0, world_size: int = 1):
        self.config = config
        self.metrics = TrainingMetrics()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.local_rank = local_rank
        self.world_size = world_size
        self.is_main_process = (self.local_rank == 0)
        if self.is_main_process:
            self.setup_directories()
        
        # è®¾ç½®æ—¥å¿—
        if self.is_main_process:
            self.setup_logging()
        
        # åˆå§‹åŒ–è®¾å¤‡
        if world_size > 1:
            torch.cuda.set_device(local_rank)
            self.device = torch.device(f"cuda:{local_rank}")
        else:
            self.device = torch.device(config.device)
        if self.is_main_process:
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ğŸ”¥ åˆå§‹åŒ–å¢å¼ºåŠŸèƒ½
        self.enhanced_enabled = config.enhanced_features.get('enable_enhanced_training', False)
        self.training_monitor = None
        self.enhanced_optimizer = None
        self.enhanced_metrics = None
        
        if self.enhanced_enabled:
            self._setup_enhanced_features()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self.setup_model()
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        # âš¡ å¯é€‰ torch.compile
        if self.config.use_torch_compile:
            try:
                self.model = torch.compile(self.model, mode=self.config.torch_compile_mode)
                if self.is_main_process:
                    logger.info(f"âš¡ å·²å¯ç”¨ torch.compile (mode={self.config.torch_compile_mode})")
            except Exception as e:
                if self.is_main_process:
                    logger.warning(f"torch.compile å¯ç”¨å¤±è´¥: {e}")
        # ç»Ÿè®¡å¹¶æ‰“å°å¯è®­ç»ƒå‚æ•°æ€»æ•°
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        if self.is_main_process:
            logger.info(f"å¯è®­ç»ƒå‚æ•°æ€»æ•°: {total_params}")
        if world_size > 1:
            self.model = DDP(self.model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=True)
            self.using_ddp = True
            self.num_gpus = world_size
            if self.is_main_process:
                logger.info(f"ä½¿ç”¨DDPï¼ŒGPUæ•°é‡: {self.num_gpus}")
        else:
            # å•å¡å·²åœ¨ä¸Šé¢ to(device)
            self.using_ddp = False
            self.num_gpus = 1
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.setup_data_loaders()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.setup_optimizer_and_scheduler()
        
        # åˆå§‹åŒ–tensorboard
        if self.is_main_process:
            self.writer = SummaryWriter(log_dir=str(self.config.output_dir) + "/tensorboard")
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.mixed_precision and self.device.type == 'cuda':
            self.scaler = torch.cuda.amp.GradScaler()
            if self.is_main_process:
                logger.info("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        else:
            self.scaler = None
        
        # è®°å½•å¼€å§‹æ—¶é—´
        self.start_time = time.time()
    
    def _setup_enhanced_features(self):
        """è®¾ç½®å¢å¼ºåŠŸèƒ½"""
        logger.info("ğŸ”¥ å¯ç”¨å¢å¼ºè®­ç»ƒåŠŸèƒ½")
        
        # è®­ç»ƒç›‘æ§
        if self.config.enhanced_features['monitoring']['enable_performance_monitoring']:
            self.training_monitor = TrainingMonitor(self.config.output_dir)
            logger.info("âœ… è®­ç»ƒç›‘æ§å·²å¯ç”¨")
        
        # è¯„ä¼°æŒ‡æ ‡
        if self.config.enhanced_features['evaluation']['compute_structure_metrics']:
            self.enhanced_metrics = {
                'train': EvaluationMetrics(),
                'val': EvaluationMetrics()
            }
            logger.info("âœ… å¢å¼ºè¯„ä¼°æŒ‡æ ‡å·²å¯ç”¨")
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        self.config.output_dir = Path(self.config.output_dir)
        self.config.checkpoint_dir = Path(self.config.checkpoint_dir)
        
        self.config.output_dir.mkdir(exist_ok=True)
        self.config.checkpoint_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        (self.config.output_dir / "plots").mkdir(exist_ok=True)
        (self.config.output_dir / "tensorboard").mkdir(exist_ok=True)
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_file = self.config.output_dir / "training.log"
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ åˆ°logger
        logger.addHandler(file_handler)
        logger.info("æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        logger.info("åˆå§‹åŒ–Diffoldæ¨¡å‹...")
        model = Diffold(self.config, rhofold_checkpoint_path=self.config.rhofold_checkpoint)
        logger.info("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        return model
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        if self.is_main_process:
            logger.info("è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # åˆ›å»ºåŸºç¡€æ•°æ®åŠ è½½å™¨ï¼Œä¼ é€’åˆ†å¸ƒå¼ä¿¡æ¯
        train_loader, valid_loader = create_data_loaders(
            data_dir=self.config.data_dir,
            batch_size=self.config.batch_size,
            max_length=self.config.max_sequence_length,
            num_workers=self.config.num_workers,
            fold=self.config.fold,
            use_msa=self.config.use_msa,
            use_all_folds=self.config.use_all_folds,
            world_size=self.world_size,
            local_rank=self.local_rank
        )
        # ğŸ”¥ åº”ç”¨æ•°æ®åŠ è½½ä¼˜åŒ–
        if (self.enhanced_enabled and 
            self.config.enhanced_features['dataloader']['enable_prefetch']):
            if self.is_main_process:
                logger.info("ğŸš€ å¯ç”¨æ•°æ®é¢„å–ä¼˜åŒ–")
            
            self.train_loader = DataLoaderOptimizer(
                train_loader,
                prefetch_factor=self.config.enhanced_features['dataloader']['prefetch_factor'],
                cache_size=self.config.enhanced_features['dataloader']['cache_size'],
                enable_prefetch=True
            )
            self.valid_loader = DataLoaderOptimizer(
                valid_loader,
                prefetch_factor=1,  # éªŒè¯æ—¶ä½¿ç”¨è¾ƒå°çš„é¢„å–
                enable_prefetch=True
            )
        else:
            self.train_loader = train_loader
            self.valid_loader = valid_loader
        
        if self.is_main_process:
            logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_loader)}")
            logger.info(f"éªŒè¯é›†å¤§å°: {len(valid_loader)}")
    
    def setup_optimizer_and_scheduler(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        logger.info("è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨...")
        
        # ğŸ”¥ ä½¿ç”¨å¢å¼ºä¼˜åŒ–å™¨
        if (self.enhanced_enabled and 
            self.config.enhanced_features['optimizer']['use_advanced_optimizer']):
            logger.info("ğŸ¯ ä½¿ç”¨é«˜çº§ä¼˜åŒ–å™¨")
            
            self.enhanced_optimizer = AdaptiveOptimizer(  # type: ignore[arg-type]
                model=cast(nn.Module, self.model),
                optimizer_name=self.config.enhanced_features['optimizer']['optimizer_name'],
                learning_rate=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                scheduler_config={
                    'type': self.config.enhanced_features['optimizer']['scheduler_type'],
                    'warmup_epochs': self.config.warmup_epochs,
                    'T_max': self.config.num_epochs,
                    'eta_min': 1e-6
                },
                gradient_accumulation_steps=self.config.enhanced_features['optimizer']['gradient_accumulation_steps'],
                max_grad_norm=self.config.grad_clip_norm
            )
            
            # åŒ…è£…åŸæ¥å£
            self.optimizer = self.enhanced_optimizer.optimizer
            self.scheduler = self.enhanced_optimizer.scheduler
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨plateauè°ƒåº¦å™¨ï¼ˆæ¥è‡ªå­¦ä¹ ç‡ä¿®å¤ï¼‰
            if hasattr(self, 'checkpoint_data') and self.checkpoint_data.get('use_plateau_scheduler', False):
                logger.info("ğŸ”„ æ£€æµ‹åˆ°å­¦ä¹ ç‡ä¿®å¤æ ‡è®°ï¼Œåˆ‡æ¢åˆ°plateauè°ƒåº¦å™¨")
                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer, mode='min', factor=0.8, patience=5, verbose=True
                )
            
        else:
            # ä½¿ç”¨åŸç‰ˆä¼˜åŒ–å™¨
            if hasattr(self.model, 'get_trainable_parameters'):
                trainable_params = self.model.get_trainable_parameters()
            else:
                trainable_params = self.model.parameters()
            
            self.optimizer = torch.optim.AdamW(
                trainable_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
            
            # åˆ›å»ºè°ƒåº¦å™¨
            if self.config.scheduler_type == "cosine":
                self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer, T_max=self.config.num_epochs, eta_min=1e-6
                )
            elif self.config.scheduler_type == "plateau":
                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer, mode='min', factor=0.5, patience=self.config.patience, verbose=True
                )
            else:
                self.scheduler = None
        
        logger.info("ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å®Œæˆ")
    
    def train_one_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # å…¼å®¹DataParallel
        if self.using_ddp:
            self.model.module.set_train_mode()
        else:
            self.model.set_train_mode()
        
        total_loss = 0.0
        num_batches = 0
        
        # ğŸ”¥ é‡ç½®å¢å¼ºæŒ‡æ ‡
        if self.enhanced_metrics:
            self.enhanced_metrics['train'].reset()
        
        # æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶batchæ•°é‡
        if self.config.test_mode:
            max_batches = min(self.config.test_samples // self.config.batch_size + 1, 5)
        else:
            max_batches = len(self.train_loader)
        
        progress_bar = tqdm(
            enumerate(self.train_loader),
            total=min(max_batches, len(self.train_loader)),
            desc=f"Epoch {epoch+1}/{self.config.num_epochs}",
            leave=False,
            disable=not self.is_main_process
        )
        
        for batch_idx, batch in progress_bar:
            if self.config.test_mode and batch_idx >= max_batches:
                break
            
            batch_start_time = time.time()
            
            try:
                loss = self.train_step(batch, batch_idx, epoch)
                
                if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):
                    total_loss += loss.item()
                    num_batches += 1
                    
                    batch_time = time.time() - batch_start_time
                    
                    # ğŸ”¥ è®°å½•ç›‘æ§æ•°æ®
                    if (self.training_monitor and 
                        batch_idx % self.config.enhanced_features['monitoring']['monitoring_interval'] == 0):
                        self.training_monitor.log_training_step(
                            step=batch_idx + epoch * len(self.train_loader),
                            epoch=epoch,
                            loss_value=loss.item(),
                            learning_rate=self.optimizer.param_groups[0]['lr'],
                            batch_time=batch_time,
                            model=self.model
                        )
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    postfix_dict = {
                        'loss': loss.item(),
                        'avg_loss': total_loss / (batch_idx + 1),
                        'lr': self.optimizer.param_groups[0]['lr']
                    }
                    if self.device.type == 'cuda':
                        memory_reserved_gb = torch.cuda.memory_reserved(self.device) / 1024**3
                        postfix_dict['mem_gb'] = f"{memory_reserved_gb:.2f}"
                    
                    progress_bar.set_postfix(**postfix_dict)
                else:
                    logger.warning(f"Batch {batch_idx}: æ— æ•ˆæŸå¤±ï¼Œè·³è¿‡")
                    
            except RuntimeError as e:
                # ğŸ”¥ OOMé”™è¯¯å¤„ç†
                if ('out of memory' in str(e) and 
                    self.config.enhanced_features['error_recovery']['auto_retry_on_oom']):
                    logger.warning(f"æ£€æµ‹åˆ°OOMé”™è¯¯ï¼Œå°è¯•æ¢å¤: {e}")
                    
                    # æ¸…ç†å†…å­˜
                    if self.training_monitor:
                        self.training_monitor.memory_manager.cleanup_memory(aggressive=True)
                    else:
                        torch.cuda.empty_cache()
                    
                    # å‡å°‘batch sizeï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    if self.config.enhanced_features['error_recovery']['reduce_batch_size_on_oom']:
                        logger.warning("è€ƒè™‘å‡å°‘batch_sizeä»¥é¿å…OOM")
                    
                    continue
                else:
                    logger.error(f"Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
                    raise
            
            except Exception as e:
                logger.warning(f"Batch {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def train_step(self, batch: Dict, batch_idx: int, epoch: int) -> Optional[torch.Tensor]:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""
        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        tokens = batch['tokens'].to(self.device)
        sequences = batch['sequences']
        coordinates = batch.get('coordinates', None)
        missing_atom_masks = batch.get('missing_atom_masks', None)
        
        if coordinates is not None:
            coordinates = coordinates.to(self.device)
        if missing_atom_masks is not None:
            missing_atom_masks = missing_atom_masks.to(self.device)
        
        # rna_fm_tokenså¤„ç†
        rna_fm_tokens = batch.get('rna_fm_tokens', None)
        if rna_fm_tokens is not None:
            rna_fm_tokens = rna_fm_tokens.to(self.device)
        
        # ğŸ”¥ ä½¿ç”¨å¢å¼ºä¼˜åŒ–å™¨æˆ–åŸç‰ˆä¼˜åŒ–å™¨
        if self.enhanced_optimizer:
            # å¢å¼ºä¼˜åŒ–å™¨è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯
            self.enhanced_optimizer.zero_grad()
        else:
            self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        if self.scaler is not None:
            with torch.autocast('cuda', dtype=torch.bfloat16):
                result = self.model(
                    tokens=tokens,
                    rna_fm_tokens=rna_fm_tokens,
                    seq=sequences,
                    target_coords=coordinates,
                    missing_atom_mask=missing_atom_masks
                )
        else:
            result = self.model(
                tokens=tokens,
                rna_fm_tokens=rna_fm_tokens,
                seq=sequences,
                target_coords=coordinates,
                missing_atom_mask=missing_atom_masks
            )
        
        # å¤„ç†æ¨¡å‹è¾“å‡º
        if result is None:
            return None
        
        # æå–æŸå¤±
        if isinstance(result, dict):
            loss = result.get('loss', None)
        elif isinstance(result, tuple):
            loss = result[0]
        else:
            loss = result
        
        if loss is None:
            return None
        
        # ğŸ”¥ æ›´æ–°å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        if self.enhanced_metrics and isinstance(result, dict):
            loss_breakdown = {}
            if 'loss_breakdown' in result:
                breakdown = result['loss_breakdown']
                if hasattr(breakdown, 'total_diffusion'):
                    loss_breakdown['total_diffusion'] = breakdown.total_diffusion.item()
                if hasattr(breakdown, 'confidence'):
                    loss_breakdown['confidence'] = breakdown.confidence.item()
            
            self.enhanced_metrics['train'].update(
                loss=loss.item(),
                batch_size=batch['tokens'].size(0),
                loss_breakdown=loss_breakdown,
                predicted_coords=result.get('predicted_coords'),
                target_coords=coordinates
            )
        
        # åå‘ä¼ æ’­
        if self.enhanced_optimizer:
            # ä½¿ç”¨å¢å¼ºä¼˜åŒ–å™¨ï¼ˆè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯ï¼‰
            self.enhanced_optimizer.backward(loss)
        else:
            # ä½¿ç”¨åŸç‰ˆä¼˜åŒ–å™¨
            if self.scaler is not None:
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                if self.using_ddp:
                    torch.nn.utils.clip_grad_norm_(self.model.module.get_trainable_parameters(), self.config.grad_clip_norm)
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.get_trainable_parameters(), self.config.grad_clip_norm)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                if self.using_ddp:
                    torch.nn.utils.clip_grad_norm_(self.model.module.get_trainable_parameters(), self.config.grad_clip_norm)
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.get_trainable_parameters(), self.config.grad_clip_norm)
                self.optimizer.step()
        
        return loss
    
    def validate(self, epoch: int) -> float:
        """éªŒè¯æ¨¡å‹"""
        # å…¼å®¹DataParallel
        if self.using_ddp:
            self.model.module.set_eval_mode()
        else:
            self.model.set_eval_mode()
        
        total_loss = 0.0
        num_batches = 0
        
        # ğŸ”¥ é‡ç½®å¢å¼ºæŒ‡æ ‡
        if self.enhanced_metrics:
            self.enhanced_metrics['val'].reset()
        
        # æµ‹è¯•æ¨¡å¼ä¸‹é™åˆ¶batchæ•°é‡
        if self.config.test_mode:
            max_batches = min(3, len(self.valid_loader))
        else:
            max_batches = len(self.valid_loader)
        
        with torch.no_grad():
            progress_bar = tqdm(
                enumerate(self.valid_loader),
                total=min(max_batches, len(self.valid_loader)),
                desc="éªŒè¯",
                leave=False,
                disable=not self.is_main_process
            )
            
            for batch_idx, batch in progress_bar:
                if self.config.test_mode and batch_idx >= max_batches:
                    break
                    
                try:
                    # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                    tokens = batch['tokens'].to(self.device)
                    sequences = batch['sequences']
                    coordinates = batch.get('coordinates', None)
                    missing_atom_masks = batch.get('missing_atom_masks', None)
                    
                    if coordinates is not None:
                        coordinates = coordinates.to(self.device)
                    if missing_atom_masks is not None:
                        missing_atom_masks = missing_atom_masks.to(self.device)
                    
                    # rna_fm_tokenså¤„ç†
                    rna_fm_tokens = batch.get('rna_fm_tokens', None)
                    if rna_fm_tokens is not None:
                        rna_fm_tokens = rna_fm_tokens.to(self.device)
                    
                    # å‰å‘ä¼ æ’­
                    result = self.model(
                        tokens=tokens,
                        rna_fm_tokens=rna_fm_tokens,
                        seq=sequences,
                        target_coords=coordinates,
                        missing_atom_mask=missing_atom_masks
                    )
                    
                    if result is not None:
                        # æå–æŸå¤±
                        if isinstance(result, dict):
                            loss = result.get('loss', None)
                        elif isinstance(result, tuple):
                            loss = result[0]
                        else:
                            loss = result
                        
                        if loss is not None and not torch.isnan(loss) and not torch.isinf(loss):
                            total_loss += loss.item()
                            num_batches += 1
                            
                            # ğŸ”¥ æ›´æ–°å¢å¼ºè¯„ä¼°æŒ‡æ ‡
                            if self.enhanced_metrics and isinstance(result, dict):
                                self.enhanced_metrics['val'].update(
                                    loss=loss.item(),
                                    batch_size=batch['tokens'].size(0),
                                    predicted_coords=result.get('predicted_coords'),
                                    target_coords=coordinates,
                                    confidence_scores=result.get('confidence_logits')
                                )
                            
                            postfix_dict = {
                                'val_loss': loss.item(),
                                'avg_val_loss': total_loss / (batch_idx + 1)
                            }
                            if self.device.type == 'cuda':
                                memory_reserved_gb = torch.cuda.memory_reserved(self.device) / 1024**3
                                postfix_dict['mem_gb'] = f"{memory_reserved_gb:.2f}"
                            progress_bar.set_postfix(**postfix_dict)
                
                except Exception as e:
                    logger.warning(f"éªŒè¯ Batch {batch_idx} å¤±è´¥: {e}")
                    continue
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # å¤„ç†DataParallelçš„state_dict
        if self.using_ddp:
            model_state_dict = self.model.module.state_dict()
        else:
            model_state_dict = self.model.state_dict()
            
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state_dict,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'metrics': self.metrics.to_dict(),
            'config': self.config.__dict__,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'using_data_parallel': self.using_ddp,
            'num_gpus': self.num_gpus,
            'enhanced_enabled': self.enhanced_enabled
        }
        
        # ğŸ”¥ ä¿å­˜å¢å¼ºåŠŸèƒ½çŠ¶æ€
        if self.enhanced_optimizer:
            checkpoint['enhanced_optimizer_stats'] = self.enhanced_optimizer.get_stats()
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = self.config.checkpoint_dir / f"checkpoint_epoch_{epoch:03d}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.config.checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self.cleanup_old_checkpoints()
        
        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    def cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoint_files = list(self.config.checkpoint_dir.glob("checkpoint_epoch_*.pt"))
        if len(checkpoint_files) > self.config.keep_last_n_checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            checkpoint_files.sort(key=lambda x: x.stat().st_mtime)
            # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
            for old_file in checkpoint_files[:-self.config.keep_last_n_checkpoints]:
                old_file.unlink()
                logger.debug(f"åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_file}")
    
    def load_checkpoint(self, checkpoint_path: str) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint_path = Path(checkpoint_path)
        if not checkpoint_path.exists():
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0
        
        logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
                    checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            self.checkpoint_data = checkpoint  # ä¿å­˜æ£€æŸ¥ç‚¹æ•°æ®ä¾›è°ƒåº¦å™¨ä½¿ç”¨
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€ï¼ˆå¤„ç†DataParallelï¼‰
        model_state_dict = checkpoint['model_state_dict']
        if self.using_ddp:
            self.model.module.load_state_dict(model_state_dict)
        else:
            self.model.load_state_dict(model_state_dict)
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint.get('scheduler_state_dict'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.scaler and checkpoint.get('scaler_state_dict'):
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        # æ¢å¤è®­ç»ƒæŒ‡æ ‡
        if 'metrics' in checkpoint:
            metrics_dict = checkpoint['metrics']
            self.metrics.train_losses = metrics_dict.get('train_losses', [])
            self.metrics.valid_losses = metrics_dict.get('valid_losses', [])
            self.metrics.learning_rates = metrics_dict.get('learning_rates', [])
            self.metrics.epoch_times = metrics_dict.get('epoch_times', [])
            self.metrics.best_valid_loss = metrics_dict.get('best_valid_loss', float('inf'))
            self.metrics.best_epoch = metrics_dict.get('best_epoch', 0)
            self.metrics.early_stopping_counter = metrics_dict.get('early_stopping_counter', 0)
        
        start_epoch = checkpoint['epoch'] + 1
        logger.info(f"ä»epoch {start_epoch}ç»§ç»­è®­ç»ƒ")
        return start_epoch
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        if not self.metrics.train_losses:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        epochs = range(1, len(self.metrics.train_losses) + 1)
        axes[0, 0].plot(epochs, self.metrics.train_losses, 'b-', label='Training Loss')
        if self.metrics.valid_losses:
            valid_epochs = range(1, len(self.metrics.valid_losses) + 1)
            axes[0, 0].plot(valid_epochs, self.metrics.valid_losses, 'r-', label='Validation Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training and Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        if self.metrics.learning_rates:
            axes[0, 1].plot(epochs, self.metrics.learning_rates, 'g-')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Learning Rate')
            axes[0, 1].set_title('Learning Rate Schedule')
            axes[0, 1].grid(True)
        
        # è®­ç»ƒæ—¶é—´
        if self.metrics.epoch_times:
            axes[1, 0].plot(epochs, self.metrics.epoch_times, 'orange')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Time (seconds)')
            axes[1, 0].set_title('Epoch Training Time')
            axes[1, 0].grid(True)
        
        # æŸå¤±åˆ†å¸ƒï¼ˆæœ€è¿‘10ä¸ªepochï¼‰
        if len(self.metrics.train_losses) > 1:
            recent_losses = self.metrics.train_losses[-10:]
            axes[1, 1].hist(recent_losses, bins=min(10, len(recent_losses)), alpha=0.7, color='blue')
            axes[1, 1].set_xlabel('Loss')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Recent Training Loss Distribution')
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = self.config.output_dir / "plots" / f"training_curves_epoch_{len(self.metrics.train_losses)}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Saved training curves: {plot_path}")
    
    def train(self, resume_from: Optional[str] = None):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        if self.is_main_process:
            logger.info("å¼€å§‹è®­ç»ƒ...")
        
        # ğŸ”¥ æ‰“å°å¢å¼ºåŠŸèƒ½çŠ¶æ€
        if self.enhanced_enabled:
            if self.is_main_process:
                logger.info("ğŸ”¥ å¢å¼ºåŠŸèƒ½å·²å¯ç”¨:")
                for category, features in self.config.enhanced_features.items():
                    if isinstance(features, dict):
                        enabled_features = [k for k, v in features.items() if v]
                        if enabled_features:
                            logger.info(f"  {category}: {', '.join(enabled_features)}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        start_epoch = 0
        if resume_from:
            start_epoch = self.load_checkpoint(resume_from)
        
        # è®­ç»ƒå¾ªç¯
        num_epochs = self.config.test_epochs if self.config.test_mode else self.config.num_epochs
        
        for epoch in range(start_epoch, num_epochs):
            epoch_start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = self.train_one_epoch(epoch)
            
            # éªŒè¯
            valid_loss = None
            if epoch % self.config.validate_every == 0:
                valid_loss = self.validate(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.enhanced_optimizer:
                # ä½¿ç”¨å¢å¼ºä¼˜åŒ–å™¨
                self.enhanced_optimizer.scheduler_step(valid_loss)
            else:
                # ä½¿ç”¨åŸç‰ˆè°ƒåº¦å™¨
                if self.scheduler is not None:
                    if self.config.scheduler_type == "plateau" and valid_loss is not None:
                        self.scheduler.step(valid_loss)
                    elif self.config.scheduler_type == "cosine":
                        self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            current_lr = self.optimizer.param_groups[0]['lr']
            epoch_time = time.time() - epoch_start_time
            
            self.metrics.update_train(train_loss, current_lr, epoch_time)
            
            is_best = False
            if valid_loss is not None:
                is_best = self.metrics.update_valid(valid_loss, epoch)
            
            # ğŸ”¥ è®°å½•å¢å¼ºè¯„ä¼°æŒ‡æ ‡
            if self.enhanced_metrics:
                train_metrics = self.enhanced_metrics['train'].compute_metrics()
                val_metrics = self.enhanced_metrics['val'].compute_metrics() if valid_loss is not None else {}
                
                if 'avg_rmsd' in val_metrics and self.is_main_process:
                    logger.info(f"éªŒè¯RMSD: {val_metrics['avg_rmsd']:.4f}")
            
            # åªåœ¨ä¸»è¿›ç¨‹å†™tensorboard
            if self.is_main_process:
                self.writer.add_scalar('Loss/Train', train_loss, epoch)
                if valid_loss is not None:
                    self.writer.add_scalar('Loss/Valid', valid_loss, epoch)
                self.writer.add_scalar('LearningRate', current_lr, epoch)
                self.writer.add_scalar('EpochTime', epoch_time, epoch)
            
            # è®¡ç®—é¢„è®¡æ—¶é—´
            current_time = time.time()
            elapsed_time = current_time - self.start_time
            
            # è®¡ç®—å¹³å‡epochæ—¶é—´
            if len(self.metrics.epoch_times) > 0:
                recent_times = self.metrics.epoch_times[-5:]
                avg_epoch_time = sum(recent_times) / len(recent_times)
            else:
                avg_epoch_time = epoch_time
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            remaining_epochs = num_epochs - (epoch + 1)
            estimated_remaining_time = remaining_epochs * avg_epoch_time
            estimated_completion_time = current_time + estimated_remaining_time
            
            # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
            def format_time(seconds):
                if seconds < 60:
                    return f"{seconds:.1f}ç§’"
                elif seconds < 3600:
                    minutes = seconds / 60
                    return f"{minutes:.1f}åˆ†é’Ÿ"
                else:
                    hours = seconds / 3600
                    return f"{hours:.1f}å°æ—¶"
            
            def format_datetime(timestamp):
                return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
            
            # æ‰“å°è¿›åº¦
            if self.is_main_process:
                log_msg = f"Epoch {epoch+1}/{num_epochs} - "
                log_msg += f"è®­ç»ƒæŸå¤±: {train_loss:.6f}, "
                if valid_loss is not None:
                    log_msg += f"éªŒè¯æŸå¤±: {valid_loss:.6f}, "
                log_msg += f"å­¦ä¹ ç‡: {current_lr:.2e}, "
                log_msg += f"æ—¶é—´: {epoch_time:.1f}s"
                if is_best:
                    log_msg += " â­ æœ€ä½³æ¨¡å‹!"
                logger.info(log_msg)
                # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡ä¿¡æ¯
                time_msg = f"â±ï¸  å·²ç”¨æ—¶é—´: {format_time(elapsed_time)}, "
                time_msg += f"é¢„è®¡å‰©ä½™: {format_time(estimated_remaining_time)}, "
                time_msg += f"é¢„è®¡å®Œæˆ: {format_datetime(estimated_completion_time)}"
                logger.info(time_msg)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.is_main_process:
                if (epoch + 1) % self.config.save_every == 0:
                    self.save_checkpoint(epoch, is_best)
                # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
                if (epoch + 1) % (self.config.save_every * 2) == 0:
                    self.plot_training_curves()
                # ğŸ”¥ ä¿å­˜ç›‘æ§æŠ¥å‘Š
                if (self.training_monitor and 
                    self.config.enhanced_features['monitoring']['save_monitoring_plots'] and
                    (epoch + 1) % 5 == 0):
                    self.training_monitor.save_monitoring_report()
                    self.training_monitor.generate_performance_plots()
            
            # æ—©åœæ£€æŸ¥
            if (self.metrics.early_stopping_counter >= self.config.early_stopping_patience 
                and not self.config.test_mode):
                if self.is_main_process:
                    logger.info(f"æ—©åœè§¦å‘ (patience={self.config.early_stopping_patience})")
                break
        
        # è®­ç»ƒç»“æŸ
        total_time = time.time() - self.start_time
        if self.is_main_process:
            logger.info("="*60)
            logger.info("è®­ç»ƒå®Œæˆ!")
            logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
            logger.info(f"æœ€ä½³éªŒè¯æŸå¤±: {self.metrics.best_valid_loss:.6f} (Epoch {self.metrics.best_epoch+1})")
            # ğŸ”¥ æ˜¾ç¤ºå¢å¼ºåŠŸèƒ½ç»Ÿè®¡
            if self.enhanced_optimizer:
                stats = self.enhanced_optimizer.get_stats()
                logger.info(f"ä¼˜åŒ–å™¨ç»Ÿè®¡: æ›´æ–°æ¬¡æ•°={stats.get('update_count', 0)}, "
                           f"å¹³å‡æ¢¯åº¦èŒƒæ•°={stats.get('avg_grad_norm', 0):.4f}")
            if self.training_monitor:
                logger.info("æ€§èƒ½ç›‘æ§æŠ¥å‘Š:")
                summary = self.training_monitor.performance_monitor.get_performance_summary()
                logger.info(f"  æ€»batchæ•°: {summary.get('total_batches', 0)}")
                logger.info(f"  OOMæ¬¡æ•°: {summary.get('oom_count', 0)}")
                logger.info(f"  NaNæŸå¤±æ¬¡æ•°: {summary.get('nan_loss_count', 0)}")
                if 'avg_batch_time' in summary:
                    logger.info(f"  å¹³å‡batchæ—¶é—´: {summary['avg_batch_time']:.2f}s")
            logger.info("="*60)
            # æœ€ç»ˆä¿å­˜
            self.save_checkpoint(epoch, False)
            self.plot_training_curves()
            # ğŸ”¥ æœ€ç»ˆæŠ¥å‘Š
            if self.training_monitor:
                self.training_monitor.save_monitoring_report()
                self.training_monitor.generate_performance_plots()
            # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
            metrics_path = str(self.config.output_dir) + "/training_metrics.json"
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(self.metrics.to_dict(), f, indent=2, ensure_ascii=False)
            self.writer.close()
        # DDPç»“æŸ
        if self.world_size > 1:
            dist.destroy_process_group()


def run_small_scale_test():
    """è¿è¡Œå°è§„æ¨¡æµ‹è¯•"""
    logger.info("ğŸ§ª è¿è¡Œå°è§„æ¨¡æµ‹è¯•...")
    
    # æµ‹è¯•é…ç½®
    config = TrainingConfig()
    config.test_mode = True
    config.test_epochs = 1
    config.test_samples = 6
    config.batch_size = 2
    config.max_sequence_length = 128
    config.device = "cuda"
    config.num_workers = 0
    config.output_dir = "./test_output"
    config.checkpoint_dir = "./test_checkpoints"
    config.mixed_precision = False
    config.use_data_parallel = False
    
    # ğŸ”¥ å¯ç”¨å¢å¼ºåŠŸèƒ½è¿›è¡Œæµ‹è¯•
    if ENHANCED_FEATURES_AVAILABLE:
        config.apply_enhanced_preset('debug')
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = DiffoldTrainer(config)
        
        # è¿è¡Œè®­ç»ƒ
        trainer.train()
        
        logger.info("âœ… å°è§„æ¨¡æµ‹è¯•å®Œæˆ!")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
        logger.info(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: {config.checkpoint_dir}")
        
        exit()
    except Exception as e:
        logger.error(f"âŒ å°è§„æ¨¡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def fix_checkpoint_learning_rate(checkpoint_path, new_lr):
    """ä¿®å¤æ£€æŸ¥ç‚¹ä¸­çš„å­¦ä¹ ç‡ - é’ˆå¯¹é¢„çƒ­bugçš„å¿«é€Ÿä¿®å¤"""
    import shutil
    
    if not os.path.exists(checkpoint_path):
        logger.error(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
        return False
    
    # å¤‡ä»½
    backup_path = f"{checkpoint_path}.backup"
    if not os.path.exists(backup_path):
        shutil.copy2(checkpoint_path, backup_path)
        logger.info(f"å·²å¤‡ä»½æ£€æŸ¥ç‚¹è‡³: {backup_path}")
    
    # ä¿®å¤å­¦ä¹ ç‡ - å…¼å®¹PyTorch 2.6çš„å®‰å…¨æœºåˆ¶
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    
    if 'optimizer_state_dict' in checkpoint:
        optimizer_state = checkpoint['optimizer_state_dict']
        if 'param_groups' in optimizer_state:
            for group in optimizer_state['param_groups']:
                old_lr = group['lr']
                group['lr'] = new_lr
                logger.info(f"ğŸ”§ ä¿®å¤å­¦ä¹ ç‡: {old_lr:.2e} â†’ {new_lr:.2e}")
    
    # é‡ç½®è°ƒåº¦å™¨çŠ¶æ€ï¼Œä½¿ç”¨plateauè°ƒåº¦å™¨é¿å…ç»§ç»­ä¸‹é™
    if 'scheduler_state_dict' in checkpoint:
        logger.info("ğŸ”„ é‡ç½®è°ƒåº¦å™¨çŠ¶æ€ï¼Œæ”¹ç”¨plateauè°ƒåº¦å™¨")
        del checkpoint['scheduler_state_dict']
        # æ·»åŠ é…ç½®ä¿¡æ¯ï¼Œè®©æ¢å¤æ—¶ä½¿ç”¨plateauè°ƒåº¦å™¨
        checkpoint['use_plateau_scheduler'] = True
    
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"âœ… å­¦ä¹ ç‡ä¿®å¤å®Œæˆ: {checkpoint_path}")
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Diffoldæ¨¡å‹è®­ç»ƒ - å¢å¼ºç‰ˆ")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, default="./processed_data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=256, help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½è¿›ç¨‹æ•°")
    parser.add_argument("--fold", type=int, default=0, help="äº¤å‰éªŒè¯æŠ˜æ•° (0-9)")
    parser.add_argument("--use_all_folds", action="store_true", help="ä½¿ç”¨æ‰€æœ‰æŠ˜æ•°çš„æ•°æ®è¿›è¡Œè®­ç»ƒ")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--rhofold_checkpoint", type=str, 
                       default="./pretrained/model_20221010_params.pt", 
                       help="RhoFoldé¢„è®­ç»ƒæƒé‡è·¯å¾„")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints", help="æ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--save_every", type=int, default=5, help="æ¯Nè½®ä¿å­˜ä¸€æ¬¡")
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡ (auto/cpu/cuda)")
    parser.add_argument("--no_mixed_precision", action="store_true", help="ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--no_data_parallel", action="store_true", help="ç¦ç”¨DataParallelå¤šGPUè®­ç»ƒ")
    parser.add_argument("--gpu_ids", type=int, nargs='+', help="æŒ‡å®šä½¿ç”¨çš„GPU ID")
    parser.add_argument('--local_rank', type=int, default=0, help='DDP local rank')
    
    # ğŸ”¥ å¢å¼ºåŠŸèƒ½å‚æ•°
    parser.add_argument("--enhanced_preset", type=str, default=None,
                       choices=['performance', 'safety', 'memory', 'debug'],
                       help="å¢å¼ºåŠŸèƒ½é¢„è®¾")
    parser.add_argument("--disable_enhanced", action="store_true", 
                       help="ç¦ç”¨æ‰€æœ‰å¢å¼ºåŠŸèƒ½")
    parser.add_argument("--disable_monitoring", action="store_true",
                       help="ç¦ç”¨æ€§èƒ½ç›‘æ§")
    parser.add_argument("--disable_prefetch", action="store_true",
                       help="ç¦ç”¨æ•°æ®é¢„å–")
    parser.add_argument("--disable_advanced_optimizer", action="store_true",
                       help="ç¦ç”¨é«˜çº§ä¼˜åŒ–å™¨")
    # torch.compile ç›¸å…³å‚æ•°
    parser.add_argument("--torch_compile", action="store_true", help="å¯ç”¨ torch.compile (PyTorch>=2.0)")
    parser.add_argument("--compile_mode", type=str, default="default",
                        choices=["default", "reduce-overhead", "max-autotune"],
                        help="torch.compile ä¼˜åŒ–æ¨¡å¼")
    parser.add_argument("--grad_accum", type=int, default=None,
                       help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (gradient_accumulation_steps)ï¼Œé»˜è®¤æ ¹æ®é¢„è®¾æˆ–1")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--resume", type=str, default=None, help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œå°è§„æ¨¡æµ‹è¯•")
    
    # ğŸ”§ å­¦ä¹ ç‡ä¿®å¤å‚æ•°
    parser.add_argument("--fix_lr", type=float, default=None, 
                       help="ä¿®å¤æ£€æŸ¥ç‚¹ä¸­çš„å­¦ä¹ ç‡ (é…åˆ--resumeä½¿ç”¨)")
    parser.add_argument("--fix_lr_only", action="store_true",
                       help="ä»…ä¿®å¤å­¦ä¹ ç‡ï¼Œä¸å¼€å§‹è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼
    if args.test:
        run_small_scale_test()
        return
    
    # ğŸ”§ å¤„ç†å­¦ä¹ ç‡ä¿®å¤
    if args.fix_lr is not None:
        if args.resume is None:
            logger.error("âŒ ä½¿ç”¨ --fix_lr éœ€è¦æŒ‡å®š --resume æ£€æŸ¥ç‚¹è·¯å¾„")
            return
        
        logger.info(f"ğŸ”§ å¼€å§‹ä¿®å¤å­¦ä¹ ç‡: {args.fix_lr}")
        success = fix_checkpoint_learning_rate(args.resume, args.fix_lr)
        
        if success:
            logger.info("âœ… å­¦ä¹ ç‡ä¿®å¤å®Œæˆ!")
            if args.fix_lr_only:
                logger.info("ğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¢å¤è®­ç»ƒ:")
                logger.info(f"python train.py --resume {args.resume}")
                return
            else:
                logger.info("ğŸš€ ç»§ç»­å¼€å§‹è®­ç»ƒ...")
        else:
            logger.error("âŒ å­¦ä¹ ç‡ä¿®å¤å¤±è´¥")
            return
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig()
    
    # ğŸ”¥ åº”ç”¨å¢å¼ºåŠŸèƒ½è®¾ç½®
    if args.disable_enhanced or not ENHANCED_FEATURES_AVAILABLE:
        config.enhanced_features['enable_enhanced_training'] = False
        if args.disable_enhanced:
            logger.warning("âš ï¸ å¢å¼ºåŠŸèƒ½å·²æ‰‹åŠ¨ç¦ç”¨")
    else:
        # åº”ç”¨é¢„è®¾
        if args.enhanced_preset:
            config.apply_enhanced_preset(args.enhanced_preset)
            logger.info(f"ğŸ”¥ åº”ç”¨å¢å¼ºé¢„è®¾: {args.enhanced_preset}")
        
        # åº”ç”¨å…·ä½“ç¦ç”¨é€‰é¡¹
        if args.disable_monitoring:
            config.enhanced_features['monitoring']['enable_performance_monitoring'] = False
        if args.disable_prefetch:
            config.enhanced_features['dataloader']['enable_prefetch'] = False
        if args.disable_advanced_optimizer:
            config.enhanced_features['optimizer']['use_advanced_optimizer'] = False
    
    # å¦‚æœæŒ‡å®šäº†æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    if args.grad_accum is not None:
        config.enhanced_features['optimizer']['gradient_accumulation_steps'] = max(1, args.grad_accum)
        logger.info(f"âœ… è®¾ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä¸º: {config.enhanced_features['optimizer']['gradient_accumulation_steps']}")

    # torch.compile è®¾ç½®
    if args.torch_compile:
        config.use_torch_compile = True
        config.torch_compile_mode = args.compile_mode
        logger.info(f"âš¡ è®¡åˆ’å¯ç”¨ torch.compile (mode={config.torch_compile_mode})")
    
    # æ›´æ–°åŸºç¡€é…ç½®
    config.data_dir = args.data_dir
    config.batch_size = args.batch_size
    config.max_sequence_length = args.max_length
    config.num_workers = args.num_workers
    config.fold = args.fold
    config.use_all_folds = args.use_all_folds
    
    config.num_epochs = args.epochs
    config.learning_rate = args.learning_rate
    config.weight_decay = args.weight_decay
    config.grad_clip_norm = args.grad_clip
    
    config.rhofold_checkpoint = args.rhofold_checkpoint
    config.output_dir = args.output_dir
    config.checkpoint_dir = args.checkpoint_dir
    config.save_every = args.save_every
    
    if args.device == "auto":
        config.device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        config.device = args.device
    
    config.mixed_precision = not args.no_mixed_precision
    config.use_data_parallel = not args.no_data_parallel
    config.gpu_ids = args.gpu_ids
    
    # DDPåˆå§‹åŒ–
    world_size = int(os.environ.get('WORLD_SIZE', '1'))
    local_rank = int(os.environ.get('LOCAL_RANK', args.local_rank))
    if world_size > 1:
        dist.init_process_group(backend='nccl')
        torch.cuda.set_device(local_rank)
        device = torch.device(f'cuda:{local_rank}')
    else:
        device = torch.device(config.device)
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info("ğŸ¯ Diffoldè®­ç»ƒ - å¢å¼ºç‰ˆ")
    logger.info("="*50)
    logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {config.data_dir}")
    logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    logger.info(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {config.max_sequence_length}")
    logger.info(f"ğŸ–¥ï¸  è®¾å¤‡: {config.device}")
    logger.info(f"â±ï¸  è®­ç»ƒè½®æ•°: {config.num_epochs}")
    logger.info(f"ğŸ“Š å­¦ä¹ ç‡: {config.learning_rate}")
    
    # ğŸ”¥ æ˜¾ç¤ºå¢å¼ºåŠŸèƒ½çŠ¶æ€
    if config.enhanced_features.get('enable_enhanced_training', False):
        logger.info("ğŸ”¥ å¢å¼ºåŠŸèƒ½: å·²å¯ç”¨")
        enabled_features = []
        if config.enhanced_features['monitoring']['enable_performance_monitoring']:
            enabled_features.append("æ€§èƒ½ç›‘æ§")
        if config.enhanced_features['dataloader']['enable_prefetch']:
            enabled_features.append("æ•°æ®é¢„å–")
        if config.enhanced_features['optimizer']['use_advanced_optimizer']:
            enabled_features.append("é«˜çº§ä¼˜åŒ–å™¨")
        if config.enhanced_features['evaluation']['compute_structure_metrics']:
            enabled_features.append("ç»“æ„è¯„ä¼°")
        if enabled_features:
            logger.info(f"   â€¢ {', '.join(enabled_features)}")
    else:
        logger.info("âšª å¢å¼ºåŠŸèƒ½: å·²ç¦ç”¨ï¼ˆä½¿ç”¨åŸç‰ˆåŠŸèƒ½ï¼‰")
    
    logger.info("="*50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DiffoldTrainer(config, local_rank=local_rank, world_size=world_size)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(resume_from=args.resume)
    # DDPç»“æŸ
    if world_size > 1:
        dist.destroy_process_group()


if __name__ == "__main__":
    main()
