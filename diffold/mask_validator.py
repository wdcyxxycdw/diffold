"""
MaskéªŒè¯å’Œç›‘æ§æœºåˆ¶
ç”¨äºç¡®ä¿EDM batchå¤„ç†ä¸­maskçš„ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§
"""

import torch
import warnings
from typing import Dict, List, Optional, Tuple, Any
import logging

logger = logging.getLogger(__name__)

class MaskValidator:
    """MaskéªŒè¯å™¨ï¼Œç¡®ä¿batchä¸­ä¸åŒé•¿åº¦åºåˆ—çš„æ­£ç¡®å¤„ç†"""
    
    def __init__(self, 
                 enable_warnings: bool = True,
                 enable_logging: bool = True,
                 strict_mode: bool = False):
        """
        åˆå§‹åŒ–MaskéªŒè¯å™¨
        
        Args:
            enable_warnings: æ˜¯å¦å¯ç”¨è­¦å‘Š
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—è®°å½•
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼Œå‘ç°é—®é¢˜æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        self.enable_warnings = enable_warnings
        self.enable_logging = enable_logging
        self.strict_mode = strict_mode
        
    def validate_batch_consistency(self,
                                 tokens: torch.Tensor,
                                 sequences: List[str],
                                 coordinates: torch.Tensor,
                                 missing_atom_mask: torch.Tensor,
                                 seq_lengths: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        éªŒè¯batchæ•°æ®çš„ä¸€è‡´æ€§
        
        Args:
            tokens: tokenå¼ é‡ [batch_size, msa_depth, max_seq_len]
            sequences: åºåˆ—åˆ—è¡¨
            coordinates: åæ ‡å¼ é‡ [batch_size, max_atoms, 3]
            missing_atom_mask: ç¼ºå¤±åŸå­mask [batch_size, max_atoms]
            seq_lengths: åºåˆ—é•¿åº¦ [batch_size]
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        batch_size = tokens.shape[0]
        results = {
            'is_valid': True,
            'warnings': [],
            'errors': [],
            'statistics': {}
        }
        
        # 1. æ£€æŸ¥batchç»´åº¦ä¸€è‡´æ€§
        expected_shapes = {
            'tokens': (batch_size, tokens.shape[1], tokens.shape[2]),
            'coordinates': (batch_size, coordinates.shape[1], 3),
            'missing_atom_mask': (batch_size, missing_atom_mask.shape[1]),
            'sequences': batch_size
        }
        
        if len(sequences) != batch_size:
            error_msg = f"åºåˆ—æ•°é‡({len(sequences)})ä¸batch_size({batch_size})ä¸åŒ¹é…"
            results['errors'].append(error_msg)
            results['is_valid'] = False
            
        # 2. æ£€æŸ¥åºåˆ—é•¿åº¦å˜åŒ–
        actual_seq_lengths = [len(seq) for seq in sequences]
        max_seq_len = max(actual_seq_lengths)
        min_seq_len = min(actual_seq_lengths)
        
        results['statistics']['seq_lengths'] = actual_seq_lengths
        results['statistics']['seq_length_variance'] = max_seq_len - min_seq_len
        results['statistics']['max_seq_len'] = max_seq_len
        results['statistics']['min_seq_len'] = min_seq_len
        
        if max_seq_len != min_seq_len:
            msg = f"æ£€æµ‹åˆ°ä¸åŒåºåˆ—é•¿åº¦: æœ€é•¿{max_seq_len}, æœ€çŸ­{min_seq_len}, å·®å¼‚{max_seq_len - min_seq_len}"
            results['warnings'].append(msg)
            if self.enable_warnings:
                warnings.warn(msg)
        
        # 3. æ£€æŸ¥paddingæ­£ç¡®æ€§
        if seq_lengths is not None:
            for i, (expected_len, actual_len) in enumerate(zip(seq_lengths.tolist(), actual_seq_lengths)):
                if expected_len != actual_len:
                    error_msg = f"åºåˆ—{i}é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{expected_len}, å®é™…{actual_len}"
                    results['errors'].append(error_msg)
                    results['is_valid'] = False
        
        # 4. æ£€æŸ¥missing_atom_maskè¦†ç›–ç‡
        valid_atoms_per_seq = []
        for i in range(batch_size):
            valid_atoms = (~missing_atom_mask[i]).sum().item()
            total_atoms = missing_atom_mask[i].shape[0]
            coverage = valid_atoms / total_atoms if total_atoms > 0 else 0
            valid_atoms_per_seq.append(coverage)
            
            if coverage < 0.1:  # æœ‰æ•ˆåŸå­å°‘äº10%
                warning_msg = f"åºåˆ—{i}æœ‰æ•ˆåŸå­æ¯”ä¾‹è¿‡ä½: {coverage:.2%}"
                results['warnings'].append(warning_msg)
                if self.enable_warnings:
                    warnings.warn(warning_msg)
        
        results['statistics']['atom_coverage'] = valid_atoms_per_seq
        results['statistics']['avg_atom_coverage'] = sum(valid_atoms_per_seq) / len(valid_atoms_per_seq)
        
        # 5. æ£€æŸ¥åæ ‡æœ‰æ•ˆæ€§
        zero_coord_counts = []
        for i in range(batch_size):
            coords = coordinates[i][~missing_atom_mask[i]]  # åªæ£€æŸ¥æœ‰æ•ˆåŸå­
            if coords.shape[0] > 0:
                zero_coords = (coords.abs().sum(dim=1) < 1e-6).sum().item()
                zero_coord_counts.append(zero_coords)
                
                if zero_coords > coords.shape[0] * 0.5:  # è¶…è¿‡50%çš„åæ ‡ä¸ºé›¶
                    warning_msg = f"åºåˆ—{i}å­˜åœ¨å¤§é‡é›¶åæ ‡: {zero_coords}/{coords.shape[0]}"
                    results['warnings'].append(warning_msg)
            else:
                zero_coord_counts.append(0)
        
        results['statistics']['zero_coord_counts'] = zero_coord_counts
        
        # ä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œæœ‰é”™è¯¯å°±æŠ›å‡ºå¼‚å¸¸
        if self.strict_mode and not results['is_valid']:
            raise ValueError(f"MaskéªŒè¯å¤±è´¥: {results['errors']}")
        
        # è®°å½•æ—¥å¿—
        if self.enable_logging:
            logger.info(f"Batch maskéªŒè¯å®Œæˆ: valid={results['is_valid']}, "
                       f"warnings={len(results['warnings'])}, errors={len(results['errors'])}")
        
        return results
    
    def validate_edm_inputs(self,
                           atom_mask: torch.Tensor,
                           missing_atom_mask: torch.Tensor,
                           molecule_atom_lens: torch.Tensor,
                           mask: torch.Tensor) -> Dict[str, Any]:
        """
        éªŒè¯ä¼ é€’ç»™EDMçš„maskå‚æ•°ä¸€è‡´æ€§
        
        Args:
            atom_mask: åŸå­å­˜åœ¨mask [batch_size, max_atoms]
            missing_atom_mask: ç¼ºå¤±åŸå­mask [batch_size, max_atoms]
            molecule_atom_lens: åˆ†å­åŸå­é•¿åº¦ [batch_size, num_molecules]
            mask: åºåˆ—mask [batch_size, max_seq_len]
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        results = {
            'is_valid': True,
            'warnings': [],
            'errors': [],
            'statistics': {}
        }
        
        batch_size = atom_mask.shape[0]
        
        # 1. æ£€æŸ¥maskç»´åº¦ä¸€è‡´æ€§
        if atom_mask.shape != missing_atom_mask.shape:
            error_msg = f"atom_mask({atom_mask.shape})ä¸missing_atom_mask({missing_atom_mask.shape})å½¢çŠ¶ä¸åŒ¹é…"
            results['errors'].append(error_msg)
            results['is_valid'] = False
        
        # 2. æ£€æŸ¥maské€»è¾‘ä¸€è‡´æ€§
        # atom_maskä¸ºTrueè¡¨ç¤ºåŸå­å­˜åœ¨ï¼Œmissing_atom_maskä¸ºTrueè¡¨ç¤ºåŸå­ç¼ºå¤±
        # ç†è®ºä¸Š atom_mask åº”è¯¥ä¸ ~missing_atom_mask åŸºæœ¬ä¸€è‡´
        consistency_scores = []
        for i in range(batch_size):
            expected_atom_mask = ~missing_atom_mask[i]
            actual_atom_mask = atom_mask[i]
            
            # è®¡ç®—ä¸€è‡´æ€§
            consistent = (expected_atom_mask == actual_atom_mask).float().mean().item()
            consistency_scores.append(consistent)
            
            if consistent < 0.95:  # ä¸€è‡´æ€§ä½äº95%
                warning_msg = f"åºåˆ—{i}çš„atom_maskä¸missing_atom_maskä¸€è‡´æ€§è¾ƒä½: {consistent:.2%}"
                results['warnings'].append(warning_msg)
        
        results['statistics']['mask_consistency_scores'] = consistency_scores
        results['statistics']['avg_mask_consistency'] = sum(consistency_scores) / len(consistency_scores)
        
        # 3. æ£€æŸ¥åˆ†å­åŸå­é•¿åº¦çš„åˆç†æ€§
        molecule_lens = molecule_atom_lens.sum(dim=-1)  # æ¯ä¸ªåºåˆ—çš„æ€»åŸå­æ•°
        atom_counts = atom_mask.sum(dim=-1)  # å®é™…å­˜åœ¨çš„åŸå­æ•°
        
        len_ratios = []
        for i in range(batch_size):
            if molecule_lens[i] > 0:
                ratio = atom_counts[i].float() / molecule_lens[i].float()
                len_ratios.append(ratio.item())
                
                if ratio < 0.5 or ratio > 1.5:  # åå·®è¶…è¿‡50%
                    warning_msg = f"åºåˆ—{i}åˆ†å­é•¿åº¦ä¸å®é™…åŸå­æ•°åå·®è¾ƒå¤§: {ratio:.2f}"
                    results['warnings'].append(warning_msg)
            else:
                len_ratios.append(0.0)
        
        results['statistics']['length_ratios'] = len_ratios
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        results['statistics']['total_atoms'] = atom_mask.sum().item()
        results['statistics']['total_missing'] = missing_atom_mask.sum().item()
        results['statistics']['valid_sequences'] = (mask.sum(dim=-1) > 0).sum().item()
        
        if self.enable_logging:
            logger.info(f"EDMè¾“å…¥maskéªŒè¯: valid={results['is_valid']}, "
                       f"avg_consistency={results['statistics']['avg_mask_consistency']:.3f}")
        
        return results
    
    def monitor_loss_computation(self,
                               logits: torch.Tensor,
                               labels: torch.Tensor,
                               mask: torch.Tensor,
                               loss_type: str = "cross_entropy") -> Dict[str, Any]:
        """
        ç›‘æ§æŸå¤±è®¡ç®—ä¸­çš„maskä½¿ç”¨æƒ…å†µ
        
        Args:
            logits: é¢„æµ‹logits
            labels: çœŸå®æ ‡ç­¾
            mask: åº”ç”¨çš„mask
            loss_type: æŸå¤±ç±»å‹
            
        Returns:
            ç›‘æ§ç»“æœ
        """
        results = {
            'statistics': {},
            'warnings': []
        }
        
        # è®¡ç®—maskè¦†ç›–ç‡
        if mask.dim() == logits.dim() - 1:  # maskç»´åº¦æ¯”logitså°‘1ï¼ˆé€šå¸¸æ˜¯æœ€åçš„ç±»åˆ«ç»´åº¦ï¼‰
            mask_flat = mask.flatten()
            valid_ratio = mask_flat.float().mean().item()
        else:
            valid_ratio = 1.0  # æ— æ³•ç¡®å®šï¼Œå‡è®¾å…¨éƒ¨æœ‰æ•ˆ
        
        results['statistics']['mask_coverage'] = valid_ratio
        results['statistics']['total_elements'] = labels.numel()
        results['statistics']['valid_elements'] = mask.sum().item() if mask.dtype == torch.bool else mask.sum().item()
        
        # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
        if labels.dtype in [torch.long, torch.int]:
            unique_labels = torch.unique(labels[mask] if mask.dtype == torch.bool else labels)
            num_classes = logits.shape[-1] if logits.dim() > 1 else 1
            
            invalid_labels = (unique_labels < 0) | (unique_labels >= num_classes)
            if invalid_labels.any():
                warning_msg = f"{loss_type}æŸå¤±ä¸­æ£€æµ‹åˆ°æ— æ•ˆæ ‡ç­¾: {unique_labels[invalid_labels].tolist()}"
                results['warnings'].append(warning_msg)
        
        # æ£€æŸ¥logitsæ˜¯å¦åŒ…å«NaNæˆ–Inf
        if torch.isnan(logits).any() or torch.isinf(logits).any():
            warning_msg = f"{loss_type}æŸå¤±çš„logitsåŒ…å«NaNæˆ–Inf"
            results['warnings'].append(warning_msg)
        
        if self.enable_logging and len(results['warnings']) > 0:
            logger.warning(f"æŸå¤±è®¡ç®—ç›‘æ§å‘ç°é—®é¢˜: {results['warnings']}")
        
        return results
    
    def create_batch_report(self, 
                          validation_results: List[Dict[str, Any]], 
                          step: int = 0) -> str:
        """
        åˆ›å»ºbatchå¤„ç†æŠ¥å‘Š
        
        Args:
            validation_results: éªŒè¯ç»“æœåˆ—è¡¨
            step: è®­ç»ƒæ­¥æ•°
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = [f"\nğŸ“Š Batchå¤„ç†æŠ¥å‘Š (Step {step})"]
        report.append("=" * 50)
        
        total_warnings = sum(len(result.get('warnings', [])) for result in validation_results)
        total_errors = sum(len(result.get('errors', [])) for result in validation_results)
        
        report.append(f"æ€»ä½“çŠ¶æ€: {'âœ… æ­£å¸¸' if total_errors == 0 else 'âŒ æœ‰é”™è¯¯'}")
        report.append(f"è­¦å‘Šæ•°é‡: {total_warnings}")
        report.append(f"é”™è¯¯æ•°é‡: {total_errors}")
        
        # ç»Ÿè®¡ä¿¡æ¯æ±‡æ€»
        if validation_results:
            for i, result in enumerate(validation_results):
                if 'statistics' in result:
                    stats = result['statistics']
                    report.append(f"\néªŒè¯é˜¶æ®µ {i+1}:")
                    
                    if 'seq_length_variance' in stats:
                        report.append(f"  åºåˆ—é•¿åº¦å˜åŒ–: {stats['seq_length_variance']}")
                    
                    if 'avg_atom_coverage' in stats:
                        report.append(f"  å¹³å‡åŸå­è¦†ç›–ç‡: {stats['avg_atom_coverage']:.2%}")
                    
                    if 'avg_mask_consistency' in stats:
                        report.append(f"  Maskä¸€è‡´æ€§: {stats['avg_mask_consistency']:.2%}")
        
        # è­¦å‘Šå’Œé”™è¯¯è¯¦æƒ…
        if total_warnings > 0:
            report.append(f"\nâš ï¸ è­¦å‘Šè¯¦æƒ…:")
            for i, result in enumerate(validation_results):
                for warning in result.get('warnings', []):
                    report.append(f"  - {warning}")
        
        if total_errors > 0:
            report.append(f"\nâŒ é”™è¯¯è¯¦æƒ…:")
            for i, result in enumerate(validation_results):
                for error in result.get('errors', []):
                    report.append(f"  - {error}")
        
        return "\n".join(report)

# å…¨å±€éªŒè¯å™¨å®ä¾‹
default_validator = MaskValidator(
    enable_warnings=True,
    enable_logging=True,
    strict_mode=False
)

def validate_batch_for_edm(tokens: torch.Tensor,
                          sequences: List[str],
                          coordinates: torch.Tensor,
                          missing_atom_mask: torch.Tensor,
                          seq_lengths: Optional[torch.Tensor] = None,
                          validator: Optional[MaskValidator] = None) -> bool:
    """
    ä¸ºEDMè°ƒç”¨éªŒè¯batchæ•°æ®çš„ä¾¿æ·å‡½æ•°
    
    Returns:
        æ˜¯å¦éªŒè¯é€šè¿‡
    """
    if validator is None:
        validator = default_validator
    
    result = validator.validate_batch_consistency(
        tokens, sequences, coordinates, missing_atom_mask, seq_lengths
    )
    
    return result['is_valid'] 