"""
å¢å¼ºçš„è®­ç»ƒé…ç½®
é›†æˆæ‰€æœ‰æ–°çš„ä¿éšœæªæ–½å’Œä¼˜åŒ–åŠŸèƒ½
"""

import torch
from typing import Dict, Any, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class EnhancedTrainingConfig:
    """å¢å¼ºçš„è®­ç»ƒé…ç½®ç±»"""
    
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.data_dir = "./processed_data"
        self.batch_size = 4
        self.max_sequence_length = 256
        self.num_workers = 4
        self.use_msa = True
        
        # æ¨¡å‹é…ç½®
        self.rhofold_checkpoint = "./pretrained/model_20221010_params.pt"
        
        # è®­ç»ƒé…ç½®
        self.num_epochs = 100
        self.learning_rate = 1e-4
        self.weight_decay = 1e-5
        self.grad_clip_norm = 1.0
        
        # ğŸ”¥ æ–°å¢ï¼šé«˜çº§ä¼˜åŒ–å™¨é…ç½®
        self.optimizer_config = {
            'name': 'adamw',  # 'adamw', 'adam', 'sgd', 'lion'
            'gradient_accumulation_steps': 1,
            'max_grad_norm': 1.0
        }
        
        # ğŸ”¥ æ–°å¢ï¼šå¢å¼ºçš„å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        self.scheduler_config = {
            'type': 'warmup_cosine',  # 'warmup_cosine', 'warmup_cosine_restarts', 'plateau', 'cosine'
            'warmup_epochs': 5,
            'warmup_start_lr': 1e-7,
            'T_max': 100,
            'eta_min': 1e-6,
            # ä½™å¼¦é‡å¯å‚æ•°
            'T_0': 50,
            'T_mult': 2,
            # plateauå‚æ•°
            'factor': 0.5,
            'patience': 10
        }
        
        # ä¿å­˜é…ç½®
        self.output_dir = "./output"
        self.checkpoint_dir = "./checkpoints"
        self.save_every = 5
        self.keep_last_n_checkpoints = 5
        
        # éªŒè¯é…ç½®
        self.validate_every = 1
        self.early_stopping_patience = 20
        
        # è®¾å¤‡é…ç½®
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.mixed_precision = True
        
        # å¤šGPUé…ç½®
        self.use_data_parallel = True
        self.gpu_ids = None
        
        # ğŸ”¥ æ–°å¢ï¼šæ€§èƒ½ç›‘æ§é…ç½®
        self.monitoring_config = {
            'enable_performance_monitoring': True,
            'enable_memory_monitoring': True,
            'enable_health_checking': True,
            'monitoring_interval': 10,  # æ¯10ä¸ªbatchè®°å½•ä¸€æ¬¡
            'save_monitoring_plots': True,
            'memory_cleanup_threshold': 0.85
        }
        
        # ğŸ”¥ æ–°å¢ï¼šæ•°æ®åŠ è½½ä¼˜åŒ–é…ç½®
        self.dataloader_config = {
            'enable_prefetch': True,
            'prefetch_factor': 2,
            'cache_size': 100,
            'pin_memory': True,
            'persistent_workers': True
        }
        
        # ğŸ”¥ æ–°å¢ï¼šè‡ªé€‚åº”æ‰¹æ¬¡å¤§å°é…ç½®
        self.adaptive_batch_config = {
            'enable_adaptive_batch_size': False,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦æ—¶å¯ç”¨
            'min_batch_size': 1,
            'max_batch_size': 16,
            'oom_detection': True
        }
        
        # ğŸ”¥ æ–°å¢ï¼šè¯„ä¼°æŒ‡æ ‡é…ç½®
        self.evaluation_config = {
            'compute_structure_metrics': True,
            'compute_confidence_metrics': True,
            'save_predictions': False,  # æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ
            'evaluation_frequency': 1  # æ¯Nä¸ªepochè¯„ä¼°ä¸€æ¬¡
        }
        
        # ğŸ”¥ æ–°å¢ï¼šé”™è¯¯æ¢å¤é…ç½®
        self.error_recovery_config = {
            'auto_retry_on_oom': True,
            'max_retry_attempts': 3,
            'reduce_batch_size_on_oom': True,
            'fallback_to_cpu_on_gpu_error': False
        }
        
        # å°è§„æ¨¡æµ‹è¯•é…ç½®
        self.test_mode = False
        self.test_samples = 10
        self.test_epochs = 5
        
        # äº¤å‰éªŒè¯é…ç½®
        self.fold = 0
        self.num_folds = 10
        self.use_all_folds = False
    
    def get_optimizer_config(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨é…ç½®"""
        return {
            'optimizer_name': self.optimizer_config['name'],
            'learning_rate': self.learning_rate,
            'weight_decay': self.weight_decay,
            'scheduler_config': self.scheduler_config,
            'gradient_accumulation_steps': self.optimizer_config['gradient_accumulation_steps'],
            'max_grad_norm': self.optimizer_config['max_grad_norm']
        }
    
    def get_monitoring_config(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§é…ç½®"""
        return self.monitoring_config.copy()
    
    def get_dataloader_config(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åŠ è½½å™¨é…ç½®"""
        config = self.dataloader_config.copy()
        config.update({
            'batch_size': self.batch_size,
            'num_workers': self.num_workers,
            'shuffle': True
        })
        return config
    
    def enable_performance_mode(self):
        """å¯ç”¨æ€§èƒ½ä¼˜åŒ–æ¨¡å¼"""
        logger.info("ğŸš€ å¯ç”¨æ€§èƒ½ä¼˜åŒ–æ¨¡å¼")
        
        # å¯ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
        self.dataloader_config['enable_prefetch'] = True
        self.dataloader_config['prefetch_factor'] = 3
        self.dataloader_config['pin_memory'] = True
        self.dataloader_config['persistent_workers'] = True
        
        # å¯ç”¨æ¢¯åº¦ç´¯ç§¯
        if self.batch_size < 8:
            self.optimizer_config['gradient_accumulation_steps'] = max(1, 8 // self.batch_size)
            logger.info(f"å¯ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œæ­¥æ•°: {self.optimizer_config['gradient_accumulation_steps']}")
        
        # ä½¿ç”¨æ›´å¥½çš„è°ƒåº¦å™¨
        self.scheduler_config['type'] = 'warmup_cosine'
        
        # å¯ç”¨ç›‘æ§
        self.monitoring_config['enable_performance_monitoring'] = True
        self.monitoring_config['enable_memory_monitoring'] = True
        
        logger.info("âœ… æ€§èƒ½ä¼˜åŒ–æ¨¡å¼å·²å¯ç”¨")
    
    def enable_safety_mode(self):
        """å¯ç”¨å®‰å…¨æ¨¡å¼"""
        logger.info("ğŸ›¡ï¸ å¯ç”¨å®‰å…¨æ¨¡å¼")
        
        # å¯ç”¨æ‰€æœ‰å®‰å…¨æ£€æŸ¥
        self.monitoring_config['enable_health_checking'] = True
        self.error_recovery_config['auto_retry_on_oom'] = True
        self.error_recovery_config['reduce_batch_size_on_oom'] = True
        
        # æ›´ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°
        if self.batch_size > 4:
            self.batch_size = 4
            logger.info("é™ä½æ‰¹æ¬¡å¤§å°ä¸º4ä»¥æé«˜ç¨³å®šæ€§")
        
        # å¯ç”¨è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°
        self.adaptive_batch_config['enable_adaptive_batch_size'] = True
        
        # æ›´é¢‘ç¹çš„æ£€æŸ¥ç‚¹ä¿å­˜
        self.save_every = min(self.save_every, 3)
        
        logger.info("âœ… å®‰å…¨æ¨¡å¼å·²å¯ç”¨")
    
    def enable_memory_efficient_mode(self):
        """å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼"""
        logger.info("ğŸ’¾ å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼")
        
        # é™ä½æ‰¹æ¬¡å¤§å°
        self.batch_size = max(1, self.batch_size // 2)
        
        # å¯ç”¨æ¢¯åº¦ç´¯ç§¯è¡¥å¿
        self.optimizer_config['gradient_accumulation_steps'] *= 2
        
        # é™ä½åºåˆ—é•¿åº¦
        self.max_sequence_length = min(self.max_sequence_length, 200)
        
        # å¯ç”¨æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
        self.monitoring_config['memory_cleanup_threshold'] = 0.75
        
        # å‡å°‘é¢„å–
        self.dataloader_config['prefetch_factor'] = 1
        
        logger.info(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º: {self.batch_size}")
        logger.info(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {self.optimizer_config['gradient_accumulation_steps']}")
        logger.info("âœ… å†…å­˜é«˜æ•ˆæ¨¡å¼å·²å¯ç”¨")
    
    def enable_debug_mode(self):
        """å¯ç”¨è°ƒè¯•æ¨¡å¼"""
        logger.info("ğŸ› å¯ç”¨è°ƒè¯•æ¨¡å¼")
        
        # å¯ç”¨è¯¦ç»†ç›‘æ§
        self.monitoring_config['monitoring_interval'] = 1
        self.monitoring_config['save_monitoring_plots'] = True
        
        # æ›´é¢‘ç¹çš„ä¿å­˜
        self.save_every = 1
        
        # å¯ç”¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        self.evaluation_config['compute_structure_metrics'] = True
        self.evaluation_config['compute_confidence_metrics'] = True
        self.evaluation_config['save_predictions'] = True
        
        # å°æ‰¹æ¬¡æµ‹è¯•
        if not self.test_mode:
            self.test_mode = True
            self.test_epochs = 3
            self.test_samples = 12
        
        logger.info("âœ… è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'data_dir': self.data_dir,
            'batch_size': self.batch_size,
            'max_sequence_length': self.max_sequence_length,
            'num_workers': self.num_workers,
            'use_msa': self.use_msa,
            'rhofold_checkpoint': self.rhofold_checkpoint,
            'num_epochs': self.num_epochs,
            'learning_rate': self.learning_rate,
            'weight_decay': self.weight_decay,
            'grad_clip_norm': self.grad_clip_norm,
            'optimizer_config': self.optimizer_config,
            'scheduler_config': self.scheduler_config,
            'output_dir': self.output_dir,
            'checkpoint_dir': self.checkpoint_dir,
            'save_every': self.save_every,
            'keep_last_n_checkpoints': self.keep_last_n_checkpoints,
            'validate_every': self.validate_every,
            'early_stopping_patience': self.early_stopping_patience,
            'device': self.device,
            'mixed_precision': self.mixed_precision,
            'use_data_parallel': self.use_data_parallel,
            'gpu_ids': self.gpu_ids,
            'monitoring_config': self.monitoring_config,
            'dataloader_config': self.dataloader_config,
            'adaptive_batch_config': self.adaptive_batch_config,
            'evaluation_config': self.evaluation_config,
            'error_recovery_config': self.error_recovery_config,
            'test_mode': self.test_mode,
            'test_samples': self.test_samples,
            'test_epochs': self.test_epochs,
            'fold': self.fold,
            'num_folds': self.num_folds,
            'use_all_folds': self.use_all_folds
        }


def get_recommended_config(scenario: str = "balanced") -> EnhancedTrainingConfig:
    """è·å–æ¨èé…ç½®
    
    Args:
        scenario: é…ç½®åœºæ™¯
            - "balanced": å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰
            - "performance": æ€§èƒ½ä¼˜å…ˆ
            - "safety": å®‰å…¨ä¼˜å…ˆ
            - "memory": å†…å­˜é«˜æ•ˆ
            - "debug": è°ƒè¯•æ¨¡å¼
    """
    config = EnhancedTrainingConfig()
    
    if scenario == "performance":
        config.enable_performance_mode()
    elif scenario == "safety":
        config.enable_safety_mode()
    elif scenario == "memory":
        config.enable_memory_efficient_mode()
    elif scenario == "debug":
        config.enable_debug_mode()
    elif scenario == "balanced":
        # å¹³è¡¡æ¨¡å¼ï¼šå¯ç”¨éƒ¨åˆ†ä¼˜åŒ–
        config.dataloader_config['enable_prefetch'] = True
        config.monitoring_config['enable_performance_monitoring'] = True
        config.monitoring_config['enable_memory_monitoring'] = True
        config.error_recovery_config['auto_retry_on_oom'] = True
        logger.info("âœ… ä½¿ç”¨å¹³è¡¡é…ç½®æ¨¡å¼")
    else:
        logger.warning(f"æœªçŸ¥çš„é…ç½®åœºæ™¯: {scenario}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    return config


# é¢„å®šä¹‰é…ç½®æ¨¡æ¿
PRESET_CONFIGS = {
    "small_model": {
        "batch_size": 8,
        "max_sequence_length": 128,
        "learning_rate": 2e-4,
        "scheduler_config": {"type": "warmup_cosine", "warmup_epochs": 3}
    },
    
    "large_model": {
        "batch_size": 2,
        "max_sequence_length": 512,
        "learning_rate": 5e-5,
        "optimizer_config": {"gradient_accumulation_steps": 4},
        "scheduler_config": {"type": "warmup_cosine_restarts", "T_0": 30}
    },
    
    "production": {
        "save_every": 10,
        "keep_last_n_checkpoints": 3,
        "early_stopping_patience": 30,
        "monitoring_config": {"monitoring_interval": 50}
    },
    
    "research": {
        "save_every": 1,
        "evaluation_config": {
            "compute_structure_metrics": True,
            "save_predictions": True
        },
        "monitoring_config": {"save_monitoring_plots": True}
    }
}


def create_config_from_preset(preset_name: str, 
                            base_scenario: str = "balanced") -> EnhancedTrainingConfig:
    """ä»é¢„è®¾åˆ›å»ºé…ç½®
    
    Args:
        preset_name: é¢„è®¾åç§°
        base_scenario: åŸºç¡€åœºæ™¯
    """
    if preset_name not in PRESET_CONFIGS:
        logger.warning(f"æœªçŸ¥çš„é¢„è®¾: {preset_name}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return get_recommended_config(base_scenario)
    
    config = get_recommended_config(base_scenario)
    preset = PRESET_CONFIGS[preset_name]
    
    # åº”ç”¨é¢„è®¾é…ç½®
    for key, value in preset.items():
        if hasattr(config, key):
            if isinstance(value, dict) and isinstance(getattr(config, key), dict):
                getattr(config, key).update(value)
            else:
                setattr(config, key, value)
    
    logger.info(f"âœ… åº”ç”¨é¢„è®¾é…ç½®: {preset_name}")
    return config 