"""
é«˜çº§ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨æ¨¡å—
åŒ…å«å­¦ä¹ ç‡é¢„çƒ­ã€æ”¹è¿›çš„è°ƒåº¦å™¨ã€æ•°æ®é¢„åŠ è½½ä¼˜åŒ–ç­‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
import math
import warnings
from typing import Dict, List, Optional, Any, Tuple, Union
import logging
from pathlib import Path
import threading
import queue
import time
from collections import defaultdict

logger = logging.getLogger(__name__)


class WarmupLRScheduler(_LRScheduler):
    """å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨"""
    
    def __init__(self, 
                 optimizer: optim.Optimizer,
                 warmup_steps: int,
                 base_scheduler: _LRScheduler = None,
                 warmup_start_lr: float = 1e-7,
                 last_epoch: int = -1,
                 verbose: bool = False):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            warmup_steps: é¢„çƒ­æ­¥æ•°
            base_scheduler: é¢„çƒ­åä½¿ç”¨çš„åŸºç¡€è°ƒåº¦å™¨
            warmup_start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
            last_epoch: ä¸Šæ¬¡epochç´¢å¼•
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.warmup_steps = warmup_steps
        self.base_scheduler = base_scheduler
        self.warmup_start_lr = warmup_start_lr
        
        # ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„PyTorch
        try:
            super().__init__(optimizer, last_epoch, verbose)
        except TypeError:
            # æ–°ç‰ˆæœ¬PyTorchä¸æ”¯æŒverboseå‚æ•°
            super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿ - ä¿®å¤: ä½¿ç”¨ (last_epoch + 1) ç¡®ä¿è¾¾åˆ°ç›®æ ‡å­¦ä¹ ç‡
            progress = (self.last_epoch + 1) / self.warmup_steps
            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * progress
                    for base_lr in self.base_lrs]
        else:
            # é¢„çƒ­ç»“æŸåä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨
            if self.base_scheduler is not None:
                # è°ƒæ•´åŸºç¡€è°ƒåº¦å™¨çš„æ­¥æ•°ï¼ˆä»0å¼€å§‹ï¼‰
                effective_step = self.last_epoch - self.warmup_steps
                self.base_scheduler.last_epoch = effective_step
                return self.base_scheduler.get_lr()
            else:
                return self.base_lrs
    
    def step(self, epoch=None):
        super().step(epoch)
        
        # å¦‚æœæœ‰åŸºç¡€è°ƒåº¦å™¨ä¸”è¿‡äº†é¢„çƒ­æœŸï¼ŒåŒæ­¥æ›´æ–°
        if (self.base_scheduler is not None and 
            self.last_epoch >= self.warmup_steps):
            self.base_scheduler.step()


class CosineAnnealingWarmRestarts(_LRScheduler):
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«é‡å¯è°ƒåº¦å™¨"""
    
    def __init__(self, 
                 optimizer: optim.Optimizer,
                 T_0: int,
                 T_mult: int = 1,
                 eta_min: float = 0,
                 warmup_steps: int = 0,
                 warmup_start_lr: float = 1e-7,
                 last_epoch: int = -1):
        """
        Args:
            T_0: ç¬¬ä¸€æ¬¡é‡å¯å‰çš„epochæ•°
            T_mult: é‡å¯åå‘¨æœŸçš„ä¹˜æ•°
            eta_min: æœ€å°å­¦ä¹ ç‡
            warmup_steps: æ¯æ¬¡é‡å¯åçš„é¢„çƒ­æ­¥æ•°
            warmup_start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
        """
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.warmup_steps = warmup_steps
        self.warmup_start_lr = warmup_start_lr
        
        # è·Ÿè¸ªå½“å‰å‘¨æœŸ
        self.T_cur = 0
        self.T_i = T_0
        self.cycle = 0
        
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.T_cur < self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µ - ä¿®å¤: ä½¿ç”¨ (T_cur + 1) ç¡®ä¿è¾¾åˆ°ç›®æ ‡å­¦ä¹ ç‡
            progress = (self.T_cur + 1) / self.warmup_steps
            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * progress
                    for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            effective_t = self.T_cur - self.warmup_steps
            effective_T_i = self.T_i - self.warmup_steps
            
            return [self.eta_min + (base_lr - self.eta_min) * 
                    (1 + math.cos(math.pi * effective_t / effective_T_i)) / 2
                    for base_lr in self.base_lrs]
    
    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1
        
        self.T_cur += 1
        
        if self.T_cur >= self.T_i:
            # é‡å¯
            self.cycle += 1
            self.T_cur = 0
            self.T_i = self.T_0 * (self.T_mult ** self.cycle)
            
        self.last_epoch = epoch
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr


class AdaptiveOptimizer:
    """è‡ªé€‚åº”ä¼˜åŒ–å™¨åŒ…è£…å™¨"""
    
    def __init__(self, 
                 model: nn.Module,
                 optimizer_name: str = "adamw",
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-5,
                 scheduler_config: Dict[str, Any] = None,
                 gradient_accumulation_steps: int = 1,
                 max_grad_norm: float = 1.0):
        """
        Args:
            model: è¦ä¼˜åŒ–çš„æ¨¡å‹
            optimizer_name: ä¼˜åŒ–å™¨åç§° ("adamw", "adam", "sgd", "lion")
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            scheduler_config: è°ƒåº¦å™¨é…ç½®
            gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            max_grad_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°
        """
        self.model = model
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm
        self.accumulated_steps = 0
        
        # è·å–å¯è®­ç»ƒå‚æ•°
        if hasattr(model, 'get_trainable_parameters'):
            trainable_params = model.get_trainable_parameters()
        else:
            trainable_params = model.parameters()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer(
            optimizer_name, trainable_params, learning_rate, weight_decay
        )
        
        # åˆ›å»ºè°ƒåº¦å™¨
        self.scheduler = self._create_scheduler(scheduler_config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'grad_norm_history': [],
            'lr_history': [],
            'param_norm_history': [],
            'update_count': 0
        }
    
    def _create_optimizer(self, 
                         optimizer_name: str, 
                         params, 
                         lr: float, 
                         weight_decay: float) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_name = optimizer_name.lower()
        
        if optimizer_name == "adamw":
            return optim.AdamW(
                params, lr=lr, weight_decay=weight_decay,
                betas=(0.9, 0.999), eps=1e-8
            )
        elif optimizer_name == "adam":
            return optim.Adam(
                params, lr=lr, weight_decay=weight_decay,
                betas=(0.9, 0.999), eps=1e-8
            )
        elif optimizer_name == "sgd":
            return optim.SGD(
                params, lr=lr, weight_decay=weight_decay,
                momentum=0.9, nesterov=True
            )
        elif optimizer_name == "lion":
            try:
                from lion_pytorch import Lion
                return Lion(params, lr=lr, weight_decay=weight_decay)
            except ImportError:
                logger.warning("Lion optimizerä¸å¯ç”¨ï¼Œå›é€€åˆ°AdamW")
                return optim.AdamW(
                    params, lr=lr, weight_decay=weight_decay,
                    betas=(0.9, 0.999), eps=1e-8
                )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")
    
    def _create_scheduler(self, config: Dict[str, Any] = None) -> Optional[_LRScheduler]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if config is None:
            return None
        
        scheduler_type = config.get('type', 'cosine')
        
        if scheduler_type == 'warmup_cosine':
            # è®¡ç®—é¢„çƒ­åçš„æ€»æ­¥æ•°
            total_steps = config.get('T_max', 100)
            warmup_steps = config.get('warmup_steps', 100)
            remaining_steps = total_steps - warmup_steps
            
            base_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=remaining_steps,  # é¢„çƒ­åçš„å‰©ä½™æ­¥æ•°
                eta_min=config.get('eta_min', 1e-6)
            )
            return WarmupLRScheduler(
                self.optimizer,
                warmup_steps=warmup_steps,
                base_scheduler=base_scheduler,
                warmup_start_lr=config.get('warmup_start_lr', 1e-7)
            )
        
        elif scheduler_type == 'warmup_cosine_restarts':
            return CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=config.get('T_0', 50),
                T_mult=config.get('T_mult', 2),
                eta_min=config.get('eta_min', 1e-6),
                warmup_steps=config.get('warmup_steps', 100),
                warmup_start_lr=config.get('warmup_start_lr', 1e-6)
            )
        
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=config.get('factor', 0.5),
                patience=config.get('patience', 10),
                verbose=True
            )
        
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=config.get('T_max', 100),
                eta_min=config.get('eta_min', 1e-6)
            )
        
        else:
            logger.warning(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
            return None
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        self.optimizer.zero_grad()
    
    def backward(self, loss: torch.Tensor):
        """åå‘ä¼ æ’­"""
        # ç¼©æ”¾æŸå¤±ä»¥è€ƒè™‘æ¢¯åº¦ç´¯ç§¯
        scaled_loss = loss / self.gradient_accumulation_steps
        scaled_loss.backward()
        
        self.accumulated_steps += 1
        
        # æ¢¯åº¦ç´¯ç§¯
        if self.accumulated_steps >= self.gradient_accumulation_steps:
            self.step()
            self.accumulated_steps = 0
    
    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = self._compute_grad_norm()
        self.stats['grad_norm_history'].append(grad_norm)
        
        # æ¢¯åº¦è£å‰ª
        if self.max_grad_norm > 0:
            if hasattr(self.model, 'get_trainable_parameters'):
                torch.nn.utils.clip_grad_norm_(
                    self.model.get_trainable_parameters(), 
                    self.max_grad_norm
                )
            else:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.max_grad_norm
                )
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        self.optimizer.step()
        self.stats['update_count'] += 1
        
        # è°ƒåº¦å™¨æ­¥éª¤ï¼ˆå¯¹äºstep-basedè°ƒåº¦å™¨ï¼‰
        if self.scheduler is not None:
            # å¯¹äºstep-basedè°ƒåº¦å™¨ï¼Œåœ¨æ¯ä¸ªä¼˜åŒ–å™¨æ­¥éª¤åè°ƒç”¨
            if isinstance(self.scheduler, (WarmupLRScheduler, CosineAnnealingWarmRestarts)):
                self.scheduler.step()
            # å¯¹äºepoch-basedè°ƒåº¦å™¨ï¼ˆå¦‚ReduceLROnPlateauï¼‰ï¼Œä¸åœ¨è¿™é‡Œè°ƒç”¨
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = self.optimizer.param_groups[0]['lr']
        self.stats['lr_history'].append(current_lr)
        
        # è®°å½•å‚æ•°èŒƒæ•°
        param_norm = self._compute_param_norm()
        self.stats['param_norm_history'].append(param_norm)
    
    def scheduler_step(self, metrics: float = None):
        """è°ƒåº¦å™¨æ­¥éª¤"""
        if self.scheduler is not None:
            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                if metrics is not None:
                    self.scheduler.step(metrics)
            else:
                self.scheduler.step()
    
    def _compute_grad_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        
        if hasattr(self.model, 'get_trainable_parameters'):
            parameters = self.model.get_trainable_parameters()
        else:
            parameters = self.model.parameters()
        
        for param in parameters:
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        
        return total_norm ** (1. / 2)
    
    def _compute_param_norm(self) -> float:
        """è®¡ç®—å‚æ•°èŒƒæ•°"""
        total_norm = 0.0
        
        if hasattr(self.model, 'get_trainable_parameters'):
            parameters = self.model.get_trainable_parameters()
        else:
            parameters = self.model.parameters()
        
        for param in parameters:
            param_norm = param.data.norm(2)
            total_norm += param_norm.item() ** 2
        
        return total_norm ** (1. / 2)
    
    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizer.param_groups[0]['lr']
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if self.stats['grad_norm_history']:
            stats['avg_grad_norm'] = sum(self.stats['grad_norm_history']) / len(self.stats['grad_norm_history'])
            stats['max_grad_norm'] = max(self.stats['grad_norm_history'])
        
        if self.stats['param_norm_history']:
            stats['avg_param_norm'] = sum(self.stats['param_norm_history']) / len(self.stats['param_norm_history'])
        
        return stats
    
    def load_stats(self, stats_dict: Dict[str, Any]):
        """åŠ è½½ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats_dict: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # æ¢å¤åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        for key in ['update_count', 'grad_norm_history', 'lr_history', 'param_norm_history']:
            if key in stats_dict:
                self.stats[key] = stats_dict[key]
        
        # æ¢å¤ç´¯ç§¯æ­¥æ•°
        self.accumulated_steps = stats_dict.get('accumulated_steps', 0)
        
        logger.info(f"ğŸ“Š åŠ è½½ä¼˜åŒ–å™¨ç»Ÿè®¡: æ›´æ–°æ¬¡æ•°={self.stats['update_count']}, "
                   f"æ¢¯åº¦å†å²={len(self.stats['grad_norm_history'])}, "
                   f"å­¦ä¹ ç‡å†å²={len(self.stats['lr_history'])}")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'update_count': 0,
            'grad_norm_history': [],
            'lr_history': [],
            'param_norm_history': []
        }
        self.accumulated_steps = 0
        logger.info("ğŸ“Š ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


class DataLoaderOptimizer:
    """æ•°æ®åŠ è½½å™¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 base_dataloader,
                 prefetch_factor: int = 2,
                 cache_size: int = 100,
                 enable_prefetch: bool = True):
        """
        Args:
            base_dataloader: åŸºç¡€æ•°æ®åŠ è½½å™¨
            prefetch_factor: é¢„å–å› å­
            cache_size: ç¼“å­˜å¤§å°
            enable_prefetch: æ˜¯å¦å¯ç”¨é¢„å–
        """
        self.base_dataloader = base_dataloader
        self.prefetch_factor = prefetch_factor
        self.cache_size = cache_size
        self.enable_prefetch = enable_prefetch
        
        # ç¼“å­˜
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # é¢„å–é˜Ÿåˆ—
        self.prefetch_queue = queue.Queue(maxsize=prefetch_factor * 2)
        self.prefetch_thread = None
        self.stop_prefetch = threading.Event()
    
    def __iter__(self):
        if self.enable_prefetch:
            return self._prefetch_iter()
        else:
            return iter(self.base_dataloader)
    
    def __len__(self):
        return len(self.base_dataloader)
    
    def _prefetch_iter(self):
        """é¢„å–è¿­ä»£å™¨"""
        # å¯åŠ¨é¢„å–çº¿ç¨‹
        self.stop_prefetch.clear()
        self.prefetch_thread = threading.Thread(target=self._prefetch_worker)
        self.prefetch_thread.start()
        
        try:
            while True:
                try:
                    batch = self.prefetch_queue.get(timeout=30)  # 30ç§’è¶…æ—¶
                    if batch is None:  # ç»“æŸæ ‡å¿—
                        break
                    yield batch
                except queue.Empty:
                    logger.warning("é¢„å–é˜Ÿåˆ—è¶…æ—¶ï¼Œå¯èƒ½å‡ºç°æ€§èƒ½é—®é¢˜")
                    break
        finally:
            # åœæ­¢é¢„å–çº¿ç¨‹
            self.stop_prefetch.set()
            if self.prefetch_thread and self.prefetch_thread.is_alive():
                self.prefetch_thread.join(timeout=5)
    
    def _prefetch_worker(self):
        """é¢„å–å·¥ä½œçº¿ç¨‹"""
        try:
            for batch in self.base_dataloader:
                if self.stop_prefetch.is_set():
                    break
                
                # æ•°æ®é¢„å¤„ç†ï¼ˆå¦‚ç§»åŠ¨åˆ°GPUï¼‰
                processed_batch = self._preprocess_batch(batch)
                
                # æ”¾å…¥é˜Ÿåˆ—
                self.prefetch_queue.put(processed_batch)
            
            # å‘é€ç»“æŸä¿¡å·
            self.prefetch_queue.put(None)
            
        except Exception as e:
            logger.error(f"é¢„å–çº¿ç¨‹å‡ºé”™: {e}")
            self.prefetch_queue.put(None)
    
    def _preprocess_batch(self, batch):
        """é¢„å¤„ç†batchæ•°æ®"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®é¢„å¤„ç†é€»è¾‘
        # æ¯”å¦‚ç§»åŠ¨åˆ°GPUã€æ•°æ®æ ¼å¼è½¬æ¢ç­‰
        return batch
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        }


class RNAEvaluationMetrics:
    """RNAç»“æ„è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨ - ä¸“ä¸ºRNAç»“æ„é¢„æµ‹è®¾è®¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–RNAè¯„ä¼°æŒ‡æ ‡"""
        self.reset()
    
    def reset(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        self.total_loss = 0.0
        self.total_samples = 0
        self.batch_count = 0
        
        # æŸå¤±åˆ†è§£
        self.loss_components = defaultdict(float)
        
        # RNAæ ¸å¿ƒç»“æ„è¯„ä¼°æŒ‡æ ‡
        self.rmsd_values = []
        self.tm_scores = []  # RNA TM-score
        self.lddt_scores = []  # RNA lDDT
        self.clash_scores = []  # RNA clash score
        
        # ç½®ä¿¡åº¦æŒ‡æ ‡
        self.confidence_scores = []
    
    def update(self, 
              loss: float, 
              batch_size: int,
              loss_breakdown: Dict[str, float] = None,
              predicted_coords: torch.Tensor = None,
              target_coords: torch.Tensor = None,
              confidence_scores: torch.Tensor = None):
        """æ›´æ–°æŒ‡æ ‡"""
        self.total_loss += loss * batch_size
        self.total_samples += batch_size
        self.batch_count += 1
        
        # æ›´æ–°æŸå¤±åˆ†è§£
        if loss_breakdown:
            for component, value in loss_breakdown.items():
                self.loss_components[component] += value * batch_size
        
        # æ›´æ–°RNAç»“æ„æŒ‡æ ‡
        if predicted_coords is not None and target_coords is not None:
            self._update_rna_structure_metrics(predicted_coords, target_coords)
        
        # æ›´æ–°ç½®ä¿¡åº¦æŒ‡æ ‡
        if confidence_scores is not None:
            # æ£€æŸ¥ confidence_scores çš„ç±»å‹
            if hasattr(confidence_scores, 'plddt') and confidence_scores.plddt is not None:
                # ConfidenceHeadLogits å¯¹è±¡ï¼Œæå– plddt åˆ†æ•°
                plddt_scores = confidence_scores.plddt
                if torch.is_tensor(plddt_scores):
                    self.confidence_scores.extend(plddt_scores.flatten().tolist())
            elif torch.is_tensor(confidence_scores):
                # æ™®é€šå¼ é‡
                self.confidence_scores.extend(confidence_scores.flatten().tolist())
    
    def _update_rna_structure_metrics(self, 
                                    predicted_coords: torch.Tensor, 
                                    target_coords: torch.Tensor):
        """æ›´æ–°RNAç»“æ„è¯„ä¼°æŒ‡æ ‡"""
        try:
            # è®¡ç®—RMSD
            rmsd = self._compute_rmsd(predicted_coords, target_coords)
            self.rmsd_values.extend(rmsd.tolist())
            
            # è®¡ç®—RNA TM-score
            tm_scores = self._compute_rna_tm_score(predicted_coords, target_coords)
            self.tm_scores.extend(tm_scores.tolist())
            
            # è®¡ç®—RNA lDDT
            lddt_scores = self._compute_rna_lddt(predicted_coords, target_coords)
            self.lddt_scores.extend(lddt_scores.tolist())
            
            # è®¡ç®—RNA clash score
            clash_scores = self._compute_rna_clash_score(predicted_coords)
            self.clash_scores.extend(clash_scores.tolist())
            
        except Exception as e:
            logger.warning(f"RNAç»“æ„æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
    
    def _compute_rmsd(self, 
                     pred_coords: torch.Tensor, 
                     target_coords: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ç»“æ„å¯¹é½åçš„RMSD (ä½¿ç”¨Kabschç®—æ³•)"""
        # ç¡®ä¿åæ ‡å½¢çŠ¶åŒ¹é…
        if pred_coords.shape != target_coords.shape:
            min_len = min(pred_coords.shape[1], target_coords.shape[1])
            pred_coords = pred_coords[:, :min_len]
            target_coords = target_coords[:, :min_len]
        
        # æ£€æŸ¥åæ ‡ç»´åº¦å’Œæœ‰æ•ˆæ€§
        if pred_coords.dim() != 3 or target_coords.dim() != 3:
            logger.warning(f"åæ ‡ç»´åº¦é”™è¯¯: pred {pred_coords.shape}, target {target_coords.shape}")
            # å›é€€åˆ°åŸå§‹è®¡ç®—
            diff = pred_coords - target_coords
            squared_diff = torch.sum(diff ** 2, dim=-1)
            return torch.sqrt(torch.mean(squared_diff, dim=-1))
        
        if pred_coords.shape[-1] != 3 or target_coords.shape[-1] != 3:
            logger.warning(f"åæ ‡ä¸æ˜¯3D: pred {pred_coords.shape}, target {target_coords.shape}")
            # å›é€€åˆ°åŸå§‹è®¡ç®—
            diff = pred_coords - target_coords
            squared_diff = torch.sum(diff ** 2, dim=-1)
            return torch.sqrt(torch.mean(squared_diff, dim=-1))
        
        # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœåæ ‡å®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›0
        if torch.allclose(pred_coords, target_coords, atol=1e-6):
            return torch.zeros(pred_coords.shape[0], device=pred_coords.device)
        
        try:
            return self._compute_aligned_rmsd_kabsch(pred_coords, target_coords)
        except Exception as e:
            logger.warning(f"Kabschå¯¹é½å¤±è´¥ï¼Œä½¿ç”¨æœªå¯¹é½RMSD: {e}")
            # å›é€€åˆ°åŸå§‹è®¡ç®—
            diff = pred_coords - target_coords
            squared_diff = torch.sum(diff ** 2, dim=-1)
            return torch.sqrt(torch.mean(squared_diff, dim=-1))
    
    def _compute_aligned_rmsd_kabsch(self, 
                                   pred_coords: torch.Tensor, 
                                   target_coords: torch.Tensor) -> torch.Tensor:
        """ä½¿ç”¨æ ‡å‡†Kabschç®—æ³•è®¡ç®—å¯¹é½åçš„RMSD
        
        æ ‡å‡†Kabschç®—æ³•å®ç°ï¼Œå‚è€ƒç»å…¸è®ºæ–‡å’Œå¯é å®ç°
        W. Kabsch (1976) "A solution for the best rotation to relate two sets of vectors"
        
        Args:
            pred_coords: [batch_size, n_atoms, 3] é¢„æµ‹åæ ‡ (è¦è¢«å¯¹é½çš„ç‚¹é›†)
            target_coords: [batch_size, n_atoms, 3] ç›®æ ‡åæ ‡ (å‚è€ƒç‚¹é›†)
            
        Returns:
            rmsd: [batch_size] æ¯ä¸ªæ ·æœ¬çš„å¯¹é½RMSD
        """
        batch_size, n_atoms, _ = pred_coords.shape
        device = pred_coords.device
        
        # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœåæ ‡å®Œå…¨ç›¸åŒï¼Œç›´æ¥è¿”å›0
        if torch.allclose(pred_coords, target_coords, atol=1e-6):
            return torch.zeros(batch_size, device=device)
        
        # 1. è´¨å¿ƒå¯¹é½
        pred_centroid = torch.mean(pred_coords, dim=1, keepdim=True)  # [B, 1, 3]
        target_centroid = torch.mean(target_coords, dim=1, keepdim=True)  # [B, 1, 3]
        
        pred_centered = pred_coords - pred_centroid  # [B, N, 3]
        target_centered = target_coords - target_centroid  # [B, N, 3]
        
        # 2. è®¡ç®—äº¤å‰åæ–¹å·®çŸ©é˜µ H = pred_centered^T @ target_centered
        H = torch.bmm(pred_centered.transpose(-2, -1), target_centered)  # [B, 3, 3]
        
        # 3. SVDåˆ†è§£: H = U @ S @ V^T
        try:
            # ä½¿ç”¨ç¨³å®šçš„SVDåˆ†è§£
            U, S, Vt = torch.linalg.svd(H) if hasattr(torch.linalg, 'svd') else torch.svd(H)
        except Exception as e:
            logger.warning(f"SVDåˆ†è§£å¤±è´¥: {e}")
            # å›é€€åˆ°æœªå¯¹é½RMSD
            diff = pred_coords - target_coords
            return torch.sqrt(torch.mean(torch.sum(diff ** 2, dim=-1), dim=-1))
        
        # 4. è®¡ç®—æ—‹è½¬çŸ©é˜µ R = V @ U^T
        # æ³¨æ„ï¼šè¿™é‡ŒVæ˜¯Vtçš„è½¬ç½®
        V = Vt.transpose(-2, -1)  # [B, 3, 3]
        R = torch.bmm(U, Vt)  # [B, 3, 3]
        
        # 5. å¤„ç†åå°„æƒ…å†µï¼šç¡®ä¿det(R) = +1ï¼ˆå³æ‰‹ç³»ï¼‰
        det_R = torch.det(R)  # [B]
        
        # å¯¹äºdet(R) < 0çš„æƒ…å†µï¼Œä¿®æ­£Vçš„æœ€åä¸€åˆ—
        det_R = torch.det(R)
        neg_mask = det_R < 0
        if neg_mask.any():
            # åœ¨ U çš„æœ€åä¸€åˆ—ä¹˜ -1 å†é‡ç®— R
            U_fix = U.clone()
            U_fix[neg_mask, :, -1] *= -1
            R[neg_mask] = torch.bmm(U_fix[neg_mask], Vt[neg_mask])
        
        # 6. åº”ç”¨æœ€ä¼˜æ—‹è½¬ï¼špred_aligned = pred_centered @ R
        pred_aligned = torch.bmm(pred_centered, R)  # [B, N, 3]
        
        # 7. è®¡ç®—å¯¹é½åçš„RMSD
        diff = pred_aligned - target_centered  # [B, N, 3]
        squared_distances = torch.sum(diff ** 2, dim=-1)  # [B, N]
        rmsd = torch.sqrt(torch.mean(squared_distances, dim=-1))  # [B]
        
        # 8. éªŒè¯ç»“æœçš„åˆç†æ€§
        # è®¡ç®—æœªå¯¹é½RMSDä½œä¸ºä¸Šç•Œ
        unaligned_diff = pred_coords - target_coords
        unaligned_rmsd = torch.sqrt(torch.mean(torch.sum(unaligned_diff ** 2, dim=-1), dim=-1))
        
        # æ­£å¸¸æƒ…å†µä¸‹ï¼Œå¯¹é½åçš„RMSDåº”è¯¥å°äºç­‰äºæœªå¯¹é½çš„RMSD
        # å¦‚æœä¸æ˜¯ï¼Œè¯´æ˜ç®—æ³•æœ‰é—®é¢˜ï¼Œä½†æˆ‘ä»¬ä»ç„¶è¿”å›ç»“æœå¹¶è®°å½•è­¦å‘Š
        worse_alignment = rmsd > unaligned_rmsd * 1.01  # å…è®¸1%çš„æ•°å€¼è¯¯å·®
        if worse_alignment.any():
            logger.warning(f"è­¦å‘Š: {worse_alignment.sum()}/{batch_size} ä¸ªæ ·æœ¬çš„å¯¹é½RMSDå¤§äºæœªå¯¹é½RMSD")
        
        return torch.clamp(rmsd, min=0.0)
    
    def _compute_rna_tm_score(self, 
                         pred_coords: torch.Tensor, 
                         target_coords: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—RNA TM-score (Template Modeling score)
        
        RNA TM-scoreæ˜¯è¡¡é‡ä¸¤ä¸ªRNAç»“æ„ç›¸ä¼¼æ€§çš„æŒ‡æ ‡ï¼ŒèŒƒå›´0-1ï¼Œå€¼è¶Šé«˜è¶Šå¥½
        å‚è€ƒZhang Labçš„RNA-alignæ–¹æ³•ï¼Œä½¿ç”¨é€‚åˆRNAçš„å½’ä¸€åŒ–å› å­
        
        Args:
            pred_coords: [batch_size, n_atoms, 3] é¢„æµ‹åæ ‡
            target_coords: [batch_size, n_atoms, 3] ç›®æ ‡åæ ‡
            
        Returns:
            tm_score: [batch_size] æ¯ä¸ªæ ·æœ¬çš„RNA TM-score
        """
        batch_size, n_atoms, _ = pred_coords.shape
        device = pred_coords.device
        
        # ç¡®ä¿åæ ‡å½¢çŠ¶åŒ¹é…
        if pred_coords.shape != target_coords.shape:
            min_len = min(pred_coords.shape[1], target_coords.shape[1])
            pred_coords = pred_coords[:, :min_len]
            target_coords = target_coords[:, :min_len]
            n_atoms = min_len
        
        try:
            # 1. è·å–æœ€ä¼˜å¯¹é½ï¼ˆä½¿ç”¨ä¸RMSDç›¸åŒçš„å¯¹é½æ–¹æ³•ï¼‰
            pred_centroid = torch.mean(pred_coords, dim=1, keepdim=True)
            target_centroid = torch.mean(target_coords, dim=1, keepdim=True)
            
            pred_centered = pred_coords - pred_centroid
            target_centered = target_coords - target_centroid
            
            # è®¡ç®—æ—‹è½¬çŸ©é˜µ
            H = torch.bmm(pred_centered.transpose(-2, -1), target_centered)
            
            # SVDåˆ†è§£è·å¾—æœ€ä¼˜æ—‹è½¬
            U, S, Vt = torch.linalg.svd(H) if hasattr(torch.linalg, 'svd') else torch.svd(H)
            V = Vt.transpose(-2, -1)
            R = torch.bmm(U, Vt)
            
            # å¤„ç†åå°„
            det_R = torch.det(R)
            neg_mask = det_R < 0
            if neg_mask.any():
                U_fix = U.clone()
                U_fix[neg_mask, :, -1] *= -1
                R[neg_mask] = torch.bmm(U_fix[neg_mask], Vt[neg_mask])
            
            # åº”ç”¨æ—‹è½¬
            pred_aligned = torch.bmm(pred_centered, R)
            
            # 2. è®¡ç®—è·ç¦»
            distances = torch.sqrt(torch.sum((pred_aligned - target_centered) ** 2, dim=-1))  # [B, N]
            
            # 3. RNA TM-scoreè®¡ç®— - ä½¿ç”¨RNAä¼˜åŒ–çš„d0å…¬å¼
            L = n_atoms
            
            # RNAä¸“ç”¨çš„d0è®¡ç®—ï¼ˆåŸºäºRNA-alignæ–¹æ³•ï¼‰
            if L > 30:
                d0 = 1.24 * ((L - 15) ** (1/3)) - 1.8 + 0.3  # å¢åŠ 0.3åŸƒçš„åç§»
                d0 = max(d0, 0.5)  # æœ€å°å€¼ä¿æŒä¸º0.5
            else:
                d0 = 0.5 + 0.3 * (L / 30.0)  # çŸ­åºåˆ—çš„å¹³æ»‘è¿‡æ¸¡
            
            # TM-score = 1/L * sum(1 / (1 + (di/d0)^2))
            d0_squared = d0 ** 2
            tm_scores = torch.mean(1.0 / (1.0 + distances ** 2 / d0_squared), dim=-1)  # [B]
            
            return torch.clamp(tm_scores, min=0.0, max=1.0)
            
        except Exception as e:
            logger.warning(f"RNA TM-scoreè®¡ç®—å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return torch.zeros(batch_size, device=device)
    
    def _compute_rna_lddt(self, 
                     pred_coords: torch.Tensor, 
                     target_coords: torch.Tensor,
                     cutoff_distances: List[float] = None,
                     inclusion_radius: float = None) -> torch.Tensor:
        """è®¡ç®—RNA lDDT (local Distance Difference Test)
        
        RNA lDDTæ˜¯åŸºäºè·ç¦»å·®å¼‚çš„æœ¬åœ°ç»“æ„è´¨é‡æŒ‡æ ‡ï¼ŒèŒƒå›´0-100ï¼Œå€¼è¶Šé«˜è¶Šå¥½
        ä½¿ç”¨é’ˆå¯¹RNAè°ƒæ•´çš„inclusion_radiuså’Œè·ç¦»é˜ˆå€¼
        
        Args:
            pred_coords: [batch_size, n_atoms, 3] é¢„æµ‹åæ ‡
            target_coords: [batch_size, n_atoms, 3] ç›®æ ‡åæ ‡
            cutoff_distances: è·ç¦»å·®å¼‚çš„é˜ˆå€¼åˆ—è¡¨ï¼ˆé»˜è®¤RNAä¼˜åŒ–å€¼ï¼‰
            inclusion_radius: è€ƒè™‘çš„åŸå­å¯¹è·ç¦»åŠå¾„ï¼ˆé»˜è®¤RNAä¼˜åŒ–å€¼ï¼‰
            
        Returns:
            lddt_score: [batch_size] æ¯ä¸ªæ ·æœ¬çš„RNA lDDTåˆ†æ•°
        """
        batch_size, n_atoms, _ = pred_coords.shape
        device = pred_coords.device
        
        # RNAä¸“ç”¨çš„é»˜è®¤å‚æ•°
        if cutoff_distances is None:
            cutoff_distances = [1.0, 2.0, 4.0, 8.0]  # RNAä½¿ç”¨æ›´å¤§çš„é˜ˆå€¼
        
        if inclusion_radius is None:
            inclusion_radius = 20.0  # RNAä½¿ç”¨æ›´å¤§çš„inclusion radius
        
        # ç¡®ä¿åæ ‡å½¢çŠ¶åŒ¹é…
        if pred_coords.shape != target_coords.shape:
            min_len = min(pred_coords.shape[1], target_coords.shape[1])
            pred_coords = pred_coords[:, :min_len]
            target_coords = target_coords[:, :min_len]
            n_atoms = min_len
        
        try:
            # 1. è®¡ç®—æ‰€æœ‰åŸå­å¯¹ä¹‹é—´çš„è·ç¦»
            # pred_distances: [B, N, N]
            pred_diff = pred_coords.unsqueeze(2) - pred_coords.unsqueeze(1)  # [B, N, N, 3]
            pred_distances = torch.sqrt(torch.sum(pred_diff ** 2, dim=-1))  # [B, N, N]
            
            target_diff = target_coords.unsqueeze(2) - target_coords.unsqueeze(1)  # [B, N, N, 3]
            target_distances = torch.sqrt(torch.sum(target_diff ** 2, dim=-1))  # [B, N, N]
            
            # 2. åˆ›å»ºmaskï¼šåªè€ƒè™‘inclusion_radiusèŒƒå›´å†…çš„åŸå­å¯¹
            inclusion_mask = target_distances <= inclusion_radius  # [B, N, N]
            
            # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„è·ç¦»ï¼‰
            diag_mask = torch.eye(n_atoms, device=device).bool().unsqueeze(0).expand(batch_size, -1, -1)
            inclusion_mask = inclusion_mask & (~diag_mask)
            
            # 3. è®¡ç®—è·ç¦»å·®å¼‚
            distance_diff = torch.abs(pred_distances - target_distances)  # [B, N, N]
            
            # 4. å¯¹æ¯ä¸ªé˜ˆå€¼è®¡ç®—ä¿å­˜çš„æ¥è§¦æ•°
            lddt_scores = []
            for cutoff in cutoff_distances:
                preserved = (distance_diff <= cutoff) & inclusion_mask  # [B, N, N]
                preserved_count = torch.sum(preserved.float(), dim=(1, 2))  # [B]
                total_count = torch.sum(inclusion_mask.float(), dim=(1, 2))  # [B]
                
                # é¿å…é™¤é›¶
                score = torch.where(total_count > 0, 
                                  preserved_count / total_count, 
                                  torch.zeros_like(preserved_count))
                lddt_scores.append(score)
            
            # 5. lDDTæ˜¯æ‰€æœ‰é˜ˆå€¼çš„å¹³å‡å€¼ï¼Œä¹˜ä»¥100
            lddt_final = torch.stack(lddt_scores, dim=0).mean(dim=0) * 100.0  # [B]
            
            return torch.clamp(lddt_final, min=0.0, max=100.0)
            
        except Exception as e:
            logger.warning(f"RNA lDDTè®¡ç®—å¤±è´¥: {e}")
            return torch.zeros(batch_size, device=device)
    
    def _compute_rna_clash_score(self, 
                           pred_coords: torch.Tensor,
                           clash_threshold: float = None,
                           vdw_radii: Dict[str, float] = None) -> torch.Tensor:
        """è®¡ç®—RNA clash scoreï¼ˆåŸå­å†²çªåˆ†æ•°ï¼‰
        
        æ£€æµ‹RNAç»“æ„ä¸­è¿‡äºæ¥è¿‘çš„åŸå­å¯¹ï¼Œå€¼è¶Šä½è¶Šå¥½
        ä½¿ç”¨é’ˆå¯¹RNAè°ƒæ•´çš„å†²çªé˜ˆå€¼
        
        Args:
            pred_coords: [batch_size, n_atoms, 3] é¢„æµ‹åæ ‡
            clash_threshold: å†²çªè·ç¦»é˜ˆå€¼ï¼ˆåŸƒï¼‰ï¼Œé»˜è®¤RNAä¼˜åŒ–å€¼
            vdw_radii: åŸå­çš„èŒƒå¾·ååŠå¾„å­—å…¸ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
            
        Returns:
            clash_score: [batch_size] æ¯ä¸ªæ ·æœ¬çš„å†²çªåˆ†æ•°
        """
        batch_size, n_atoms, _ = pred_coords.shape
        device = pred_coords.device
        
        # RNAä¸“ç”¨çš„å†²çªé˜ˆå€¼
        if clash_threshold is None:
            clash_threshold = 2.5  # RNAä½¿ç”¨æ›´å¤§çš„é˜ˆå€¼
        
        try:
            # è®¡ç®—æ‰€æœ‰åŸå­å¯¹ä¹‹é—´çš„è·ç¦»
            coord_diff = pred_coords.unsqueeze(2) - pred_coords.unsqueeze(1)  # [B, N, N, 3]
            distances = torch.sqrt(torch.sum(coord_diff ** 2, dim=-1))  # [B, N, N]
            
            # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±ï¼‰
            mask = torch.eye(n_atoms, device=device).bool().unsqueeze(0).expand(batch_size, -1, -1)
            distances = distances.masked_fill(mask, float('inf'))
            
            # æ£€æµ‹å†²çª
            clashes = distances < clash_threshold  # [B, N, N]
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å†²çªæ•°é‡ï¼ˆé™¤ä»¥2å› ä¸ºæ¯ä¸ªå†²çªè¢«è®¡ç®—äº†ä¸¤æ¬¡ï¼‰
            clash_count = torch.sum(clashes.float(), dim=(1, 2)) / 2.0  # [B]
            
            # å½’ä¸€åŒ–ï¼šé™¤ä»¥å¯èƒ½çš„åŸå­å¯¹æ•°é‡
            total_pairs = n_atoms * (n_atoms - 1) / 2.0
            clash_score = clash_count / total_pairs * 100.0  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            
            return clash_score
            
        except Exception as e:
            logger.warning(f"RNA Clash scoreè®¡ç®—å¤±è´¥: {e}")
            return torch.zeros(batch_size, device=device)
    
    def compute_metrics(self) -> Dict[str, float]:
        """è®¡ç®—æœ€ç»ˆRNAè¯„ä¼°æŒ‡æ ‡"""
        if self.total_samples == 0:
            return {}
        
        metrics = {
            'avg_loss': self.total_loss / self.total_samples,
            'batch_count': self.batch_count,
            'total_samples': self.total_samples,
            'structure_type': 'RNA'  # ä¸“é—¨ä¸ºRNAè®¾è®¡
        }
        
        # æ·»åŠ æŸå¤±åˆ†è§£
        for component, total_value in self.loss_components.items():
            metrics[f'avg_{component}'] = total_value / self.total_samples
        
        # æ·»åŠ RNAç»“æ„æŒ‡æ ‡
        if self.rmsd_values:
            import numpy as np
            metrics['avg_rmsd'] = np.mean(self.rmsd_values)
            metrics['median_rmsd'] = np.median(self.rmsd_values)
            metrics['std_rmsd'] = np.std(self.rmsd_values)
        
        # æ·»åŠ RNA TM-scoreæŒ‡æ ‡
        if self.tm_scores:
            import numpy as np
            metrics['avg_tm_score'] = np.mean(self.tm_scores)
            metrics['median_tm_score'] = np.median(self.tm_scores)
            metrics['std_tm_score'] = np.std(self.tm_scores)
            
            # RNAç‰¹æœ‰çš„é˜ˆå€¼è¯„ä¼°
            # RNA family similarity threshold (â‰¥0.45 for same Rfam family)
            good_predictions = np.sum(np.array(self.tm_scores) >= 0.45)
            metrics['tm_score_good_ratio'] = good_predictions / len(self.tm_scores)
            
            # Excellent predictions (â‰¥0.6)
            excellent_predictions = np.sum(np.array(self.tm_scores) >= 0.6)
            metrics['tm_score_excellent_ratio'] = excellent_predictions / len(self.tm_scores)
        
        # æ·»åŠ RNA lDDTæŒ‡æ ‡
        if self.lddt_scores:
            import numpy as np
            metrics['avg_lddt'] = np.mean(self.lddt_scores)
            metrics['median_lddt'] = np.median(self.lddt_scores)
            metrics['std_lddt'] = np.std(self.lddt_scores)
            
            # lDDTè´¨é‡åˆ†çº§
            lddt_array = np.array(self.lddt_scores)
            metrics['lddt_high_quality_ratio'] = np.sum(lddt_array >= 70.0) / len(self.lddt_scores)  # é«˜è´¨é‡
            metrics['lddt_good_quality_ratio'] = np.sum(lddt_array >= 50.0) / len(self.lddt_scores)   # è‰¯å¥½è´¨é‡
        
        # æ·»åŠ RNA clash scoreæŒ‡æ ‡
        if self.clash_scores:
            import numpy as np
            metrics['avg_clash_score'] = np.mean(self.clash_scores)
            metrics['median_clash_score'] = np.median(self.clash_scores)
            metrics['std_clash_score'] = np.std(self.clash_scores)
            
            # Clash scoreè´¨é‡è¯„ä¼° (å€¼è¶Šä½è¶Šå¥½)
            clash_array = np.array(self.clash_scores)
            metrics['clash_low_ratio'] = np.sum(clash_array <= 5.0) / len(self.clash_scores)  # ä½å†²çªæ¯”ä¾‹
        
        # æ·»åŠ ç½®ä¿¡åº¦æŒ‡æ ‡
        if self.confidence_scores:
            import numpy as np
            metrics['avg_confidence'] = np.mean(self.confidence_scores)
            metrics['median_confidence'] = np.median(self.confidence_scores)
        
        return metrics


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿æŒEvaluationMetricsåˆ«å
EvaluationMetrics = RNAEvaluationMetrics 