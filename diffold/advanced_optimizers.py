"""
é«˜çº§ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨æ¨¡å—
åŒ…å«å­¦ä¹ ç‡é¢„çƒ­ã€æ”¹è¿›çš„è°ƒåº¦å™¨ã€æ•°æ®é¢„åŠ è½½ä¼˜åŒ–ç­‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
import math
import warnings
from typing import Dict, List, Optional, Any, Tuple, Union
import logging
from pathlib import Path
import threading
import queue
import time
from collections import defaultdict

logger = logging.getLogger(__name__)


class WarmupLRScheduler(_LRScheduler):
    """å­¦ä¹ ç‡é¢„çƒ­è°ƒåº¦å™¨"""
    
    def __init__(self, 
                 optimizer: optim.Optimizer,
                 warmup_steps: int,
                 base_scheduler: _LRScheduler = None,
                 warmup_start_lr: float = 1e-7,
                 last_epoch: int = -1,
                 verbose: bool = False):
        """
        Args:
            optimizer: ä¼˜åŒ–å™¨
            warmup_steps: é¢„çƒ­æ­¥æ•°
            base_scheduler: é¢„çƒ­åä½¿ç”¨çš„åŸºç¡€è°ƒåº¦å™¨
            warmup_start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
            last_epoch: ä¸Šæ¬¡epochç´¢å¼•
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.warmup_steps = warmup_steps
        self.base_scheduler = base_scheduler
        self.warmup_start_lr = warmup_start_lr
        
        # ä¿å­˜åˆå§‹å­¦ä¹ ç‡
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]
        
        # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„PyTorch
        try:
            super().__init__(optimizer, last_epoch, verbose)
        except TypeError:
            # æ–°ç‰ˆæœ¬PyTorchä¸æ”¯æŒverboseå‚æ•°
            super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿ - ä¿®å¤: ä½¿ç”¨ (last_epoch + 1) ç¡®ä¿è¾¾åˆ°ç›®æ ‡å­¦ä¹ ç‡
            progress = (self.last_epoch + 1) / self.warmup_steps
            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * progress
                    for base_lr in self.base_lrs]
        else:
            # é¢„çƒ­ç»“æŸåä½¿ç”¨åŸºç¡€è°ƒåº¦å™¨
            if self.base_scheduler is not None:
                # è°ƒæ•´åŸºç¡€è°ƒåº¦å™¨çš„æ­¥æ•°ï¼ˆä»0å¼€å§‹ï¼‰
                effective_step = self.last_epoch - self.warmup_steps
                self.base_scheduler.last_epoch = effective_step
                return self.base_scheduler.get_lr()
            else:
                return self.base_lrs
    
    def step(self, epoch=None):
        super().step(epoch)
        
        # å¦‚æœæœ‰åŸºç¡€è°ƒåº¦å™¨ä¸”è¿‡äº†é¢„çƒ­æœŸï¼ŒåŒæ­¥æ›´æ–°
        if (self.base_scheduler is not None and 
            self.last_epoch >= self.warmup_steps):
            self.base_scheduler.step()


class CosineAnnealingWarmRestarts(_LRScheduler):
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«é‡å¯è°ƒåº¦å™¨"""
    
    def __init__(self, 
                 optimizer: optim.Optimizer,
                 T_0: int,
                 T_mult: int = 1,
                 eta_min: float = 0,
                 warmup_steps: int = 0,
                 warmup_start_lr: float = 1e-7,
                 last_epoch: int = -1):
        """
        Args:
            T_0: ç¬¬ä¸€æ¬¡é‡å¯å‰çš„epochæ•°
            T_mult: é‡å¯åå‘¨æœŸçš„ä¹˜æ•°
            eta_min: æœ€å°å­¦ä¹ ç‡
            warmup_steps: æ¯æ¬¡é‡å¯åçš„é¢„çƒ­æ­¥æ•°
            warmup_start_lr: é¢„çƒ­èµ·å§‹å­¦ä¹ ç‡
        """
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.warmup_steps = warmup_steps
        self.warmup_start_lr = warmup_start_lr
        
        # è·Ÿè¸ªå½“å‰å‘¨æœŸ
        self.T_cur = 0
        self.T_i = T_0
        self.cycle = 0
        
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.T_cur < self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µ - ä¿®å¤: ä½¿ç”¨ (T_cur + 1) ç¡®ä¿è¾¾åˆ°ç›®æ ‡å­¦ä¹ ç‡
            progress = (self.T_cur + 1) / self.warmup_steps
            return [self.warmup_start_lr + (base_lr - self.warmup_start_lr) * progress
                    for base_lr in self.base_lrs]
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            effective_t = self.T_cur - self.warmup_steps
            effective_T_i = self.T_i - self.warmup_steps
            
            return [self.eta_min + (base_lr - self.eta_min) * 
                    (1 + math.cos(math.pi * effective_t / effective_T_i)) / 2
                    for base_lr in self.base_lrs]
    
    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1
        
        self.T_cur += 1
        
        if self.T_cur >= self.T_i:
            # é‡å¯
            self.cycle += 1
            self.T_cur = 0
            self.T_i = self.T_0 * (self.T_mult ** self.cycle)
            
        self.last_epoch = epoch
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr


class AdaptiveOptimizer:
    """è‡ªé€‚åº”ä¼˜åŒ–å™¨åŒ…è£…å™¨"""
    
    def __init__(self, 
                 model: nn.Module,
                 optimizer_name: str = "adamw",
                 learning_rate: float = 1e-4,
                 weight_decay: float = 1e-5,
                 scheduler_config: Dict[str, Any] = None,
                 gradient_accumulation_steps: int = 1,
                 max_grad_norm: float = 1.0):
        """
        Args:
            model: è¦ä¼˜åŒ–çš„æ¨¡å‹
            optimizer_name: ä¼˜åŒ–å™¨åç§° ("adamw", "adam", "sgd", "lion")
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            scheduler_config: è°ƒåº¦å™¨é…ç½®
            gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
            max_grad_norm: æœ€å¤§æ¢¯åº¦èŒƒæ•°
        """
        self.model = model
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.max_grad_norm = max_grad_norm
        self.accumulated_steps = 0
        
        # è·å–å¯è®­ç»ƒå‚æ•°
        if hasattr(model, 'get_trainable_parameters'):
            trainable_params = model.get_trainable_parameters()
        else:
            trainable_params = model.parameters()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer(
            optimizer_name, trainable_params, learning_rate, weight_decay
        )
        
        # åˆ›å»ºè°ƒåº¦å™¨
        self.scheduler = self._create_scheduler(scheduler_config)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'grad_norm_history': [],
            'lr_history': [],
            'param_norm_history': [],
            'update_count': 0
        }
    
    def _create_optimizer(self, 
                         optimizer_name: str, 
                         params, 
                         lr: float, 
                         weight_decay: float) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_name = optimizer_name.lower()
        
        if optimizer_name == "adamw":
            return optim.AdamW(
                params, lr=lr, weight_decay=weight_decay,
                betas=(0.9, 0.999), eps=1e-8
            )
        elif optimizer_name == "adam":
            return optim.Adam(
                params, lr=lr, weight_decay=weight_decay,
                betas=(0.9, 0.999), eps=1e-8
            )
        elif optimizer_name == "sgd":
            return optim.SGD(
                params, lr=lr, weight_decay=weight_decay,
                momentum=0.9, nesterov=True
            )
        elif optimizer_name == "lion":
            try:
                from lion_pytorch import Lion
                return Lion(params, lr=lr, weight_decay=weight_decay)
            except ImportError:
                logger.warning("Lion optimizerä¸å¯ç”¨ï¼Œå›é€€åˆ°AdamW")
                return optim.AdamW(
                    params, lr=lr, weight_decay=weight_decay,
                    betas=(0.9, 0.999), eps=1e-8
                )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")
    
    def _create_scheduler(self, config: Dict[str, Any] = None) -> Optional[_LRScheduler]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if config is None:
            return None
        
        scheduler_type = config.get('type', 'cosine')
        
        if scheduler_type == 'warmup_cosine':
            # è®¡ç®—é¢„çƒ­åçš„æ€»æ­¥æ•°
            total_steps = config.get('T_max', 100)
            warmup_steps = config.get('warmup_steps', 100)
            remaining_steps = total_steps - warmup_steps
            
            base_scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=remaining_steps,  # é¢„çƒ­åçš„å‰©ä½™æ­¥æ•°
                eta_min=config.get('eta_min', 1e-6)
            )
            return WarmupLRScheduler(
                self.optimizer,
                warmup_steps=warmup_steps,
                base_scheduler=base_scheduler,
                warmup_start_lr=config.get('warmup_start_lr', 1e-7)
            )
        
        elif scheduler_type == 'warmup_cosine_restarts':
            return CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=config.get('T_0', 50),
                T_mult=config.get('T_mult', 2),
                eta_min=config.get('eta_min', 1e-6),
                warmup_steps=config.get('warmup_steps', 100),
                warmup_start_lr=config.get('warmup_start_lr', 1e-6)
            )
        
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=config.get('factor', 0.5),
                patience=config.get('patience', 10),
                verbose=True
            )
        
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=config.get('T_max', 100),
                eta_min=config.get('eta_min', 1e-6)
            )
        
        else:
            logger.warning(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")
            return None
    
    def zero_grad(self):
        """æ¸…é›¶æ¢¯åº¦"""
        self.optimizer.zero_grad()
    
    def backward(self, loss: torch.Tensor):
        """åå‘ä¼ æ’­"""
        # ç¼©æ”¾æŸå¤±ä»¥è€ƒè™‘æ¢¯åº¦ç´¯ç§¯
        scaled_loss = loss / self.gradient_accumulation_steps
        scaled_loss.backward()
        
        self.accumulated_steps += 1
        
        # æ¢¯åº¦ç´¯ç§¯
        if self.accumulated_steps >= self.gradient_accumulation_steps:
            self.step()
            self.accumulated_steps = 0
    
    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = self._compute_grad_norm()
        self.stats['grad_norm_history'].append(grad_norm)
        
        # æ¢¯åº¦è£å‰ª
        if self.max_grad_norm > 0:
            if hasattr(self.model, 'get_trainable_parameters'):
                torch.nn.utils.clip_grad_norm_(
                    self.model.get_trainable_parameters(), 
                    self.max_grad_norm
                )
            else:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.max_grad_norm
                )
        
        # ä¼˜åŒ–å™¨æ­¥éª¤
        self.optimizer.step()
        self.stats['update_count'] += 1
        
        # è°ƒåº¦å™¨æ­¥éª¤ï¼ˆå¯¹äºstep-basedè°ƒåº¦å™¨ï¼‰
        if self.scheduler is not None:
            # å¯¹äºstep-basedè°ƒåº¦å™¨ï¼Œåœ¨æ¯ä¸ªä¼˜åŒ–å™¨æ­¥éª¤åè°ƒç”¨
            if isinstance(self.scheduler, (WarmupLRScheduler, CosineAnnealingWarmRestarts)):
                self.scheduler.step()
            # å¯¹äºepoch-basedè°ƒåº¦å™¨ï¼ˆå¦‚ReduceLROnPlateauï¼‰ï¼Œä¸åœ¨è¿™é‡Œè°ƒç”¨
        
        # è®°å½•å­¦ä¹ ç‡
        current_lr = self.optimizer.param_groups[0]['lr']
        self.stats['lr_history'].append(current_lr)
        
        # è®°å½•å‚æ•°èŒƒæ•°
        param_norm = self._compute_param_norm()
        self.stats['param_norm_history'].append(param_norm)
    
    def scheduler_step(self, metrics: float = None):
        """è°ƒåº¦å™¨æ­¥éª¤"""
        if self.scheduler is not None:
            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                if metrics is not None:
                    self.scheduler.step(metrics)
            else:
                self.scheduler.step()
    
    def _compute_grad_norm(self) -> float:
        """è®¡ç®—æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        
        if hasattr(self.model, 'get_trainable_parameters'):
            parameters = self.model.get_trainable_parameters()
        else:
            parameters = self.model.parameters()
        
        for param in parameters:
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        
        return total_norm ** (1. / 2)
    
    def _compute_param_norm(self) -> float:
        """è®¡ç®—å‚æ•°èŒƒæ•°"""
        total_norm = 0.0
        
        if hasattr(self.model, 'get_trainable_parameters'):
            parameters = self.model.get_trainable_parameters()
        else:
            parameters = self.model.parameters()
        
        for param in parameters:
            param_norm = param.data.norm(2)
            total_norm += param_norm.item() ** 2
        
        return total_norm ** (1. / 2)
    
    def get_lr(self) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        return self.optimizer.param_groups[0]['lr']
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if self.stats['grad_norm_history']:
            stats['avg_grad_norm'] = sum(self.stats['grad_norm_history']) / len(self.stats['grad_norm_history'])
            stats['max_grad_norm'] = max(self.stats['grad_norm_history'])
        
        if self.stats['param_norm_history']:
            stats['avg_param_norm'] = sum(self.stats['param_norm_history']) / len(self.stats['param_norm_history'])
        
        return stats
    
    def load_stats(self, stats_dict: Dict[str, Any]):
        """åŠ è½½ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats_dict: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        # æ¢å¤åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        for key in ['update_count', 'grad_norm_history', 'lr_history', 'param_norm_history']:
            if key in stats_dict:
                self.stats[key] = stats_dict[key]
        
        # æ¢å¤ç´¯ç§¯æ­¥æ•°
        self.accumulated_steps = stats_dict.get('accumulated_steps', 0)
        
        logger.info(f"ğŸ“Š åŠ è½½ä¼˜åŒ–å™¨ç»Ÿè®¡: æ›´æ–°æ¬¡æ•°={self.stats['update_count']}, "
                   f"æ¢¯åº¦å†å²={len(self.stats['grad_norm_history'])}, "
                   f"å­¦ä¹ ç‡å†å²={len(self.stats['lr_history'])}")
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'update_count': 0,
            'grad_norm_history': [],
            'lr_history': [],
            'param_norm_history': []
        }
        self.accumulated_steps = 0
        logger.info("ğŸ“Š ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


class DataLoaderOptimizer:
    """æ•°æ®åŠ è½½å™¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 base_dataloader,
                 prefetch_factor: int = 2,
                 cache_size: int = 100,
                 enable_prefetch: bool = True):
        """
        Args:
            base_dataloader: åŸºç¡€æ•°æ®åŠ è½½å™¨
            prefetch_factor: é¢„å–å› å­
            cache_size: ç¼“å­˜å¤§å°
            enable_prefetch: æ˜¯å¦å¯ç”¨é¢„å–
        """
        self.base_dataloader = base_dataloader
        self.prefetch_factor = prefetch_factor
        self.cache_size = cache_size
        self.enable_prefetch = enable_prefetch
        
        # ç¼“å­˜
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # é¢„å–é˜Ÿåˆ—
        self.prefetch_queue = queue.Queue(maxsize=prefetch_factor * 2)
        self.prefetch_thread = None
        self.stop_prefetch = threading.Event()
    
    def __iter__(self):
        if self.enable_prefetch:
            return self._prefetch_iter()
        else:
            return iter(self.base_dataloader)
    
    def __len__(self):
        return len(self.base_dataloader)
    
    def _prefetch_iter(self):
        """é¢„å–è¿­ä»£å™¨"""
        # å¯åŠ¨é¢„å–çº¿ç¨‹
        self.stop_prefetch.clear()
        self.prefetch_thread = threading.Thread(target=self._prefetch_worker)
        self.prefetch_thread.start()
        
        try:
            while True:
                try:
                    batch = self.prefetch_queue.get(timeout=30)  # 30ç§’è¶…æ—¶
                    if batch is None:  # ç»“æŸæ ‡å¿—
                        break
                    yield batch
                except queue.Empty:
                    logger.warning("é¢„å–é˜Ÿåˆ—è¶…æ—¶ï¼Œå¯èƒ½å‡ºç°æ€§èƒ½é—®é¢˜")
                    break
        finally:
            # åœæ­¢é¢„å–çº¿ç¨‹
            self.stop_prefetch.set()
            if self.prefetch_thread and self.prefetch_thread.is_alive():
                self.prefetch_thread.join(timeout=5)
    
    def _prefetch_worker(self):
        """é¢„å–å·¥ä½œçº¿ç¨‹"""
        try:
            for batch in self.base_dataloader:
                if self.stop_prefetch.is_set():
                    break
                
                # æ•°æ®é¢„å¤„ç†ï¼ˆå¦‚ç§»åŠ¨åˆ°GPUï¼‰
                processed_batch = self._preprocess_batch(batch)
                
                # æ”¾å…¥é˜Ÿåˆ—
                self.prefetch_queue.put(processed_batch)
            
            # å‘é€ç»“æŸä¿¡å·
            self.prefetch_queue.put(None)
            
        except Exception as e:
            logger.error(f"é¢„å–çº¿ç¨‹å‡ºé”™: {e}")
            self.prefetch_queue.put(None)
    
    def _preprocess_batch(self, batch):
        """é¢„å¤„ç†batchæ•°æ®"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®é¢„å¤„ç†é€»è¾‘
        # æ¯”å¦‚ç§»åŠ¨åˆ°GPUã€æ•°æ®æ ¼å¼è½¬æ¢ç­‰
        return batch
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        } 
        