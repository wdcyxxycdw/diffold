from torch import nn
import torch
import os
import logging
from typing import NamedTuple
import torch.nn.functional as F

from alphafold3_pytorch import(
    InputFeatureEmbedder,
    ElucidatedAtomDiffusion,
    DiffusionModule,
    RelativePositionEncoding,
    ConfidenceHead,
    DistogramHead,
    ComputeAlignmentError,
)
from alphafold3_pytorch.alphafold3 import(
    to_pairwise_mask,
    masked_average,
    batch_repeat_interleave,
    distance_to_dgram,
    IS_RNA_INDEX,
    IS_DNA_INDEX,
)
from alphafold3_pytorch.inputs import NUM_MOLECULE_IDS
from .input_processor import process_alphafold3_input, process_multiple_alphafold3_inputs, align_input
from .mask_validator import MaskValidator

from rhofold.rhofold import RhoFold

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# æŸå¤±åˆ†è§£ç±»ï¼Œä»¿ç…§AlphaFold3çš„LossBreakdown
class DiffoldLossBreakdown(NamedTuple):
    total_loss: torch.Tensor
    total_diffusion: torch.Tensor
    distogram: torch.Tensor
    pae: torch.Tensor
    pde: torch.Tensor
    plddt: torch.Tensor
    resolved: torch.Tensor
    confidence: torch.Tensor
    diffusion_mse: torch.Tensor
    diffusion_bond: torch.Tensor
    diffusion_smooth_lddt: torch.Tensor

class Diffold(nn.Module):
    def __init__(self, config, rhofold_checkpoint_path=None):
        super().__init__()

        self.config = config

        dim_atom_inputs = 3
        atoms_per_window = 27
        dim_atom = 128
        dim_atompair_inputs = 5
        dim_atompair = 16
        dim_input_embedder_token = 384
        dim_single = 384
        dim_pairwise = 128
        dim_token = 768
        sigma_data = 16
        num_molecule_types: int = NUM_MOLECULE_IDS

        dim_additional_token_feats = 33
        dim_single_inputs = dim_input_embedder_token + dim_additional_token_feats
        lddt_mask_nucleic_acid_cutoff = 30.0
        lddt_mask_other_cutoff = 15.0

        # ç½®ä¿¡åº¦ç›¸å…³å‚æ•°
        self.num_plddt_bins = 50
        self.num_pae_bins = 64
        self.num_pde_bins = 64
        self.ignore_index = -1
        
        # è·ç¦»å’ŒPAE/PDEçš„binsï¼ˆä¿æŒä¸ºtensoræ ¼å¼ï¼‰
        self.register_buffer('distance_bins', torch.linspace(2, 22, 64).float())
        self.register_buffer('pae_bins', torch.linspace(0.5, 32, 64).float())
        self.register_buffer('pde_bins', torch.linspace(0.5, 32, 64).float())
        
        # LDDTè®¡ç®—ç›¸å…³å‚æ•°
        self.lddt_mask_nucleic_acid_cutoff = lddt_mask_nucleic_acid_cutoff
        self.lddt_mask_other_cutoff = lddt_mask_other_cutoff
        
        # æŸå¤±æƒé‡
        self.loss_confidence_weight = 1e-4
        self.loss_distogram_weight = 1e-2
        self.loss_diffusion_weight = 4.0

        # ä½¿ç”¨ RhoFold è‡ªå¸¦çš„é…ç½®
        from rhofold.config import rhofold_config
        self.rhofold = RhoFold(rhofold_config)
        
        # åŠ è½½RhoFoldé¢„è®­ç»ƒæƒé‡
        if rhofold_checkpoint_path is None:
            rhofold_checkpoint_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'pretrained', 'model_20221010_params.pt')
        
        if os.path.exists(rhofold_checkpoint_path):
            logger.info(f"æ­£åœ¨åŠ è½½RhoFoldé¢„è®­ç»ƒæƒé‡: {rhofold_checkpoint_path}")
            checkpoint = torch.load(rhofold_checkpoint_path, map_location='cpu', weights_only=False)
            if 'model' in checkpoint:
                self.rhofold.load_state_dict(checkpoint['model'])
            else:
                self.rhofold.load_state_dict(checkpoint)
            logger.info("âœ“ RhoFoldé¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ")
        else:
            logger.warning(f"âš  è­¦å‘Š: æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡æ–‡ä»¶: {rhofold_checkpoint_path}")
        
        # å›ºå®šRhoFoldæ¨¡å‹å‚æ•°ï¼Œä¸å‚ä¸è®­ç»ƒ
        for param in self.rhofold.parameters():
            param.requires_grad = False
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.rhofold.eval()

        self.relative_position_encoding = RelativePositionEncoding(
            dim_out = dim_pairwise,
            r_max = 32,
            s_max = 2,
        )

        self.input_embedder = InputFeatureEmbedder(
            num_molecule_types = num_molecule_types,
            dim_atom_inputs = dim_atom_inputs,
            dim_atompair_inputs = dim_atompair_inputs,
            atoms_per_window = atoms_per_window,
            dim_atom = dim_atom,
            dim_atompair = dim_atompair,
            dim_token = dim_input_embedder_token,
            dim_single = dim_single,
            dim_pairwise = dim_pairwise,
            dim_additional_token_feats = dim_additional_token_feats,
            atom_transformer_blocks = 3,
            atom_transformer_heads = 4,
            atom_transformer_kwargs = dict()
        )

        self.diffusion = DiffusionModule(
            dim_pairwise_trunk=dim_pairwise,
            dim_pairwise_rel_pos_feats=dim_pairwise,
            atoms_per_window=atoms_per_window,
            dim_pairwise=dim_pairwise,
            sigma_data=sigma_data,
            dim_atom=dim_atom,
            dim_atompair=dim_atompair,
            dim_token=dim_token,
            dim_single=dim_single + dim_single_inputs,
            checkpoint=False,
            single_cond_kwargs=dict(
                num_transitions=2,
                transition_expansion_factor=2,
            ),
            pairwise_cond_kwargs=dict(
                num_transitions=2
            ),
            atom_encoder_depth=3,
            atom_encoder_heads=4,
            token_transformer_depth=16,
            token_transformer_heads=16,
            atom_decoder_depth=3,
            atom_decoder_heads=4,
        )

        self.edm = ElucidatedAtomDiffusion(
            self.diffusion,
            sigma_data=sigma_data,
            smooth_lddt_loss_kwargs=dict(
                nucleic_acid_cutoff=lddt_mask_nucleic_acid_cutoff,
                other_cutoff=lddt_mask_other_cutoff,
            ),
            sigma_min=0.002,
            sigma_max=80,
            rho=7,
            P_mean=-1.2,
            P_std=1.2,
            S_churn=80,
            S_tmin=0.05,
            S_tmax=50,
            S_noise=1.003,
        )
        
        # æ·»åŠ è®­ç»ƒç›¸å…³çš„æƒé‡å‚æ•°
        self.nucleotide_loss_weight = 1.0
        self.ligand_loss_weight = 1.0
        
        # æ·»åŠ ç»´åº¦é€‚é…å±‚ï¼Œå°†RhoFoldçš„è¾“å‡ºè°ƒæ•´åˆ°diffusionæ¨¡å—æœŸæœ›çš„ç»´åº¦
        self.single_dim_adapter = nn.Linear(256, 384)
        
        # æ·»åŠ ç½®ä¿¡åº¦å¤´éƒ¨ï¼Œä»¿ç…§AlphaFold3
        # ConfidenceHeadéœ€è¦åˆ—è¡¨æ ¼å¼çš„bins
        distance_bins_list = torch.linspace(2, 22, 64).float().tolist()
        self.confidence_head = ConfidenceHead(
            dim_single_inputs=dim_single_inputs,
            dim_atom=dim_atom,
            atompair_dist_bins=distance_bins_list,
            dim_single=dim_single,
            dim_pairwise=dim_pairwise,
            num_plddt_bins=self.num_plddt_bins,
            num_pde_bins=self.num_pde_bins,
            num_pae_bins=self.num_pae_bins,
            pairformer_depth=4,
            pairformer_kwargs=dict(),
            checkpoint=False
        )
        
        # æ·»åŠ  Distogram å¤´éƒ¨
        self.distogram_head = DistogramHead(
            dim_pairwise=dim_pairwise,
            num_dist_bins=len(distance_bins_list),
            dim_atom=dim_atom,
            atom_resolution=False,  # åœ¨ token çº§åˆ«é¢„æµ‹è·ç¦»
            checkpoint=False
        )
        
        # æ·»åŠ å¯¹é½è¯¯å·®è®¡ç®—æ¨¡å—
        self.compute_alignment_error = ComputeAlignmentError()
        
        # LDDTé˜ˆå€¼å¸¸é‡
        self.lddt_threshold_values = [0.5, 1.0, 2.0, 4.0]
        
        # æ·»åŠ maskéªŒè¯å™¨
        self.mask_validator = MaskValidator(
            enable_warnings=True,
            enable_logging=True,
            strict_mode=True  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥è®¾ä¸ºTrue
        )
        
    def _compute_confidence_loss(self, single_fea, single_inputs, pair_fea, denoised_atom_pos, atom_feats,
                                atom_pos_ground_truth, molecule_atom_indices, molecule_atom_lens,
                                is_molecule_types, additional_molecule_feats, mask, atom_mask):
        """è®¡ç®—ç½®ä¿¡åº¦æŸå¤±ï¼Œä»¿ç…§AlphaFold3çš„æ–¹å¼"""
        device = single_fea.device
        batch_size = single_fea.shape[0]
        atom_seq_len = atom_feats.shape[1]
        
        # è®¡ç®—ç½®ä¿¡åº¦å¤´éƒ¨çš„è¾“å‡º
        ch_logits = self.confidence_head(
            single_repr=single_fea.detach(),
            single_inputs_repr=single_inputs.detach(),
            pairwise_repr=pair_fea.detach(),
            pred_atom_pos=denoised_atom_pos.detach(),
            atom_feats=atom_feats.detach(),
            molecule_atom_indices=molecule_atom_indices,
            molecule_atom_lens=molecule_atom_lens,
            mask=mask,
            return_pae_logits=True
        )
        
        # è®¡ç®—distogramå¤´éƒ¨çš„è¾“å‡º
        distogram_logits = self.distogram_head(
            pairwise_repr=pair_fea.detach(),
            molecule_atom_lens=molecule_atom_lens,
            atom_feats=atom_feats.detach()
        )
        
        # è®¡ç®—PAEæ ‡ç­¾
        pae_labels = self._compute_pae_labels(
            denoised_atom_pos, atom_pos_ground_truth, molecule_atom_indices, mask
        )
        
        # è®¡ç®—PDEæ ‡ç­¾
        pde_labels = self._compute_pde_labels(
            denoised_atom_pos, atom_pos_ground_truth, molecule_atom_indices, mask
        )
        
        # è®¡ç®—pLDDTæ ‡ç­¾
        plddt_labels = self._compute_plddt_labels(
            denoised_atom_pos, atom_pos_ground_truth, is_molecule_types, molecule_atom_lens, atom_mask
        )
        
        # è®¡ç®—distogramæ ‡ç­¾
        distogram_labels = self._compute_distogram_labels(
            denoised_atom_pos, atom_pos_ground_truth, molecule_atom_indices, mask
        )
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        ignore_index = self.ignore_index
        label_pairwise_mask = to_pairwise_mask(mask)
        
        # PAEæŸå¤±
        if ch_logits.pae is not None:
            # æ­£ç¡®å¤„ç†PAE logitsçš„å½¢çŠ¶ï¼š[batch, bins, seq, seq] -> [batch, seq, seq, bins]
            pae_logits_reordered = ch_logits.pae.permute(0, 2, 3, 1)  # [batch, seq, seq, bins]
            pae_logits_flat = pae_logits_reordered.reshape(-1, pae_logits_reordered.shape[-1])  # [batch*seq*seq, bins]
            pae_labels_flat = pae_labels.reshape(-1)  # [batch*seq*seq]
            
            # å¤„ç†æ ‡ç­¾ä¸­çš„è¶Šç•Œå€¼
            num_pae_bins = pae_logits_flat.shape[-1]
            pae_labels_flat = torch.clamp(pae_labels_flat, min=0, max=num_pae_bins-1)
            
            pae_loss = F.cross_entropy(
                pae_logits_flat,
                pae_labels_flat,
                ignore_index=ignore_index,
                reduction='mean'
            )
        else:
            pae_loss = torch.tensor(0.0, device=device)
        
        # PDEæŸå¤±
        if ch_logits.pde is not None:
            # æ­£ç¡®å¤„ç†PDE logitsçš„å½¢çŠ¶ï¼š[batch, bins, seq, seq] -> [batch, seq, seq, bins]
            pde_logits_reordered = ch_logits.pde.permute(0, 2, 3, 1)  # [batch, seq, seq, bins]
            pde_logits_flat = pde_logits_reordered.reshape(-1, pde_logits_reordered.shape[-1])  # [batch*seq*seq, bins]
            pde_labels_flat = pde_labels.reshape(-1)  # [batch*seq*seq]
            
            # å¤„ç†æ ‡ç­¾ä¸­çš„è¶Šç•Œå€¼
            num_pde_bins = pde_logits_flat.shape[-1]
            pde_labels_flat = torch.clamp(pde_labels_flat, min=0, max=num_pde_bins-1)
            
            pde_loss = F.cross_entropy(
                pde_logits_flat,
                pde_labels_flat,
                ignore_index=ignore_index,
                reduction='mean'
            )
        else:
            pde_loss = torch.tensor(0.0, device=device)
        
        # pLDDTæŸå¤±
        if ch_logits.plddt is not None:
            # pLDDT logitsçš„å½¢çŠ¶é€šå¸¸æ˜¯[batch, seq, bins]ï¼Œåº”è¯¥ä¸[batch, seq]çš„æ ‡ç­¾åŒ¹é…
            plddt_logits_flat = ch_logits.plddt.reshape(-1, ch_logits.plddt.shape[-1])  # [batch*seq, bins]
            plddt_labels_flat = plddt_labels.reshape(-1)  # [batch*seq]
            
            # å¤„ç†æ ‡ç­¾ä¸­çš„è¶Šç•Œå€¼
            num_plddt_bins = ch_logits.plddt.shape[-1]
            plddt_labels_flat = torch.clamp(plddt_labels_flat, min=0, max=num_plddt_bins-1)
            
            # å¦‚æœå½¢çŠ¶ä»ç„¶ä¸åŒ¹é…ï¼Œä½¿ç”¨æœ€å°å°ºå¯¸
            if plddt_logits_flat.shape[0] != plddt_labels_flat.shape[0]:
                min_size = min(plddt_logits_flat.shape[0], plddt_labels_flat.shape[0])
                plddt_logits_flat = plddt_logits_flat[:min_size]
                plddt_labels_flat = plddt_labels_flat[:min_size]
            
            plddt_loss = F.cross_entropy(
                plddt_logits_flat,
                plddt_labels_flat,
                ignore_index=ignore_index,
                reduction='mean'
            )
        else:
            plddt_loss = torch.tensor(0.0, device=device)
        
        # DistogramæŸå¤±
        if distogram_logits is not None:
            # distogram logitsçš„å½¢çŠ¶é€šå¸¸æ˜¯[batch, bins, seq, seq]ï¼Œéœ€è¦é‡æ–°æ’åˆ—
            distogram_logits_reordered = distogram_logits.permute(0, 2, 3, 1)  # [batch, seq, seq, bins]
            distogram_logits_flat = distogram_logits_reordered.reshape(-1, distogram_logits_reordered.shape[-1])
            distogram_labels_flat = distogram_labels.reshape(-1)
            
            # å¤„ç†æ ‡ç­¾ä¸­çš„è¶Šç•Œå€¼
            num_distogram_bins = distogram_logits_flat.shape[-1]
            distogram_labels_flat = torch.clamp(distogram_labels_flat, min=0, max=num_distogram_bins-1)
            
            distogram_loss = F.cross_entropy(
                distogram_logits_flat,
                distogram_labels_flat,
                ignore_index=ignore_index,
                reduction='mean'
            )
        else:
            distogram_loss = torch.tensor(0.0, device=device)
        
        # ResolvedæŸå¤±ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        resolved_loss = torch.mean(ch_logits.resolved * 0.0)
        
        # æ€»ç½®ä¿¡åº¦æŸå¤±
        total_confidence_loss = pae_loss + pde_loss + plddt_loss + resolved_loss
        
        confidence_results = {
            'pae_loss': pae_loss,
            'pde_loss': pde_loss,
            'plddt_loss': plddt_loss,
            'resolved_loss': resolved_loss,
            'distogram_loss': distogram_loss,
            'logits': ch_logits,
            'distogram_logits': distogram_logits
        }
        
        return total_confidence_loss, confidence_results
    
    def _compute_pae_labels(self, pred_coords, true_coords, molecule_atom_indices, mask):
        """è®¡ç®—PAEæ ‡ç­¾"""
        device = pred_coords.device
        batch_size = pred_coords.shape[0]
        

        # å¤„ç†molecule_atom_indicesä¸­çš„-1å€¼
        # å°†-1æ›¿æ¢ä¸º0ï¼Œå¹¶åˆ›å»ºæœ‰æ•ˆç´¢å¼•çš„mask
        valid_indices_mask = molecule_atom_indices >= 0
        safe_indices = torch.where(valid_indices_mask, molecule_atom_indices, 0)
        
        # æå–åˆ†å­ä½ç½®
        molecule_pos = pred_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        true_molecule_pos = true_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        
        # å¯¹æ— æ•ˆä½ç½®è¿›è¡Œmaskå¤„ç†
        valid_mask = valid_indices_mask.unsqueeze(-1).expand(-1, -1, 3)
        molecule_pos = molecule_pos * valid_mask.float()
        true_molecule_pos = true_molecule_pos * valid_mask.float()
        
        # è®¡ç®—å¯¹é½è¯¯å·®
        align_error = torch.cdist(molecule_pos, true_molecule_pos, p=2)
        
        # è½¬æ¢ä¸ºbins
        pae_labels = distance_to_dgram(align_error, self.pae_bins, return_labels=True)
        
        # åº”ç”¨mask
        pair_mask = to_pairwise_mask(mask)
        # åŒæ—¶è€ƒè™‘æœ‰æ•ˆç´¢å¼•mask
        valid_pair_mask = valid_indices_mask.unsqueeze(-1) & valid_indices_mask.unsqueeze(-2)
        final_mask = pair_mask & valid_pair_mask
        pae_labels = torch.where(final_mask, pae_labels, self.ignore_index)
        
        return pae_labels
    
    def _compute_pde_labels(self, pred_coords, true_coords, molecule_atom_indices, mask):
        """è®¡ç®—PDEæ ‡ç­¾"""
        device = pred_coords.device
        
        # å¤„ç†molecule_atom_indicesä¸­çš„-1å€¼
        # å°†-1æ›¿æ¢ä¸º0ï¼Œå¹¶åˆ›å»ºæœ‰æ•ˆç´¢å¼•çš„mask
        valid_indices_mask = molecule_atom_indices >= 0
        safe_indices = torch.where(valid_indices_mask, molecule_atom_indices, 0)
        
        # æå–åˆ†å­ä½ç½®
        molecule_pos = pred_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        true_molecule_pos = true_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        
        # å¯¹æ— æ•ˆä½ç½®è¿›è¡Œmaskå¤„ç†
        valid_mask = valid_indices_mask.unsqueeze(-1).expand(-1, -1, 3)
        molecule_pos = molecule_pos * valid_mask.float()
        true_molecule_pos = true_molecule_pos * valid_mask.float()
        
        # è®¡ç®—è·ç¦»
        pred_dist = torch.cdist(molecule_pos, molecule_pos, p=2)
        true_dist = torch.cdist(true_molecule_pos, true_molecule_pos, p=2)
        
        # è®¡ç®—è·ç¦»è¯¯å·®
        dist_error = torch.abs(pred_dist - true_dist)
        
        # è½¬æ¢ä¸ºbins
        pde_labels = distance_to_dgram(dist_error, self.pde_bins, return_labels=True)
        
        # åº”ç”¨mask
        pair_mask = to_pairwise_mask(mask)
        # åŒæ—¶è€ƒè™‘æœ‰æ•ˆç´¢å¼•mask
        valid_pair_mask = valid_indices_mask.unsqueeze(-1) & valid_indices_mask.unsqueeze(-2)
        final_mask = pair_mask & valid_pair_mask
        pde_labels = torch.where(final_mask, pde_labels, self.ignore_index)
        
        return pde_labels
    
    def _compute_plddt_labels(self, pred_coords, true_coords, is_molecule_types, molecule_atom_lens, atom_mask):
        """è®¡ç®—pLDDTæ ‡ç­¾"""
        device = pred_coords.device
        
        # è®¡ç®—è·ç¦»
        pred_dists = torch.cdist(pred_coords, pred_coords, p=2)
        true_dists = torch.cdist(true_coords, true_coords, p=2)
        
        # åˆ›å»ºinclusion mask
        is_rna = batch_repeat_interleave(is_molecule_types[..., IS_RNA_INDEX], molecule_atom_lens)
        is_dna = batch_repeat_interleave(is_molecule_types[..., IS_DNA_INDEX], molecule_atom_lens)
        is_nucleotide = is_rna | is_dna
        
        is_any_nucleotide_pair = is_nucleotide.unsqueeze(-1) | is_nucleotide.unsqueeze(-2)
        
        inclusion_radius = torch.where(
            is_any_nucleotide_pair,
            true_dists < self.lddt_mask_nucleic_acid_cutoff,
            true_dists < self.lddt_mask_other_cutoff,
        )
        
        # è®¡ç®—LDDT
        dist_diff = torch.abs(true_dists - pred_dists)
        # ä½¿ç”¨é¢„å®šä¹‰çš„LDDTé˜ˆå€¼
        lddt_thresholds = torch.tensor(self.lddt_threshold_values, device=device)
        lddt = (dist_diff.unsqueeze(-1) < lddt_thresholds).float().mean(dim=-1)
        
        # åº”ç”¨mask
        plddt_mask = inclusion_radius & to_pairwise_mask(atom_mask)
        plddt_mask = plddt_mask & ~torch.eye(pred_coords.shape[1], dtype=torch.bool, device=device)
        
        # è®¡ç®—å¹³å‡LDDT
        lddt_mean = masked_average(lddt, plddt_mask, dim=-1)
        
        # è½¬æ¢ä¸ºbins
        plddt_labels = torch.clamp(
            torch.floor(lddt_mean * self.num_plddt_bins).long(),
            max=self.num_plddt_bins - 1
        )
        
        return plddt_labels
    
    def _compute_distogram_labels(self, pred_coords, true_coords, molecule_atom_indices, mask):
        """è®¡ç®—distogramæ ‡ç­¾"""
        device = pred_coords.device
        
        # å¤„ç†molecule_atom_indicesä¸­çš„-1å€¼
        valid_indices_mask = molecule_atom_indices >= 0
        safe_indices = torch.where(valid_indices_mask, molecule_atom_indices, 0)
        
        # æå–åˆ†å­ä½ç½®
        molecule_pos = pred_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        true_molecule_pos = true_coords.gather(1, safe_indices.unsqueeze(-1).expand(-1, -1, 3))
        
        # å¯¹æ— æ•ˆä½ç½®è¿›è¡Œmaskå¤„ç†
        valid_mask = valid_indices_mask.unsqueeze(-1).expand(-1, -1, 3)
        true_molecule_pos = true_molecule_pos * valid_mask.float()
        
        # è®¡ç®—çœŸå®è·ç¦»
        true_distances = torch.cdist(true_molecule_pos, true_molecule_pos, p=2)
        
        # è½¬æ¢ä¸ºbins
        distogram_labels = distance_to_dgram(true_distances, self.distance_bins, return_labels=True)
        
        # åº”ç”¨mask
        pair_mask = to_pairwise_mask(mask)
        valid_pair_mask = valid_indices_mask.unsqueeze(-1) & valid_indices_mask.unsqueeze(-2)
        final_mask = pair_mask & valid_pair_mask
        distogram_labels = torch.where(final_mask, distogram_labels, self.ignore_index)
        
        return distogram_labels
    


    def set_train_mode(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.train()
        # ç¡®ä¿RhoFoldå§‹ç»ˆä¿æŒevalæ¨¡å¼
        self.rhofold.eval()
    
    def set_eval_mode(self):
        """è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼"""
        self.eval()
    
    def forward(self, tokens, rna_fm_tokens, seq, target_coords=None, missing_atom_mask=None, seq_lengths=None, **kwargs):
        # ç¡®ä¿RhoFoldå‰å‘ä¼ æ’­æ—¶ä¸è®¡ç®—æ¢¯åº¦
        batch_num = tokens.shape[0]
        is_batched = batch_num > 1
        try:
            if is_batched:
                # è·å–åºåˆ—é•¿åº¦ä¿¡æ¯
                if seq_lengths is None:
                    # å¦‚æœæ²¡æœ‰æä¾›seq_lengthsï¼Œå°è¯•ä»seqä¸­æ¨æ–­
                    if isinstance(seq, list):
                        seq_lengths = torch.tensor([len(s) for s in seq], device=tokens.device)
                    else:
                        # å‡è®¾æ‰€æœ‰åºåˆ—éƒ½æ˜¯æœ€å¤§é•¿åº¦ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
                        seq_lengths = torch.full((batch_num,), tokens.shape[1], device=tokens.device)
                
                # å‡†å¤‡å­˜å‚¨ç»“æœçš„åˆ—è¡¨
                single_features = []
                pair_features = []
                max_seq_len = tokens.shape[2]  # paddingåçš„æœ€å¤§é•¿åº¦
                
                logger.info(f"å¤„ç†batch: {batch_num}ä¸ªåºåˆ—, æœ€å¤§é•¿åº¦: {max_seq_len}")
                
                for i in range(batch_num):
                    if seq_lengths is not None:
                        current_seq_len = seq_lengths[i].item()
                    else:
                        current_seq_len = tokens.shape[2]  # ä½¿ç”¨paddingåçš„æœ€å¤§é•¿åº¦
                    logger.debug(f"å¤„ç†åºåˆ— {i+1}/{batch_num}, çœŸå®é•¿åº¦: {current_seq_len}")
                    
                    # å»æ‰paddingï¼Œåªä¿ç•™çœŸå®åºåˆ—éƒ¨åˆ†
                    current_tokens = tokens[i:i+1, :, :current_seq_len]  # [1, MSA_depth, real_len]
                    current_rna_fm_tokens = rna_fm_tokens[i:i+1, :current_seq_len] if rna_fm_tokens is not None else None
                    
                    current_seq = seq[i]
                    
                    # å¤„ç†å½“å‰åºåˆ—
                    with torch.no_grad():
                        logger.debug(f"RhoFoldè¾“å…¥[{i}]: tokens {current_tokens.shape}, rna_fm_tokens {current_rna_fm_tokens.shape if current_rna_fm_tokens is not None else 'None'}")
                        outputs, single, pair = self.rhofold(current_tokens, current_rna_fm_tokens, current_seq, **kwargs)
                        
                        # single: [1, real_len, 256], pair: [1, real_len, real_len, 128]
                        logger.debug(f"RhoFoldè¾“å‡º[{i}]: single {single.shape}, pair {pair.shape}")
                        
                        # å°†ç»“æœpaddingå›åŸå§‹å¤§å°ä»¥ä¾¿åç»­batchå¤„ç†
                        padded_single = torch.zeros(1, max_seq_len, single.shape[-1], device=tokens.device)
                        padded_pair = torch.zeros(1, max_seq_len, max_seq_len, pair.shape[-1], device=tokens.device)
                        
                        padded_single[:, :current_seq_len] = single
                        padded_pair[:, :current_seq_len, :current_seq_len] = pair
                        
                        single_features.append(padded_single)
                        pair_features.append(padded_pair)
                
                # å°†æ‰€æœ‰åºåˆ—çš„ç»“æœé‡æ–°ç»„åˆæˆbatch
                single_fea = torch.cat(single_features, dim=0)  # [batch_num, max_seq_len, 256]
                pair_fea = torch.cat(pair_features, dim=0)      # [batch_num, max_seq_len, max_seq_len, 128]
                
                logger.debug(f"åˆå¹¶åçš„ç‰¹å¾: single {single_fea.shape}, pair {pair_fea.shape}")
                
            else:
                with torch.no_grad():
                    logger.debug(f"RhoFoldè¾“å…¥: {tokens.shape} {rna_fm_tokens.shape if rna_fm_tokens is not None else 'None'}")
                    outputs, single_fea, pair_fea = self.rhofold(tokens, rna_fm_tokens, seq[0], **kwargs)
        except Exception as e:
            logger.error(f"âš ï¸ RhoFoldå‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            exit()
                
        # å¦‚æœéœ€è¦æ¢¯åº¦ç”¨äºåç»­è®¡ç®—ï¼Œå¯ä»¥detachå¹¶é‡æ–°è®¾ç½®requires_grad
        if single_fea is not None:
            single_fea = single_fea.detach()
            # ä½¿ç”¨ç»´åº¦é€‚é…å™¨è°ƒæ•´single_feaä»256ç»´åˆ°384ç»´
            single_fea = self.single_dim_adapter(single_fea)
        if pair_fea is not None:
            pair_fea = pair_fea.detach()
            
        # single_fea: [bs, seq_len, dim_single(384)] 
        # pair_fea: [bs, seq_len, seq_len, dim_pairwise(128)]

        # å¦‚æœæ²¡æœ‰ç›®æ ‡åæ ‡ä¸”åœ¨è®­ç»ƒæ¨¡å¼ï¼Œè¿”å›RhoFoldçš„è¾“å‡ºç”¨äºæµ‹è¯•
        if target_coords is None:
            if self.training:
                logger.warning("âš ï¸ è®­ç»ƒæ¨¡å¼ä½†æ²¡æœ‰ç›®æ ‡åæ ‡")
                return None, None, None
        
        # å¤„ç†target_coordsï¼Œå°†batchå¼ é‡è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        atom_pos_list = None
        if target_coords is not None:
            if is_batched:
                # ä¸ºbatchå¤„ç†å‡†å¤‡è¾“å…¥åˆ—è¡¨
                input_list = []
                for i in range(batch_num):
                    if seq_lengths is not None:
                        current_seq_len = seq_lengths[i].item()
                    else:
                        current_seq_len = tokens.shape[2]  # ä½¿ç”¨paddingåçš„æœ€å¤§é•¿åº¦
                    
                    # è·å–å½“å‰åºåˆ—çš„åæ ‡
                    if missing_atom_mask is not None:
                        # ä½¿ç”¨missing_atom_maskç¡®å®šçœŸå®åŸå­æ•°é‡
                        current_missing_mask = missing_atom_mask[i]  # type: ignore
                        # æ‰¾åˆ°æœ‰æ•ˆåŸå­çš„æ•°é‡
                        valid_atoms = (~current_missing_mask).sum().item()
                        if valid_atoms > 0:
                            current_coords = target_coords[i, :valid_atoms]  # [valid_atoms, 3]
                        else:
                            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŸå­ï¼Œåˆ›å»ºç©ºå¼ é‡
                            current_coords = torch.empty(0, 3, device=target_coords.device)
                    else:
                        # å¦‚æœæ²¡æœ‰missing_atom_maskï¼Œä½¿ç”¨æ‰€æœ‰åæ ‡
                        current_coords = target_coords[i]  # [max_atoms, 3]
                    
                    # ä¸ºæ¯ä¸ªåºåˆ—åˆ›å»ºå•ç‹¬çš„è¾“å…¥å­—å…¸
                    input_dict = {
                        'ss_rna': [seq[i]],  # type: ignore # å•ä¸ªåºåˆ—ä½œä¸ºåˆ—è¡¨
                        'atom_pos': [current_coords]  # å•ä¸ªåæ ‡å¼ é‡ä½œä¸ºåˆ—è¡¨
                    }
                    input_list.append(input_dict)
                    
                    logger.debug(f"åºåˆ—[{i}]: {seq[i][:10] if seq is not None else 'None'}..., åºåˆ—é•¿åº¦: {current_seq_len}, åŸå­æ•°: {current_coords.shape[0]}")
                
                # ä½¿ç”¨process_multiple_alphafold3_inputså¤„ç†batch
                result = process_multiple_alphafold3_inputs(
                    input_list, 
                )
                af_in, atom_mask = result  # type: ignore
            else:
                # å•ä¸ªåºåˆ—çš„æƒ…å†µï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼
                if missing_atom_mask is not None:
                    valid_atoms = (~missing_atom_mask[0]).sum().item()
                    if valid_atoms > 0:
                        atom_pos_list = [target_coords[0, :valid_atoms]]
                    else:
                        atom_pos_list = [torch.empty(0, 3, device=target_coords.device)]
                else:
                    atom_pos_list = [target_coords[0]]
                
                af_in, atom_mask = process_alphafold3_input(
                    ss_rna=[seq[0]] if seq is not None else [],
                    atom_pos=atom_pos_list,
                )
        else:
            logger.warning("æ²¡æœ‰ç›®æ ‡åæ ‡")
            exit()

        logger.debug("AlphaFold3è¾“å…¥ï¼ˆå¯¹é½å‰ï¼‰:",
            "atom_inputs", af_in.atom_inputs.shape if af_in.atom_inputs is not None else 'None',
            "atompair_inputs", af_in.atompair_inputs.shape if af_in.atompair_inputs is not None else 'None',
            "additional_token_feats", af_in.additional_token_feats.shape if af_in.additional_token_feats is not None else 'None',
            "molecule_atom_lens", af_in.molecule_atom_lens.shape if af_in.molecule_atom_lens is not None else 'None',
            "molecule_ids", af_in.molecule_ids.shape if af_in.molecule_ids is not None else 'None',
            "atom_mask", atom_mask.shape if atom_mask is not None else 'None'
        )

        # ğŸ”§ ä½¿ç”¨align_inputå‡½æ•°å°†AlphaFold3æ ¼å¼è½¬æ¢ä¸ºRhoFold+æ ¼å¼
        logger.debug("å¼€å§‹æ ¼å¼å¯¹é½ï¼šAlphaFold3 -> RhoFold+")
        af_in_aligned, aligned_atom_mask = align_input(af_in, seq)
        
        # æ›´æ–°atom_maskä¸ºå¯¹é½åçš„ç‰ˆæœ¬
        atom_mask = aligned_atom_mask
        
        logger.debug("AlphaFold3è¾“å…¥ï¼ˆå¯¹é½åï¼‰:",
            "atom_inputs", af_in_aligned.atom_inputs.shape if af_in_aligned.atom_inputs is not None else 'None',
            "atompair_inputs", af_in_aligned.atompair_inputs.shape if af_in_aligned.atompair_inputs is not None else 'None',
            "additional_token_feats", af_in_aligned.additional_token_feats.shape if af_in_aligned.additional_token_feats is not None else 'None',
            "molecule_atom_lens", af_in_aligned.molecule_atom_lens.shape if af_in_aligned.molecule_atom_lens is not None else 'None',
            "molecule_ids", af_in_aligned.molecule_ids.shape if af_in_aligned.molecule_ids is not None else 'None',
            "atom_mask", atom_mask.shape if atom_mask is not None else 'None'
        )
        
        # ä½¿ç”¨å¯¹é½åçš„af_in
        af_in = af_in_aligned

        # è·å–è®¾å¤‡ä¿¡æ¯å¹¶å°†æ‰€æœ‰è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        device = tokens.device
        
        # å°†af_inçš„æ‰€æœ‰å±æ€§ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if af_in.atom_inputs is not None:
            af_in.atom_inputs = af_in.atom_inputs.to(device)
        if af_in.atompair_inputs is not None:
            af_in.atompair_inputs = af_in.atompair_inputs.to(device)
        if atom_mask is not None:
            atom_mask = atom_mask.to(device)
        if af_in.additional_token_feats is not None:
            af_in.additional_token_feats = af_in.additional_token_feats.to(device)
        if af_in.molecule_atom_lens is not None:
            af_in.molecule_atom_lens = af_in.molecule_atom_lens.to(device)
        if af_in.molecule_ids is not None:
            af_in.molecule_ids = af_in.molecule_ids.to(device)
        if af_in.atom_parent_ids is not None:
            af_in.atom_parent_ids = af_in.atom_parent_ids.to(device)
        if missing_atom_mask is not None:
            missing_atom_mask = missing_atom_mask.to(device)
        if af_in.additional_molecule_feats is not None:
            af_in.additional_molecule_feats = af_in.additional_molecule_feats.to(device)
        if af_in.is_molecule_types is not None:
            af_in.is_molecule_types = af_in.is_molecule_types.to(device)
        if af_in.molecule_atom_indices is not None:
            af_in.molecule_atom_indices = af_in.molecule_atom_indices.to(device)
        if hasattr(af_in, 'token_bonds') and af_in.token_bonds is not None:
            af_in.token_bonds = af_in.token_bonds.to(device)

        (
            single_inputs,
            _,
            _,
            atom_feats,
            atompair_feats

        ) = self.input_embedder(
            atom_inputs = af_in.atom_inputs,
            atompair_inputs = af_in.atompair_inputs,
            atom_mask = atom_mask,
            additional_token_feats = af_in.additional_token_feats,
            molecule_atom_lens = af_in.molecule_atom_lens,
            molecule_ids = af_in.molecule_ids
        )

        # ç”Ÿæˆæ›´ç²¾ç¡®çš„maskï¼Œè€ƒè™‘åºåˆ—é•¿åº¦çš„å·®å¼‚
        if af_in.molecule_atom_lens is not None:
            mask = af_in.molecule_atom_lens > 0
            
            # å¦‚æœæä¾›äº†seq_lengthsï¼Œè¿›ä¸€æ­¥ç»†åŒ–maskä»¥å¤„ç†åºåˆ—é•¿åº¦å·®å¼‚
            if seq_lengths is not None and is_batched:
                # åŸºäºçœŸå®åºåˆ—é•¿åº¦è°ƒæ•´å•ä¸ªè¡¨ç¤ºçš„mask
                batch_size, max_seq_len = single_fea.shape[0], single_fea.shape[1]
                seq_mask = torch.zeros(batch_size, max_seq_len, device=device, dtype=torch.bool)
                
                for i in range(batch_size):
                    actual_len = seq_lengths[i].item()
                    seq_mask[i, :actual_len] = True
                
                logger.debug(f"ç”Ÿæˆåºåˆ—mask: {seq_mask.shape}, çœŸå®é•¿åº¦: {seq_lengths.tolist() if seq_lengths is not None else 'None'}")
                
                # å°†seq_maskåº”ç”¨åˆ°singleå’Œpairç‰¹å¾ä¸Š
                single_fea = single_fea * seq_mask.unsqueeze(-1)  # [bs, seq_len, dim] * [bs, seq_len, 1]
                pair_fea = pair_fea * seq_mask.unsqueeze(-1).unsqueeze(-2)  # åº”ç”¨åˆ°è¡Œ
                pair_fea = pair_fea * seq_mask.unsqueeze(-2).unsqueeze(-1)  # åº”ç”¨åˆ°åˆ—
        else:
            logger.error("æ²¡æœ‰æä¾›molecule_atom_lens")
            exit()

        # ğŸ”§ å¤„ç†missing_atom_maskçš„å¯¹é½ - åŒæ ·éœ€è¦åˆ é™¤æ¯ä¸ªç¢±åŸºæœ€åä¸€ä¸ªåŸå­çš„mask
        if af_in.atom_pos is not None and missing_atom_mask is not None:
            from rhofold.utils.constants import ATOM_NAMES_PER_RESD
            
            # é‡æ–°è®¡ç®—keep_indicesï¼Œä¸align_inputå‡½æ•°ä¸­çš„é€»è¾‘ä¿æŒä¸€è‡´
            ATOMS_PER_BASE = {
                "A": len(ATOM_NAMES_PER_RESD["A"]),  # 22
                "G": len(ATOM_NAMES_PER_RESD["G"]),  # 23  
                "U": len(ATOM_NAMES_PER_RESD["U"]),  # 20
                "C": len(ATOM_NAMES_PER_RESD["C"])   # 20
            }
            
            # å¤„ç†åºåˆ—æ ¼å¼
            if isinstance(seq, str):
                sequences = [seq]
            elif isinstance(seq, list):
                sequences = seq
            else:
                sequences = seq
            
            # è®¡ç®—éœ€è¦ä¿ç•™çš„åŸå­ç´¢å¼•ï¼ˆä¸align_inputé€»è¾‘ä¸€è‡´ï¼‰
            keep_indices = []
            current_atom_idx = 0
            
            for sequence in sequences:
                for base in sequence:
                    if base in ATOMS_PER_BASE:
                        atoms_count = ATOMS_PER_BASE[base]
                        base_keep_indices = list(range(current_atom_idx, current_atom_idx + atoms_count - 1))
                        keep_indices.extend(base_keep_indices)
                        current_atom_idx += atoms_count
            
            # å¯¹missing_atom_maskè¿›è¡Œç›¸åŒçš„è£åˆ‡
            if len(keep_indices) > 0:
                keep_indices_tensor = torch.tensor(keep_indices, dtype=torch.long, device=device)
                
                # ç¡®ä¿keep_indicesä¸è¶…å‡ºmissing_atom_maskçš„èŒƒå›´
                max_original_atoms = missing_atom_mask.shape[1]
                valid_keep_indices = keep_indices_tensor[keep_indices_tensor < max_original_atoms]
                
                if len(valid_keep_indices) > 0:
                    missing_atom_mask_aligned = missing_atom_mask[:, valid_keep_indices]
                    logger.debug(f"å¯¹é½missing_atom_mask: {missing_atom_mask.shape} -> {missing_atom_mask_aligned.shape}")
                    missing_atom_mask = missing_atom_mask_aligned
                else:
                    logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„keep_indicesç”¨äºmissing_atom_maskå¯¹é½")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ‰©å±•missing_atom_maskä»¥åŒ¹é…af_in.atom_pos
            batch_num = af_in.atom_pos.shape[0]
            padded_atom_num = af_in.atom_pos.shape[1]
            current_atom_num = missing_atom_mask.shape[1]
            
            if padded_atom_num > current_atom_num:    
                missing_atom_mask = torch.cat([
                    missing_atom_mask, 
                    torch.ones(batch_num, padded_atom_num - current_atom_num, device=device, dtype=missing_atom_mask.dtype)
                ], dim=1)
                logger.debug(f"æ‰©å±•å¯¹é½åçš„missing_atom_mask: {current_atom_num} -> {padded_atom_num}")

        relative_position_encoding = self.relative_position_encoding(
            additional_molecule_feats = af_in.additional_molecule_feats,
        )
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ground truthæ•°æ®
        has_ground_truth = target_coords is not None and af_in.atom_pos is not None
        
        if has_ground_truth:
            # æœ‰ground truthï¼šè®¡ç®—æŸå¤±ï¼ˆæ— è®ºtrainingè¿˜æ˜¯evalæ¨¡å¼ï¼‰
            logger.debug(f"è®¡ç®—æŸå¤± - æ¨¡å¼: {'è®­ç»ƒ' if self.training else 'éªŒè¯'}")
            
            # ğŸ” éªŒè¯batchæ•°æ®ä¸€è‡´æ€§ï¼ˆåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
            if self.training and hasattr(self, 'mask_validator') and missing_atom_mask is not None:
                try:
                    # åªæœ‰åœ¨æœ‰å¿…è¦æ•°æ®æ—¶æ‰è¿›è¡ŒéªŒè¯
                    if target_coords is not None and seq is not None:
                        batch_validation = self.mask_validator.validate_batch_consistency(
                            tokens=tokens,
                            sequences=seq,
                            coordinates=target_coords,
                            missing_atom_mask=missing_atom_mask,
                            seq_lengths=seq_lengths
                        )
                        
                        # éªŒè¯EDMè¾“å…¥çš„maskä¸€è‡´æ€§
                        edm_validation = self.mask_validator.validate_edm_inputs(
                            atom_mask=atom_mask,
                            missing_atom_mask=missing_atom_mask,
                            molecule_atom_lens=af_in.molecule_atom_lens,
                            mask=mask
                        )
                        
                        # å¦‚æœæœ‰ä¸¥é‡é—®é¢˜ï¼Œè®°å½•è­¦å‘Š
                        if not batch_validation['is_valid'] or not edm_validation['is_valid']:
                            logger.warning(f"âš ï¸ MaskéªŒè¯å‘ç°é—®é¢˜:")
                            for error in batch_validation.get('errors', []) + edm_validation.get('errors', []):
                                logger.warning(f"  - {error}")
                            
                            # åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè¿™é‡Œåªè®°å½•
                            if len(batch_validation.get('errors', [])) > 0:
                                logger.warning(f"  batchéªŒè¯é”™è¯¯æ•°: {len(batch_validation['errors'])}")
                            if len(edm_validation.get('errors', [])) > 0:
                                logger.warning(f"  EDMè¾“å…¥éªŒè¯é”™è¯¯æ•°: {len(edm_validation['errors'])}")
                        
                        # è®°å½•ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                        if batch_validation.get('statistics'):
                            stats = batch_validation['statistics']
                            if 'seq_length_variance' in stats and stats['seq_length_variance'] > 0:
                                logger.debug(f"  åºåˆ—é•¿åº¦å·®å¼‚: {stats['seq_length_variance']}")
                            if 'avg_atom_coverage' in stats:
                                logger.debug(f"  å¹³å‡åŸå­è¦†ç›–ç‡: {stats['avg_atom_coverage']:.2%}")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ MaskéªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
                    # éªŒè¯å¤±è´¥ä¸åº”è¯¥å½±å“è®­ç»ƒï¼Œç»§ç»­æ‰§è¡Œ
            
            diffusion_loss, denoised_atom_pos, diffusion_loss_breakdown, _ = self.edm(
                atom_pos_ground_truth = af_in.atom_pos,
                additional_molecule_feats = af_in.additional_molecule_feats,
                is_molecule_types = af_in.is_molecule_types,
                add_smooth_lddt_loss = False,
                add_bond_loss = False,
                atom_feats = atom_feats,
                atompair_feats = atompair_feats,
                atom_parent_ids = af_in.atom_parent_ids,
                missing_atom_mask = missing_atom_mask,
                atom_mask = atom_mask,
                mask = mask,
                single_trunk_repr = single_fea,
                single_inputs_repr = single_inputs,
                pairwise_trunk = pair_fea,
                pairwise_rel_pos_feats = relative_position_encoding,
                molecule_atom_lens = af_in.molecule_atom_lens,
                molecule_atom_indices = af_in.molecule_atom_indices,
                token_bonds = af_in.token_bonds,
                return_denoised_pos = True,
                nucleotide_loss_weight = self.nucleotide_loss_weight,
                ligand_loss_weight = self.ligand_loss_weight,
            )
            
            # è®¡ç®—ç½®ä¿¡åº¦æŸå¤±ï¼ˆä½¿ç”¨ä¿®å¤çš„molecule_atom_indicesï¼‰
            try:
                # é¢„å¤„ç†molecule_atom_indicesï¼Œå°†-1æ›¿æ¢ä¸º0ä»¥é¿å…ç´¢å¼•è¶Šç•Œ
                if af_in.molecule_atom_indices is not None:
                    safe_molecule_atom_indices = af_in.molecule_atom_indices.clone()
                    safe_molecule_atom_indices = torch.where(
                        safe_molecule_atom_indices >= 0,
                        safe_molecule_atom_indices,
                        0
                    )
                else:
                    safe_molecule_atom_indices = None
                
                # è°ƒç”¨å®Œæ•´çš„ç½®ä¿¡åº¦æŸå¤±è®¡ç®—ï¼Œä½¿ç”¨å®‰å…¨çš„ç´¢å¼•
                confidence_loss, confidence_logits = self._compute_confidence_loss(
                    single_fea, single_inputs, pair_fea, denoised_atom_pos, atom_feats,
                    af_in.atom_pos, safe_molecule_atom_indices, af_in.molecule_atom_lens,
                    af_in.is_molecule_types, af_in.additional_molecule_feats, mask, atom_mask
                )
                
            except Exception as e:
                logger.warning(f"WARNING: ç½®ä¿¡åº¦æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                
                # å›é€€åˆ°ç®€å•æŸå¤±
                confidence_loss = torch.tensor(0.0, device=diffusion_loss.device)
                confidence_logits = {
                    'pae_loss': torch.tensor(0.0, device=diffusion_loss.device),
                    'pde_loss': torch.tensor(0.0, device=diffusion_loss.device),
                    'plddt_loss': torch.tensor(0.0, device=diffusion_loss.device),
                    'resolved_loss': torch.tensor(0.0, device=diffusion_loss.device),
                    'logits': None
                }
            
            # è®¡ç®—æ€»æŸå¤±
            distogram_loss = confidence_logits.get('distogram_loss', torch.tensor(0.0, device=diffusion_loss.device))
            total_loss = (diffusion_loss * self.loss_diffusion_weight + 
                         confidence_loss * self.loss_confidence_weight + 
                         distogram_loss * self.loss_distogram_weight)
            
            # åˆ›å»ºè¯¦ç»†çš„æŸå¤±åˆ†è§£
            zero_tensor = torch.tensor(0.0, device=diffusion_loss.device)
            loss_breakdown = DiffoldLossBreakdown(
                total_loss=total_loss,
                total_diffusion=diffusion_loss,
                distogram=distogram_loss,
                pae=confidence_logits.get('pae_loss', zero_tensor),
                pde=confidence_logits.get('pde_loss', zero_tensor),
                plddt=confidence_logits.get('plddt_loss', zero_tensor),
                resolved=confidence_logits.get('resolved_loss', zero_tensor),
                confidence=confidence_loss,
                diffusion_mse=diffusion_loss_breakdown.diffusion_mse,
                diffusion_bond=diffusion_loss_breakdown.diffusion_bond,
                diffusion_smooth_lddt=diffusion_loss_breakdown.diffusion_smooth_lddt,
            )
            
            return {
                'loss': total_loss,
                'predicted_coords': denoised_atom_pos,
                'loss_breakdown': loss_breakdown,
                'confidence_logits': confidence_logits.get('logits', None),
                'distogram_logits': confidence_logits.get('distogram_logits', None),
                'atom_mask': atom_mask,
                'mode': 'supervised'
            }
        else:
            # æ²¡æœ‰ground truthï¼šè¿›è¡Œé‡‡æ ·
            logger.info("è¿›è¡Œé‡‡æ ·ç”Ÿæˆ")
            
            sampled_coords = self.edm.sample(
                atom_feats = atom_feats,
                atompair_feats = atompair_feats,
                atom_parent_ids = af_in.atom_parent_ids,
                atom_mask = atom_mask,
                mask = mask,
                single_trunk_repr = single_fea,
                single_inputs_repr = single_inputs,
                pairwise_trunk = pair_fea,
                pairwise_rel_pos_feats = relative_position_encoding,
                molecule_atom_lens = af_in.molecule_atom_lens,
                num_sample_steps = 32,  # é‡‡æ ·æ­¥æ•°
                use_tqdm_pbar = False,
            )
            
            return {
                'predicted_coords': sampled_coords,
                'atom_mask': atom_mask,
                'mode': 'sampling'
            }

