#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Diffold æ¨¡å‹ç®€åŒ–è®­ç»ƒæµ‹è¯•è„šæœ¬

å¦‚æœå®Œæ•´ç‰ˆæµ‹è¯•å‡ºç°ä¾èµ–é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨æ­¤ç®€åŒ–ç‰ˆæœ¬
ç›´æ¥æ„é€ å¿…è¦çš„è¾“å…¥å¼ é‡æ¥æµ‹è¯•æ¨¡å‹è®­ç»ƒèƒ½åŠ›
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import traceback

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from rhofold.config import rhofold_config
from diffold.diffold import Diffold


def generate_mock_inputs(seq_length=16, msa_depth=32, batch_size=1):
    """ç”Ÿæˆæ¨¡æ‹Ÿè¾“å…¥æ•°æ®"""
    print(f"ç”Ÿæˆæ¨¡æ‹Ÿè¾“å…¥æ•°æ®...")
    print(f"åºåˆ—é•¿åº¦: {seq_length}, MSAæ·±åº¦: {msa_depth}, æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # RNAåºåˆ— (ACGU alphabet)
    seq = "ACGUACGUACGUACGU"[:seq_length]
    
    # ä¿®æ­£ï¼šæ¨¡æ‹ŸMSA tokens (RhoFoldä½¿ç”¨çš„æ ¼å¼)
    # æŒ‰ç…§get_msa_featureçš„å¤„ç†æ–¹å¼ï¼Œæœ€ç»ˆå½¢çŠ¶åº”è¯¥æ˜¯ [batch_size, msa_depth, seq_len]
    # ä¸åŒ…å«cls/eos token
    tokens = torch.randint(0, 9, (batch_size, msa_depth, seq_length))  # RNA alphabet size â‰ˆ 9
    
    # ä¿®æ­£ï¼šæ¨¡æ‹ŸRNA-FM tokens
    # æŒ‰ç…§get_rna_fm_tokençš„å¤„ç†ï¼Œåº”è¯¥æ˜¯ [batch_size, seq_len]ï¼Œä¸åŒ…å«cls/eos
    rna_fm_tokens = torch.randint(0, 25, (batch_size, seq_length))  # RNA-FM alphabet size
    
    # ä¿®æ­£ï¼šæ ¹æ®å®é™…çš„RNAåºåˆ—ç¡®å®šæ­£ç¡®çš„åŸå­æ•°é‡
    # å…ˆç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„åæ ‡ç”¨äºç¡®å®šåŸå­æ•°é‡
    temp_coords = torch.randn(1, seq_length * 30, 3) * 10  # ä¸´æ—¶ä¼°è®¡ï¼Œç¨å¾®å¤šä¸€äº›
    
    # å¯¼å…¥å¿…è¦çš„å‡½æ•°
    from diffold.input_processor import process_alphafold3_input
    
    # è°ƒç”¨process_alphafold3_inputæ¥ç¡®å®šå®é™…çš„åŸå­æ•°é‡
    af_in, atom_mask = process_alphafold3_input(
        ss_rna=[seq],
        atom_pos=[temp_coords.squeeze(0)],  # éœ€è¦æ˜¯å¼ é‡åˆ—è¡¨ï¼Œå¹¶ä¸”å»æ‰batchç»´åº¦
    )
    
    # è·å–å®é™…éœ€è¦çš„åŸå­æ•°é‡
    actual_num_atoms = af_in.atom_inputs.shape[1]
    print(f"âœ“ æ ¹æ®åºåˆ—'{seq}'ç¡®å®šå®é™…åŸå­æ•°é‡: {actual_num_atoms}")
    
    # ç”Ÿæˆæ­£ç¡®æ•°é‡çš„åŸå­åæ ‡
    target_coords = torch.randn(batch_size, actual_num_atoms, 3) * 10  # åˆç†çš„3Dåæ ‡èŒƒå›´
    
    print(f"âœ“ tokenså½¢çŠ¶: {tokens.shape}")
    print(f"âœ“ rna_fm_tokenså½¢çŠ¶: {rna_fm_tokens.shape}")
    print(f"âœ“ target_coordså½¢çŠ¶: {target_coords.shape}")
    print(f"âœ“ åºåˆ—: {seq}")
    
    return {
        'seq': seq,
        'tokens': tokens,
        'rna_fm_tokens': rna_fm_tokens,
        'target_coords': target_coords
    }


def generate_mock_coordinates(sequence, atoms_per_residue=27):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„åŸå­åæ ‡"""
    seq_len = len(sequence)
    # å…ˆç”¨åˆç†çš„ä¼°è®¡å€¼ï¼Œå®é™…æ•°é‡ä¼šåœ¨process_alphafold3_inputä¸­ç¡®å®š
    num_atoms = seq_len * atoms_per_residue
    
    # ç”Ÿæˆéšæœºçš„3Dåæ ‡ï¼Œä½†ä¿æŒåˆç†çš„ç»“æ„
    coords = torch.randn(1, num_atoms, 3) * 10  # æ‰©å±•åˆ°åˆç†çš„åŸƒå•ä½
    
    return coords


def adjust_coordinates_to_match_input(target_coords, af_in):
    """æ ¹æ®process_alphafold3_inputçš„è¾“å‡ºè°ƒæ•´åæ ‡æ•°é‡"""
    if hasattr(af_in, 'atom_inputs') and af_in.atom_inputs is not None:
        expected_atoms = af_in.atom_inputs.shape[1]  # å®é™…çš„åŸå­æ•°
        current_atoms = target_coords.shape[1]
        
        if expected_atoms != current_atoms:
            print(f"è°ƒæ•´åŸå­åæ ‡æ•°é‡: {current_atoms} -> {expected_atoms}")
            if expected_atoms > current_atoms:
                # å¦‚æœéœ€è¦æ›´å¤šåŸå­ï¼Œç”¨é›¶åæ ‡å¡«å……
                padding = torch.zeros(target_coords.shape[0], expected_atoms - current_atoms, 3, 
                                    device=target_coords.device)
                target_coords = torch.cat([target_coords, padding], dim=1)
            else:
                # å¦‚æœåŸå­å¤ªå¤šï¼Œæˆªå–åˆ°éœ€è¦çš„æ•°é‡
                target_coords = target_coords[:, :expected_atoms, :]
    
    return target_coords


def test_model_initialization():
    """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
    print("=" * 50)
    print("æµ‹è¯• 1: æ¨¡å‹åˆå§‹åŒ–")
    print("=" * 50)
    
    try:
        # ä½¿ç”¨æŒ‡å®šçš„æƒé‡è·¯å¾„
        rhofold_checkpoint_path = "/home/yamanashi/RhoFold/pretrained/model_20221010_params.pt"
        
        print(f"åˆå§‹åŒ–Diffoldæ¨¡å‹...")
        print(f"RhoFoldæƒé‡è·¯å¾„: {rhofold_checkpoint_path}")
        
        # æ£€æŸ¥æƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists(rhofold_checkpoint_path):
            print("âœ“ æƒé‡æ–‡ä»¶å­˜åœ¨")
        else:
            print(f"âš ï¸ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {rhofold_checkpoint_path}")
            print("å°†å°è¯•ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡è¿›è¡Œæµ‹è¯•")
            rhofold_checkpoint_path = None
        
        model = Diffold(rhofold_config, rhofold_checkpoint_path=rhofold_checkpoint_path)
        
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥RhoFoldå‚æ•°æ˜¯å¦è¢«å†»ç»“
        rhofold_params = list(model.rhofold.parameters())
        rhofold_param_count = sum(p.numel() for p in rhofold_params)  # è®¡ç®—å®é™…å‚æ•°æ•°é‡
        frozen_count = sum(1 for p in rhofold_params if not p.requires_grad)
        print(f"âœ“ RhoFoldå‚æ•°æ€»æ•°: {len(rhofold_params)}")
        print(f"âœ“ RhoFoldå‚æ•°æ•°é‡: {rhofold_param_count:,}")
        print(f"âœ“ å†»ç»“å‚æ•°æ•°é‡: {frozen_count}")
        print(f"âœ“ å†»ç»“æ¯”ä¾‹: {frozen_count/len(rhofold_params)*100:.1f}%")
        
        # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
        trainable_params = model.get_trainable_parameters()
        trainable_param_count = sum(p.numel() for p in trainable_params)
        total_param_count = sum(p.numel() for p in model.parameters())
        
        print(f"âœ“ å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(trainable_params)}")
        print(f"âœ“ å¯è®­ç»ƒå‚æ•°æ€»æ•°: {trainable_param_count:,}")
        print(f"âœ“ æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_param_count:,}")
        print(f"âœ“ å†»ç»“å‚æ•°æ•°é‡: {total_param_count - trainable_param_count:,}")
        print(f"âœ“ å†»ç»“å‚æ•°æ¯”ä¾‹: {(total_param_count - trainable_param_count)/total_param_count*100:.1f}%")
        
        return model
        
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        traceback.print_exc()
        return None


def test_forward_pass(model, data):
    """æµ‹è¯•å‰å‘ä¼ æ’­"""
    print("=" * 50)
    print("æµ‹è¯• 2: å‰å‘ä¼ æ’­")
    print("=" * 50)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(device)
        tokens = data['tokens'].to(device)
        rna_fm_tokens = data['rna_fm_tokens'].to(device)
        target_coords = data['target_coords'].to(device)
        
        print("âœ“ æ¨¡å‹å’Œæ•°æ®å·²ç§»åŠ¨åˆ°è®¾å¤‡")
        
        # æµ‹è¯•æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­ï¼ˆæ— ç›®æ ‡åæ ‡ï¼‰
        print("æµ‹è¯•æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­...")
        model.set_eval_mode()
        
        with torch.no_grad():
            outputs = model(
                tokens=tokens,
                rna_fm_tokens=rna_fm_tokens,
                seq=data['seq']
            )
        
        if isinstance(outputs, tuple) and len(outputs) >= 2:
            rhofold_outputs, single_fea, pair_fea = outputs
            print("âœ“ æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"  - single_feaå½¢çŠ¶: {single_fea.shape if single_fea is not None else 'None'}")
            print(f"  - pair_feaå½¢çŠ¶: {pair_fea.shape if pair_fea is not None else 'None'}")
        else:
            print("âœ“ æ¨ç†æ¨¡å¼å‰å‘ä¼ æ’­æˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­ï¼ˆæœ‰ç›®æ ‡åæ ‡ï¼‰
        print("æµ‹è¯•è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­...")
        model.set_train_mode()
        
        loss, denoised_pos, loss_breakdown = model(
            tokens=tokens,
            rna_fm_tokens=rna_fm_tokens,
            seq=data['seq'],
            target_coords=target_coords
        )
        
        print("âœ“ è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - æŸå¤±å€¼: {loss.item():.6f}")
        print(f"  - æŸå¤±éœ€è¦æ¢¯åº¦: {loss.requires_grad}")
        print(f"  - æŸå¤±ç±»å‹: {type(loss)}")
        
        if loss_breakdown is not None and isinstance(loss_breakdown, dict):
            print("  - æŸå¤±åˆ†è§£:")
            for key, value in loss_breakdown.items():
                if isinstance(value, torch.Tensor):
                    print(f"    - {key}: {value.item():.6f}")
        
        return loss
        
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        traceback.print_exc()
        return None


def test_backward_pass(model, data):
    """æµ‹è¯•åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°"""
    print("=" * 50)
    print("æµ‹è¯• 3: åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°")
    print("=" * 50)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.set_train_mode()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        trainable_params = model.get_trainable_parameters()
        optimizer = optim.Adam(trainable_params, lr=1e-4)
        
        print(f"âœ“ åˆ›å»ºä¼˜åŒ–å™¨ï¼Œç®¡ç† {len(trainable_params)} ä¸ªå¯è®­ç»ƒå‚æ•°")
        
        # è®°å½•æ›´æ–°å‰çš„å‚æ•°å€¼ï¼ˆé‡‡æ ·å‡ ä¸ªå‚æ•°ï¼‰
        param_before = {}
        sample_size = min(5, len(trainable_params))
        for i in range(sample_size):
            param_before[i] = trainable_params[i].clone().detach()
        
        # å‰å‘ä¼ æ’­
        tokens = data['tokens'].to(device)
        rna_fm_tokens = data['rna_fm_tokens'].to(device)
        target_coords = data['target_coords'].to(device)
        
        loss, _, _ = model(
            tokens=tokens,
            rna_fm_tokens=rna_fm_tokens,
            seq=data['seq'],
            target_coords=target_coords
        )
        
        print(f"âœ“ å‰å‘ä¼ æ’­å®Œæˆï¼ŒæŸå¤±: {loss.item():.6f}")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        print("âœ“ åå‘ä¼ æ’­å®Œæˆ")
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_count = 0
        total_grad_norm = 0.0
        for param in trainable_params:
            if param.grad is not None:
                grad_count += 1
                total_grad_norm += param.grad.norm().item() ** 2
        
        total_grad_norm = total_grad_norm ** 0.5
        print(f"âœ“ æ¢¯åº¦è®¡ç®—å®Œæˆ")
        print(f"  - æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {grad_count}/{len(trainable_params)}")
        print(f"  - æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        
        # å‚æ•°æ›´æ–°
        optimizer.step()
        print("âœ“ å‚æ•°æ›´æ–°å®Œæˆ")
        
        # éªŒè¯å‚æ•°ç¡®å®æ›´æ–°äº†
        updated_count = 0
        for i in range(sample_size):
            if not torch.equal(param_before[i], trainable_params[i]):
                updated_count += 1
        
        print(f"âœ“ å‚æ•°æ›´æ–°éªŒè¯: {updated_count}/{sample_size} ä¸ªé‡‡æ ·å‚æ•°å·²æ›´æ–°")
        
        return True
        
    except Exception as e:
        print(f"âœ— åå‘ä¼ æ’­å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_training_loop(model, data, num_steps=3):
    """æµ‹è¯•è®­ç»ƒå¾ªç¯"""
    print("=" * 50)
    print(f"æµ‹è¯• 4: è®­ç»ƒå¾ªç¯ ({num_steps} æ­¥)")
    print("=" * 50)
    
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        model.set_train_mode()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        trainable_params = model.get_trainable_parameters()
        optimizer = optim.Adam(trainable_params, lr=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)
        
        print(f"âœ“ è®­ç»ƒè®¾ç½®å®Œæˆ")
        
        # å‡†å¤‡æ•°æ®
        tokens = data['tokens'].to(device)
        rna_fm_tokens = data['rna_fm_tokens'].to(device)
        target_coords = data['target_coords'].to(device)
        
        losses = []
        
        for step in range(num_steps):
            print(f"\n--- æ­¥éª¤ {step + 1}/{num_steps} ---")
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            
            loss, _, loss_breakdown = model(
                tokens=tokens,
                rna_fm_tokens=rna_fm_tokens,
                seq=data['seq'],
                target_coords=target_coords
            )
            
            losses.append(loss.item())
            print(f"æŸå¤±: {loss.item():.6f}")
            
            # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            current_lr = optimizer.param_groups[0]['lr']
            print(f"å­¦ä¹ ç‡: {current_lr:.6f}")
            
            if loss_breakdown is not None and isinstance(loss_breakdown, dict):
                print("æŸå¤±åˆ†è§£:")
                for key, value in loss_breakdown.items():
                    if isinstance(value, torch.Tensor):
                        print(f"  - {key}: {value.item():.6f}")
        
        print(f"\nâœ“ è®­ç»ƒå¾ªç¯å®Œæˆ")
        print(f"âœ“ æŸå¤±å˜åŒ–: {losses[0]:.6f} -> {losses[-1]:.6f}")
        
        if len(losses) > 1:
            loss_change = losses[-1] - losses[0]
            print(f"âœ“ æŸå¤±å˜åŒ–é‡: {loss_change:.6f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— è®­ç»ƒå¾ªç¯å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def test_training_mode(model, inputs):
    """æµ‹è¯•è®­ç»ƒæ¨¡å¼"""
    print("\nğŸ”¥ æµ‹è¯•è®­ç»ƒæ¨¡å¼...")
    
    model.set_train_mode()
    
    try:
        # è®­ç»ƒå‰å‘ä¼ æ’­
        loss, denoised_pos, loss_breakdown = model(
            tokens=inputs['tokens'].cuda(),
            rna_fm_tokens=inputs['rna_fm_tokens'].cuda(),
            seq=inputs['seq'],
            target_coords=inputs['target_coords'].cuda()
        )
        
        print(f"âœ“ è®­ç»ƒå‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"  - æ‰©æ•£æŸå¤±: {loss.item():.6f}")
        print(f"  - å»å™ªåæ ‡å½¢çŠ¶: {denoised_pos.shape if denoised_pos is not None else 'None'}")
        
        if loss_breakdown:
            print(f"  - æŸå¤±è¯¦æƒ…: {loss_breakdown}")
        
        return loss
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§¬ Diffold æ¨¡å‹ç®€åŒ–è®­ç»ƒæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    data = generate_mock_inputs(seq_length=16, msa_depth=32, batch_size=1)
    
    # æµ‹è¯•1: æ¨¡å‹åˆå§‹åŒ–
    model = test_model_initialization()
    if model is None:
        print("âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return False
    
    # æµ‹è¯•2: å‰å‘ä¼ æ’­ 
    loss = test_forward_pass(model, data)
    if loss is None:
        print("âŒ å‰å‘ä¼ æ’­å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return False
    
    # æµ‹è¯•3: åå‘ä¼ æ’­
    backward_success = test_backward_pass(model, data)
    if not backward_success:
        print("âŒ åå‘ä¼ æ’­å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return False
    
    # æµ‹è¯•4: è®­ç»ƒå¾ªç¯
    training_success = test_training_loop(model, data, num_steps=3)
    if not training_success:
        print("âŒ è®­ç»ƒå¾ªç¯å¤±è´¥")
        return False
    
    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Diffoldæ¨¡å‹å¯ä»¥æ­£å¸¸è®­ç»ƒ")
    print("âœ… æµ‹è¯•ç»“æœ:")
    print("  - âœ“ æ¨¡å‹åˆå§‹åŒ–æ­£å¸¸")
    print("  - âœ“ RhoFoldå‚æ•°æ­£ç¡®å†»ç»“")
    print("  - âœ“ å‰å‘ä¼ æ’­å·¥ä½œæ­£å¸¸")
    print("  - âœ“ åå‘ä¼ æ’­å’Œæ¢¯åº¦è®¡ç®—æ­£å¸¸")
    print("  - âœ“ å‚æ•°æ›´æ–°æ­£å¸¸")
    print("  - âœ“ è®­ç»ƒå¾ªç¯æ­£å¸¸")
    print("=" * 60)
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 